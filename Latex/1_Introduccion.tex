\chapter{INTRODUCCIÓN}
\label{ch:1}

% Descripción de los objetivo del aprendizaje adversario:

\section{Introducción}

El objetivo principal de este trabajo es el análisis de los distintos tipos de ataques y defensas que se pueden aplicar a un proceso de aprendizaje automático, nos referimos principalmente al aprendizaje profundo.
Este trabajo tratará principalmente de clasificar y explorar la seguridad que cuentan los modelos de \gls{AI} actuales.
Existen una gran variedad de modelos y objetivos en redes neuronales, los ataques que se pueden hacer sobre estos modelos están orientados en las siguientes categorías (engañar, evadir, errar, explorar o denegar) el modelo.

Debemos comprender que debemos saber que la seguridad repercute en todo el proceso de creación del modelo, desde la recogida de los datos, tratamiento, diseño y creación del modelo.
Un ataque puede estar dirigido al conjunto de datos con el que se entrenará el modelo o a la red neuronal una vez entrenada.
Además, un ataque puede dirigirse a descubrir muestras que produzcan resultados erróneos, por lo que los posibles vectores de ataque son muy variados y complejos.

Cada uno de estos tipos de ataques tiene su nomenclatura y debe definirse correctamente, ya que de lo contrario puede ser muy ambiguo el tipo de ataque que se está realizando, el objetivo que busca y los métodos que están empleando.

Lo que buscaremos en este trabajo será definir, analizar y estructurar los distintos tipos de ataques y sus posibles defensas, con el objetivo final de proteger y defender los modelos de inteligencia artificial para hacerlos más robustos.

La idea de hacer robustos los modelos es que las defensas detecten ataques a la vez que se mejora la solidez del aprendizaje, para no cometer fallos al introducir valores anómalos.

\section{Motivación}

% https://ec.europa.eu/commission/presscorner/detail/en/ip_23_6473
% Etica 

La necesidad de desarrollar un marco de inteligencia artificial fiable

Categorías

\begin{itemize}
    \item Riesgo mínimo:
    \item Riesgo alto:
    \item Riesgo inaceptable:
\end{itemize}



\section{Propósitos del proyecto}

\section{Objetivos}
% Estandares

\section{Metodología y planificación del proyecto}

\subsection{Metodología}



\section{Estructura del proyecto}

\subsection{Planificación y costes}

\section{Conclusión}






\section{Vectores de ataque}

La idea detrás del aprendizaje automático es la de poder predecir modelos predictivos, para ello se usan conjuntos de datos para el entrenamiento.

Los conjuntos de datos se han de procesar, ya que suelen contener mucho ruido, es decir, información muy poco valiosa, errónea o, por el contrario, una alta dimensionalidad de los datos que puede ser contraproducente por no poder reproducir esas medidas o por la poca información que aportan.

Los ataques al aprendizaje automático o profundo suelen estar dirigidos a las distintas etapas de creación y uso de un modelo.

\begin{itemize}
    \item Alteración de los datos de entrada
    \item Alteración del proceso de aprendizaje
    \item Bloqueo del modelo
\end{itemize}

Durante el transcurso del trabajo discutiremos los siguientes ataques y defensas que se pueden usar en los modelos.

\begin{itemize}
    \item Envenenamiento de datos.
    \item Puerta trasera.
    \item Extracción de datos
    \item Ingeniería inversa.
    \item Evasión.
\end{itemize}

Un buen símil con la seguridad clásica puede ser categorizar la seguridad de los modelos con el modelo \gls{STRIDE} de microsoft \footnote{Modelado de amenazas de microsoft \url{https://learn.microsoft.com/es-es/azure/security/develop/threat-modeling-tool-threats}}.

\newgeometry{margin=2cm} % modify this if you need even more space
\begin{landscape}

    \begin{table}[ht!]
        \centering
        \small
        \def\arraystretch{1.5}
        \begin{tabular}{lp{10cm}}
            \toprule
            \textbf{Amenaza}    & \textbf{Tipo de ataque}                                           \\
            \midrule
            Corrupción de datos & Ataques de envenenamiento de datos, ataques de puerta trasera     \\
            Extracción de datos & Ataques de inferencia de membresía, ataques de ingeniería inversa \\
            Denegación          & Ataques de envenenamiento de datos                                \\
            \bottomrule
        \end{tabular}
        \caption{Amenazas STRIDE relacionadas con la seguridad de la IA}
        \label{Tabla.Requisitos}
    \end{table}

\end{landscape}
\restoregeometry
