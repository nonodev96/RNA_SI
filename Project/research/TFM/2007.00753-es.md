# Opportunities and Challenges in Deep Learning Adversarial Robustness: A Survey

Resumen-A medida que buscamos desplegar modelos de aprendizaje automático más allá de dominios virtuales y controlados, es crítico analizar no sólo la precisión o el hecho de que funcione la mayor parte del tiempo, sino si dicho modelo es realmente robusto y fiable. Este artículo estudia estrategias para implementar algoritmos entrenados de forma robusta frente a adversarios con el fin de garantizar la seguridad de los algoritmos de aprendizaje automático. Proporcionamos una taxonomía para clasificar los ataques y defensas adversariales, formulamos el problema de Optimización Robusta en un entorno min-max, y lo dividimos en 3 subcategorías, a saber: (re)Entrenamiento Adversarial, Enfoque de Regularización y Defensas Certificadas. Analizamos los resultados más recientes e importantes en la generación de ejemplos adversariales, mecanismos de defensa con (re)Entrenamiento adversarial como su principal defensa contra las perturbaciones. También estudiamos métodos que añaden términos de regularización que cambian el comportamiento del gradiente, dificultando que los atacantes consigan su objetivo. Alternativamente, hemos estudiado métodos que derivan formalmente certificados de robustez resolviendo exactamente el problema de optimización o mediante aproximaciones utilizando límites superiores o inferiores. Además, analizamos los retos a los que se enfrentan la mayoría de los algoritmos recientes y presentamos perspectivas de investigación futuras.

## Introduction

Los modelos de aprendizaje profundo (Deep Learning, DL) ([1]) están cambiando la forma de resolver problemas que han requerido muchos intentos desde los más diversos campos de la ciencia. DL es una mejora sobre las Redes Neuronales (NN) de Inteligencia Artificial (IA), en las que se apilan más capas para otorgar un mayor nivel de abstracción y un mejor razonamiento sobre los datos en comparación con otros algoritmos de Aprendizaje Automático (ML) ([2]). Desde el surgimiento del DL, apoyado en muchos casos por entornos en la nube [3], [4], [5], la arquitectura base y sus variaciones se han aplicado en muchos avances científicos en los más diversos campos del conocimiento, por ejemplo en la predicción de la progresión de la enfermedad AMD ([6]), la predicción de potenciadores de ADN para programas de expresión génica ([7]), las elecciones y el análisis demográfico basado en imágenes de satélite ([8]), el filtrado de datos para señales de ondas gravitacionales ([9]). El enfoque DL también se ha convertido en uno de los más utilizados para el procesamiento del lenguaje natural ([10]) y el reconocimiento del habla ([11]).

Una de las variantes más populares de la arquitectura DL, las Redes Neuronales Convolucionales (CNN) han impulsado significativamente el rendimiento de los algoritmos DL en aplicaciones de visión por computador (CV) ([12]), llevándolo a varias áreas de CV como, detección de objetos ([13], [14]), reconocimiento de acciones [15], [16], estimación de poses [17], [18], segmentación de imágenes [19], [20], y seguimiento de movimiento [21]. A partir de ImageNet [22], propuesta en 2012, el campo de las CNN ha experimentado una gran mejora con un rendimiento sobrehumano en tareas específicas, proporcionando soluciones incluso a problemas médicos [23].

Impulsadas por el hecho de que nuevos frameworks, librerías y recursos de hardware están siendo mejorados y puestos a disposición del público y de la comunidad científica [24], [25], [26], las Redes Neuronales Profundas (DNN) están siendo mejoradas constantemente y logrando nuevos avances en rendimiento [27], [28], [29]. Con la madurez actual de los algoritmos DNN, se están aplicando en la resolución de problemas críticos de seguridad [30], como los coches autoconducidos [31], [32], los sistemas de vehículos aéreos multiagente con identificación facial [33], la robótica [34], [35], la detección de ingeniería social [36], la detección de anomalías en la red [37], la inspección profunda de paquetes en redes [38]. Las aplicaciones DNN ya forman parte de nuestro día a día (asistentes personales [39], recomendación de productos [40], identificación biométrica [41]) y tienden a ocupar un mayor espacio a medida que pasa el tiempo.

Como se ha visto en muchas publicaciones, las DNN han demostrado tener una precisión de nivel humano incluso para tareas significativamente complejas como jugar a juegos sin ninguna regla previa conocida, excepto los fotogramas actuales [42]. En contraste con la mencionada precisión de los modelos DNN, se ha demostrado en publicaciones anteriores [43], [44], [45], que los modelos DNN son susceptibles a pequeñas perturbaciones de entrada, en la mayoría de los casos imperceptibles para el ojo humano. Los resultados de estas publicaciones han mostrado la facilidad con la que un pequeño ruido aditivo dirigido a la imagen de entrada, hace que los modelos clasifiquen erróneamente objetos que antes podían ser identificados con un 99,99% de confianza. Más alarmante es el hecho de que tales modelos informan de una alta confianza en las predicciones. Tales perturbaciones, que pueden engañar a un modelo entrenado, se conocen como ataques adversarios. Con consecuencias tan alarmantes, el estudio de los ataques adversarios y la robustez frente a ellos se ha convertido en un tema de gran investigación en los últimos años.

En la actualidad existe un número considerablemente elevado de trabajos de investigación sobre métodos para identificar los ataques adversarios y defenderse de las incursiones contra el modelo [46], [47]. Una forma de resolver este problema es añadir una mejor intuición a los modelos, a través de la explicabilidad [48], pero tales modelos no tienen como objetivo la mejora directa del modelo. Por otro lado, se han publicado varios enfoques para generar modelos que sean robustos frente a ataques adversarios [49]. El objetivo de los investigadores es introducir en sus modelos capas de robustez tales que los modelos no se vean engañados por ejemplos fuera de distribución, ataques conocidos o desconocidos, ataques dirigidos o no dirigidos. Garantizar la precisión de estos modelos teniendo en cuenta la seguridad es de suma importancia para los arquitectos de sistemas, sobre todo haciéndolos robustos ante la presencia de ataques de adversarios, ruido, especificación errónea del modelo e incertidumbre. Este estudio pretende reunir los últimos avances en robustez de las DNN, señalando las principales líneas de investigación que se han seguido recientemente para generar modelos DNN robustos. Sacamos a la luz los últimos avances tanto teóricos como aplicados.
Inspirándonos en [50], analizamos la robustez de las DNN bajo la perspectiva de cómo se pueden generar ejemplos adversarios, y cómo se pueden formular defensas contra tales algoritmos. En palabras generales, definimos el problema de la robustez frente a ataques adversarios como un problema de optimización dual, en el que los atacantes intentan maximizar la pérdida mientras que las defensas intentan minimizar la posibilidad de que un modelo sea engañado por el atacante. En esta formulación, los modelos actuales basados en funciones de activación no lineales introducen restricciones de desigualdad no lineales en la optimización, lo que genera una compensación inherente entre las soluciones exactas y la escalabilidad del modelo. Este trade-off viene en forma de soluciones exactas lentas a través de programación lineal mixta entera, o en aproximaciones a la función del objetivo que, o bien, se basa en los métodos de ataque existentes para proporcionar una estimación heurística local a la función de maximización, o aproximaciones de los límites de las restricciones o de la función del objetivo para generar regiones de certificación, en las que no existe ningún ejemplo adversario.

Más concretamente este trabajo presenta las siguientes contribuciones:

1. Caracterizamos las defensas frente a ataques adversarios como un problema de optimización min-max, investigando soluciones que implican aproximación heurística, solución exacta y aproximaciones de límites superiores/inferiores para generar modelos robustos frente a ataques adversarios
2. Investigamos, analizamos y categorizamos las aproximaciones más recientes y/o importantes para generar ejemplos adversariales, ya que son la base para generar defensas fuertes, a través del (re)Entrenamiento Adversarial.
3. Investigamos, analizamos y categorizamos las aproximaciones más recientes e importantes para generar defensas contra ataques adversariales, proporcionando una taxonomía, descripción de los métodos y los principales resultados en dichas aproximaciones.

Organizamos este estudio de la siguiente manera. En la sección 2 describimos taxonomías tanto para la generación de ejemplos adversariales como para las defensas. Clasificamos los modelos adversariales en función del momento del ataque, la información de que dispone el atacante, el objetivo y el método de cálculo del algoritmo. Además, clasificamos el tipo de perturbación utilizado por los atacantes. Dividimos los métodos de defensa en tres categorías, a saber, enmascaramiento/ofuscación de gradiente, optimización robusta y detección de ejemplos adversarios. Centramos esta investigación en la optimización robusta, y la subdividimos a su vez en 3 grupos: Entrenamiento Adversarial, Defensas Certificadas, y Enfoque de Regularización. En la sección 3, describimos varios ataques adversariales relevantes, y los resumimos en la Tabla 2. En la sección 4 describimos los resultados más relevantes en Optimización Robusta, y proporcionamos un árbol que mapea estas publicaciones a los 3 subgrupos de Optimización Robusta. En la sección 5 discutimos los retos y oportunidades actuales en defensas robustas.

## 2 TAXONOMY OF ADVERSARIAL ATTACKS AND DEFENSES

Mantenemos un conjunto de notaciones coherente a lo largo del estudio, y para facilitar la lectura resumimos en la Tabla 1 las notaciones y símbolos más utilizados que emplearemos junto con este estudio. Para los trabajos que requieran algunos términos específicos, los definimos en la sección en la que se presentan.

### 2.1 Attack Threat Model

Se han hecho varios intentos de categorizar los ataques al aprendizaje automático. Aquí destilamos los aspectos más importantes que caracterizan a los modelos generadores de ejemplos adversarios en relación con su arquitectura. Nos centramos en los aspectos más relevantes para la discusión de la robustez de los adversarios. Para ello, clasificamos los ataques relativos a 3 timing, información, objetivos y frecuencia de ataque siguiendo lo propuesto en [51]:

- **Momento**: Una primera característica crucial para modelar los ataques adversarios es cuándo se produce. Para ello tenemos dos posibilidades, los ataques de evasión y los de envenenamiento. Los ataques de evasión son los que se producen en el momento de la inferencia y suponen que el modelo ya ha sido entrenado. Los ataques de envenenamiento en general tienen como objetivo los datos, y la fase de entrenamiento del modelo.
- **Información**: Otra característica del ataque hace referencia a la información a la que tiene acceso el atacante. En el contexto de caja blanca, el adversario tiene pleno acceso a la información relativa al modelo y al propio modelo, a diferencia del contexto de caja negra, en el que se dispone de muy poca o ninguna información. Los ataques de caja blanca se refieren a aquellos en los que el adversario puede consultar sin restricciones el modelo para obtener cualquier información, como los pesos, el gradiente, los hiperparámetros del modelo o las puntuaciones de las predicciones. Mientras que en los ataques de caja negra el adversario tiene información limitada o nula sobre estos parámetros, aunque puede obtener parte de la información indirectamente, por ejemplo, mediante consultas. Algunos también definen los ataques de caja gris, en los que los atacantes solamente pueden conocer la representación de las características y el tipo de modelo utilizado, pero no tienen acceso al conjunto de datos ni a la información del modelo. Un cuarto escenario es el denominado de caja negra restringida, o también conocido como ataque no-box. En este supuesto, el atacante no tiene acceso a ninguna información, y la investigación se centra principalmente en la transferibilidad del ataque. El objetivo es evaluar la posibilidad de transferir el ataque realizado en una DNN al modelo objetivo inaccesible [52]. En este trabajo, evaluamos los modelos en un escenario binario, el adversario tiene acceso completo a la DNN o caja negra con acceso limitado a través de consultas, que también puede proporcionar puntuaciones de clase.
- **Objetivos**: Los atacantes pueden tener diferentes razones para atacar un algoritmo específico. Pero en la mayoría de los casos, el atacante tiene un objetivo concreto y necesita que el algoritmo produzca un resultado específico, en cuyo caso se trata de un ataque dirigido, o simplemente quiere reducir la fiabilidad del algoritmo forzando un error. En este último caso, tenemos un ataque no dirigido.
- **Frecuencia del ataque**: El ataque al modelo de la víctima puede ser iterativo o puntual. En el único, la optimización de la función objetivo del atacante se produce en un solo paso, mientras que el método iterativo requiere varios pasos para generar la perturbación.

### 2.2 Attack Perturbation Type

El tamaño de la perturbación está en el núcleo del ataque adversarial, una pequeña perturbación es la premisa fundamental de tales modelos. Al diseñar un ejemplo adversarial, el atacante quiere que la entrada perturbada sea lo más parecida posible a la original, en el caso de las imágenes, lo suficientemente parecida como para que un humano no pueda distinguir una imagen de la otra. Analizamos la perturbación en relación con el alcance, la limitación y la medida.

- **Alcance de la perturbación**: El atacante puede generar perturbaciones que sean específicas para cada entrada, en lo que llamamos individual, o puede generar una única perturbación que será efectiva para todas las entradas del conjunto de datos de entrenamiento, en lo que llamamos perturbación universal.
- **Limitación de la perturbación**: Dos opciones son posibles, la perturbación optimizada y la perturbación de restricción. La perturbación optimizada es el objetivo del problema de optimización, mientras que la perturbación de restricción es el conjunto como la restricción al problema de optimización.
- **Medida de la perturbación**: Es la métrica utilizada para medir la magnitud de la perturbación. La métrica más utilizada es la $l_{p}$-norma, con muchos algoritmos que aplican $l_{0}$, $l_{2}$, $l_{\infty}$ normas.

### 2.3 Defense Methods

Como se ve en la Figura 1 basada en [53], subdividimos las defensas contra ataques adversarios en 3 categorías principales: Gradient Masking/Obfuscation, Robust Optimization, y Adversarial Example Detection, que se describen como:

- **Enmascaramiento/ofuscación de gradiente**: El aspecto central de los mecanismos de defensa basados en el enmascaramiento de gradientes es la construcción de modelos con gradientes que no sean útiles para los atacantes. Los modelos con enmascaramiento/ofuscación de gradiente, en general, producen funciones de pérdida que son muy suaves en la vecindad de los datos de entrada. Esta suavidad en torno a los puntos de datos de entrenamiento dificulta que los algoritmos de explotación encuentren direcciones significativas hacia la generación de un ejemplo adversario.
- **Optimización robusta**: Es una estrategia de defensa que se compone de métodos que mejoran la función de optimización, ya sea añadiendo términos de regularización, límites de certificación, ejemplos adversarios en la función objetivo, o modificando el modelo para añadir incertidumbre en las capas del modelo.
- **Detección de ejemplos adversos**: Los trabajos recientes se han centrado en detectar ejemplos adversos en lugar de hacer que la DNN sea robusta frente a su creación. La detección de ejemplos adversos suele hacerse encontrando valores estadísticos atípicos o entrenando subredes separadas que puedan distinguir entre imágenes perturbadas y normales.

En cuanto a los mecanismos de defensa, centramos este estudio en métodos relacionados con la optimización robusta. Entre las diversas publicaciones de este estudio, cada autor tiene su propia representación y visión de la optimización robusta. En general, incluso con diferentes notaciones y representaciones, la mayoría de los artículos que hemos estudiado se ajustan a la representación general de la Optimización Robusta.

El objetivo de entrenamiento en un modelo DL es la minimización de la pérdida deseada. El objetivo es ajustar los parámetros del modelo con respecto a los datos etiquetados, como se ve en la ecuación 1,

<!-- \label{eq:1} -->
$$
\underset{\theta}{min} ~ \mathcal{L}(\theta, x, y)
$$

en la que $\theta$ son los parámetros del modelo, ${x}$ es la entrada al modelo, $\mathcal{L}$ es la función de pérdida definida, e y es su etiqueta verdadera. Con tal formulación, buscamos minimizar con $\theta$, la función de pérdida. Dicha formulación ajusta los parámetros a los puntos de datos, de forma que ${x}$ produce predicciones $\hat{y}$$ que son iguales a la etiqueta verdadera ${y}$. En un entorno adversarial este escenario cambia, en el que el objetivo es diferente,

<!-- \label{eq:2.0} -->
$$
\underset{\delta \leq \Delta }{max}~\mathcal{L}(\theta, x+\delta,y)
$$

que estamos buscando una perturbación $\delta$, menor que una perturbación máxima $\Delta$ capaz de cambiar la decisión del clasificador de la predicción y, a y. La restricción en 3 la perturbación es un parámetro de diseño que se define en general por la $l_{p}$-norma.

Las ecuaciones 1 y 2 no incorporan la distribución de los datos ni las restricciones que se derivan del hecho de que la mayoría de los conjuntos de datos de entrenamiento no incorporan la distribución verdadera de los datos en los que los modelos realizarán la inferencia. Basándonos en la definición de [54], tenemos que, si D es la verdadera distribución de los datos, un conjunto de entrenamiento se extrae i.i.d. de ${\mathbb{D}}$, y se define como ${ D = \{ (x_{i}, y_{i}) \sim~\mathbb{D} \} }$, para ${i = 1,...,m}$. Y el riesgo empírico de un clasificador, que se basa en el conjunto de entrenamiento, se define como:
<!-- \label{eq:2.1} -->

$$
R(F,D) = \frac{1}{\lvert D \rvert}\sum_{(x,y)\in D} \mathcal{L}(f(x),y)
$$

en el que $\left\lvert D \right\rvert$ es el tamaño del conjunto de entrenamiento ${D}$. Con esta definición, el riesgo adversarial empírico se define en:

<!-- \label{eq:2.2} -->
$$
R_{adv}(F,D) = \frac{1}{\lvert D \rvert}\sum_{(x,y)\in D} \mathcal{L}(f(x+\delta),y)
$$

Cuando se trata de defensas adversariales en las lentes de Optimización Robusta, la primera solución, es resolver la pérdida combinada del peor caso, con el riesgo adversarial empírico Radv, conocido como entrenamiento adversarial.

<!-- \label{eq:3} -->
$$
\underset{\theta}{min} \frac{1}{\lvert D \rvert}\sum_{(x,y)\in D}  \underset{\delta \in \Delta}{max} \mathcal{L}(f(x+\delta),y)
$$

La solución de la Ecuación 3, requiere un manejo especial o una formulación completamente diferente que definen cómo categorizamos los mecanismos de defensa para ataques adversariales, a saber: (re)Entrenamiento Adversarial, Enfoque Bayesiano, Enfoque de Regularización y Defensas Certificadas.

#### 2.3.1 Adversarial (re)Training as a Defense Mechanism

La solución de la Ecuación 3 requiere resolver la maximización interna (Ecuación 2), que es un problema de optimización no convexo de alta dimensión prohibitivamente difícil de resolver exactamente mediante técnicas de optimización estándar. El enfoque más popular para resolver un problema de este tipo es aproximar la Ecuación 2 con el uso de una heurística en la que se busca un límite inferior para la Ecuación 2. Aunque son prometedores y han demostrado mejorar la robustez incluso para modelos grandes (ImageNet [55]), estos modelos tienen el inconveniente de que cuando se instancian en la práctica con la heurística de aproximación, no son capaces de proporcionar garantías o certificaciones de robustez. Esta clase de defensas aunque muy prácticas de implementar no pueden proporcionar una garantía de que no exista ningún ejemplo adversario en la vecindad de x capaz de engañar a ${f(.)}$.

#### 2.3.2 Certified Defenses

El problema enunciado en la Ecuación 3, define el objetivo general del entrenamiento adversario. Pero como se ha visto, incluso con los mejores métodos para encontrar una aproximación local al problema de maximización, estamos subjetivos a la eficacia del método de ataque. En la literatura se ha propuesto una forma de evitar este inconveniente, que consiste en resolver exactamente el problema de maximización o aproximarse a un conjunto de restricciones solucionable. Para definir formalmente las Defensas Certificadas, inicialmente, consideramos un modelo de amenaza en el que se permite al adversario transformar una entrada ${x \in \mathbb{R}^{d}}$ en cualquier punto de un conjunto ${\mathbb{S}_{0} (x) \subseteq R^{d}}$. Dicho conjunto representa la vecindad del punto ${x}$ generada por perturbaciones ${l_{p}}$$, transformaciones geométricas, perturbaciones semánticas u otro tipo de transformación en ${x}$. En el caso de una perturbación ${l_{p}}$, el conjunto se define como ${ \mathbb{S}_0 = \left\{ x^{\prime} \in \mathbb{R}^{d}, {\left\lVert x - x^{\prime} \right\rVert}_{p} < \epsilon \right\}}$.

El problema enunciado en la Ecuación 3, define el objetivo general del entrenamiento adversario. Pero como se ha visto, incluso con los mejores métodos para encontrar una aproximación local al problema de maximización, estamos subjetivos a la eficacia del método de ataque. En la literatura se ha propuesto una forma de evitar este inconveniente, que consiste en resolver exactamente el problema de maximización o aproximarse a un conjunto de restricciones solucionable. Para definir formalmente las Defensas Certificadas, inicialmente, consideramos un modelo de amenaza en el que se permite al adversario transformar una entrada ${x \in \mathbb{R}^{d}}$ en cualquier punto de un conjunto ${\mathbb{S}_{0} (x) \subseteq R^{d}}$. Dicho conjunto representa la vecindad del punto ${x}$ generada por perturbaciones ${l_{p}}$$, transformaciones geométricas, perturbaciones semánticas u otro tipo de transformación en ${x}$. En el caso de una perturbación ${l_{p}}$, el conjunto se define como ${ \mathbb{S}_0 = \left\{ x^{\\prime} \in \mathbb{R}^{d}, {\left\lVert x - x^{\\prime} \right\rVert}_{p} < \epsilon \right\}}$.

Seguimos ampliando el modelo ${f(.)}$ como función de sus ${k}$ capas ocultas y parámetros ${\theta}$, donde

<!-- \label{eq:4} -->
$$
f(x) = f_{\theta}^{k} \circ f_{\theta}^{k-1} \circ \cdots  \circ f_{\theta}^{1}
$$

en la que ${f_{\theta}^{i} : \mathbb{R}^{d_{i}-1} \to \mathbb{R}^{d_{i}}}$ denota la transformación no lineal aplicada en la capa oculta ${i}$. El objetivo es probar una propiedad en la salida de la red neuronal, codificada mediante una restricción lineal:

$$
c^{T} f_{\theta}(x^{\prime}) + d < 0 ~ \forall x^{\prime} \in \mathbb{S}_{0}(x)
$$

en la que ${c}$ y ${d}$ son valores vectoriales y escalares específicos de la propiedad.

Para entender la complejidad de la certificación, basándonos en la ecuación 4, definimos el objetivo de optimización adversarial por capas. Para ${z_{1} = x, x_{i+1} = f_{i}(W_{i} z_{i} + b_{i})}$

<!-- $$
\begin{split}
\underset{z_{1},\cdots, d+1}{\text{max}}   & (e_{y}-e_{y_{t~arg}})^{T} ~z_{d+1}    \\
\text{s.t.}                                & z_{1}^{\prime} \in \mathbb{S}_{0}     \\
                                           & z_{i+1} = f_{i}(W_{i} z_{i} + b_{i}), i = 1, \cdots, d - 1   \\
                                           & z_{d+1} = W_{d} z_{d} + b_{d}
\end{split}
$$ -->

en la que ${e_{i}}$ base unitaria, vectores con valor 1 en la posición de la clase ${i^{th}}$ y ceros en todas las demás. Dicha formulación requiere un manejo especial dado que tenemos una restricción no lineal definida por la función de activación. Se han propuesto varias técnicas para resolver tal problema y están dentro del ámbito de estudio de este estudio. El método para entrenar redes neuronales certificadas se basa en el cálculo de un límite superior para la pérdida interna, en contraposición a un límite inferior calculado para el entrenamiento adversarial. Estos métodos suelen denominarse defensas demostrables, ya que proporcionan garantías sobre la robustez de la red resultante, bajo cualquier tipo de ataque dentro del modelo de amenaza. Los métodos típicos para calcular las certificaciones se basan en relajaciones convexas, propagación de intervalos, solucionadores SMT, interpretación abstracta, programas lineales de enteros mixtos, relajaciones lineales o combinaciones de estos métodos. Exploramos las diversas técnicas en la subsección 4.3.

#### 2.3.3 Regularization Approach

Las técnicas de regularización se centran en introducir pequeñas modificaciones en el algoritmo de aprendizaje para que pueda generalizar mejor. En cierto modo, mejora el rendimiento del modelo en datos no vistos. Evita que el modelo se ajuste en exceso a los datos con ruido, penalizando las matrices de pesos de los nodos. En el caso concreto de la Optimización Robusta, el objetivo de las técnicas de regularización es similar, pero centrándose en evitar que pequeñas variaciones en la entrada, puedan generar cambios en la decisión del algoritmo. Para ello, amplía los límites de decisión o limita los cambios en el gradiente del modelo.

Se han propuesto muchas técnicas de regularización, siendo las más utilizadas las basadas en ${l_{p}}$. La técnica de regularización ${l_{2}}$$ se introduce para reducir el valor de los parámetros, lo que se traduce en una reducción de la varianza. Introduce un término de penalización a la función objetivo original (Pérdida), añadiendo la suma ponderada de los parámetros al cuadrado del modelo. Con ello, tenemos una pérdida regularizada LR definida como:

$$
\mathcal{L}_{R}(x + \delta, y) = \mathcal{L}(x + \delta, y) + \lambda {\lVert \theta \rVert}_{2}
$$

en el que un ${\lambda}$ pequeño permite que los parámetros crezcan sin control, mientras que un ${\lambda}$ grande fomenta la reducción de los parámetros del modelo. Los métodos de regularización no se limitan a los enfoques ${L_{p}}$ y pueden incluir la regularización de Lipschitz, la matriz jacobiana y otras técnicas que analizamos en la subsección 4.2.

## 3 METHODS FOR GENERATING ADVERSARIAL ATTACKS

El estudio de los ataques de adversarios en el ámbito de la clasificación de imágenes mejora nuestros conocimientos, ya que podemos analizar visualmente las desemejanzas entre las entradas perturbadas y las no perturbadas. Además, los datos de imágenes, a pesar de su alta dimensionalidad, tienen una representación más sencilla que otros dominios como el audio, los gráficos y los datos de ciberseguridad. A lo largo de esta sección, revisaremos los algoritmos generadores de ataques en el dominio de clasificación de imágenes que pueden ser aplicados a redes neuronales profundas (DNN) y redes neuronales convolucionales (CNN) estándar. En los que clasificamos en: caja blanca, caja negra, y ataques aplicados al mundo real. En la Tabla 2 resumimos todos los ataques descritos en esta sección, destacando la métrica de distancia utilizada, el nivel de acceso a la información, el tipo de algoritmo y el dominio en el que se aplicó en la publicación específica.

En las siguientes subsecciones, enumeramos y describimos los enfoques más populares para el ataque adversarial en modelos ML. Los enumeramos en orden cronológico y nos centramos en dar los detalles más importantes sobre estos métodos.

### 3.1 White-box Attacks

Como se ha dicho, en los ataques de caja blanca no hay restricciones en el nivel de información al que puede acceder el atacante. En consecuencia, el adversario conoce los parámetros del modelo, el conjunto de datos o cualquier otra información relativa al modelo. Bajo este supuesto, dado un modelo ${f(.)}$, una entrada ${(x, y)}$, el objetivo principal es producir ${x^{\prime}}$, que está dentro de cierta distancia de la original ${x}$ y maximiza la pérdida ${\mathcal{L}(f(x+\delta), y)}$.

$$
\underset{\delta\in\Delta}{\max} ~\mathcal{L}(f(x+\delta), y)
$$

#### 3.1.1 Box-Constrained L-BFGS

En [43], se demostró por primera vez la existencia de pequeñas perturbaciones capaces de engañar a un clasificador. En el trabajo, Szegedy et. al. propusieron calcular un ruido aditivo $\delta$, que podría añadirse a la entrada original ${x}$, capaz de engañar al clasificador pero con una distorsión mínima o no perceptible en la imagen. Encontramos el delta óptimo, ${\delta}$, con:

<!-- \label{eq:6} -->
$$
\underset{\delta}{\text{min}} ~ c{\lVert \delta \rVert}_{2} \\
\text{s.t.} ~ f(x + \delta) = y^{\prime} ~\text{all pixel in} (x + \delta) \in [0,1]
$$

en el que ${f(.)}$ es el modelo DNN parametrizado, ${y}$ es la etiqueta verdadera, ${y^{\prime}}$ es la etiqueta objetivo. Este es un problema difícil. Utilizando un L-BFGS con restricciones de caja, los autores propusieron una solución aproximada al problema planteado como:

<!-- \label{eq:6} -->

$$
\underset{\delta}{\text{min}}~~~c{\lVert \delta \rVert}_{2} + \mathcal{L}(f(x + \delta), y^{\prime}) \\
\text{s.t.} ~ \text{all pixel in} (x + \delta) \in [0,1]
$$

En este modelo, Szegedy et. al., consiguieron generar imágenes que eran visualmente indistinguibles de las originales, pero fueron capaces de engañar a los clasificadores para que las clasificaran como de otra clase. Este fue el primer resultado y publicación que ha explotado esta debilidad de los modelos de aprendizaje profundo.

#### 3.1.2 Fast Sign Gradient Method

En, [56], se introdujo un marco de ataque adversarial de un paso. La imagen de ataque, ${x^{\prime}}$, se obtiene mediante una simple perturbación aditiva:

$$
x^{\prime} = x + \delta_{ut} \\
x^{\prime} = x - \delta_{tg}
$$

en la que para el ajuste sin objetivo, $\delta{ut}$, obtenemos la perturbación de:

$$
\underset{{\lVert \delta_{ut} \rVert}_{p} \leq \epsilon}{\max} ~ \mathcal{L}(f(x+\delta), y)
$$

y en el escenario objetivo, $\delta_{tg}$, de:

$$
\underset{{\lVert \delta_{tg} \rVert}_{p} \leq \epsilon}{\max} ~ (\mathcal{L}(f(x+\delta), y) - \mathcal{L}(f(x+\delta), y^{\prime}))
$$

en la que ${\epsilon}$ es la bola definida normalmente por una ${l_{p}}$-norma.

El núcleo del método de gradiente de signo rápido maximiza la norma del vector entre la clase etiquetada originalmente y la etiqueta asignada actualmente, mientras que en el entorno objetivo, se centra en minimizar la distancia a la clase objetivo. Como se trata de un algoritmo de un solo paso, no es muy resistente a las defensas actuales, pero su aplicación es muy rápida. La figura 3 muestra el ataque a una imagen y la falsa predicción.

#### 3.1.3 DeepFool

El ataque Deepfool propuesto por [57], es un ataque de caja blanca que explora los límites del modelo de clasificación. En el algoritmo multiclase, Deepfool se inicializa con una entrada ${x}$ que se supone que está dentro de los límites del modelo clasificador ${f(x)}$. Mediante un proceso iterativo, la imagen es perturbada por un pequeño vector en la dirección de los límites de decisión. Los límites se aproximan mediante funciones lineales, más concretamente un hiperplano, definido en el algoritmo como ${\hat{l}}$. En cada paso, las perturbaciones se acumulan para formar la perturbación final de la imagen. Con perturbaciones más pequeñas que en FGSM ([56]), los autores han demostrado una tasa de éxito de los ataques similar o mejor.

#### 3.1.4 Jacobian-based Saliency Map Attack

El ataque de Mapa de Saliencia basado en Jacobiano (JSMA) difiere de la mayoría de la literatura de ataques adversarios con respecto a la norma que utiliza en la restricción de perturbación. Mientras que la mayoría de los ataques se centran en las normas ${l_{\infty}}$ o ${l_{2}}$$, JSMA, propuesto en [58], se centra en la norma l0. Bajo esta norma, penaliza el cambio de forma binaria, si el píxel ha sido cambiado o no, a diferencia de los algoritmos basados en ${l_{2}}$$ que tienen en cuenta el tamaño del cambio en los píxeles.

En este ataque, Papernot et. al., calcula el Jacobiano de una matriz de puntuación ${F}$. El modelo ejecuta el ataque de forma codiciosa. Modifica el píxel que tiene mayor impacto en la decisión del modelo. La matriz Jacobiana se define como:

$$
J_{F} (x) = \frac{\partial F(x)}{\partial(x)} = \left\{ \frac{\partial F_{j}(x)}{\partial x_{i}}  \right\} x \times j
$$

modela la influencia de los cambios en la entrada ${x}$ en la etiqueta ${hat{y}}$ predicha. El algoritmo modifica uno a uno los píxeles de la imagen no alterada para crear un mapa de saliencia. La idea principal del mapa de saliencia es la correlación entre el gradiente de la salida y la entrada. Es una guía de las variables más influyentes de la entrada, o las que probablemente pueden engañar al clasificador con menos manipulación. A partir de ahí, el algoritmo realiza modificaciones en el píxel más influyente.

### 3.1.5 Projected Gradient Descend (PGD)

También conocido como método iterativo básico, se propuso inicialmente en [59]. Se basa en el FGSM, pero en lugar de un solo paso del descenso del gradiente proyectado, itera a través de más pasos, como en:

$$
\delta \coloneqq P(\delta + \alpha \nabla_{\delta} \mathcal{L}(f(x+\delta), y))
$$

en la que ${P}$ denota la proyección sobre la bola de interés. Con tal formulación, el PGD, requiere un ajuste más fino, en la elección del tamaño del paso ${\alpha}$. En [54], Madry et. al. propusieron un método iterativo con una inicialización aleatoria para ${\delta}$.

#### 3.1.6 Carlini and Wagner Attacks (CW)

En [60], se propusieron 3 ataques ${l_{p}}$-normales ${(l_{0}, l_{2}, l_{\infty})}$ como respuesta a [61], que proponía el uso de la Destilación como estrategia de defensa. En su trabajo, Papernot et. al., presentaron con éxito un mecanismo de defensa capaz de reducir la eficacia del FGSM y L-BFGS. CW propone resolver el mismo problema planteado en FGSM, que es dada una entrada ${x}$ encontrar una perturbación mínima ${\delta}$ capaz de desplazar la predicción de clasificación del modelo. El problema se aborda como:

<!-- \label{eq:8} -->
$$
\underset{\delta}{\min} ~ c{\lVert \delta \rVert}_{p} + \mathcal{L}(f(x + \delta), y^{\prime}) \\
$$

$$
\text*{s.t.} ~ (x + \delta) \in {\left[0, 1\right]}^{n}
$$

en la que ${\mathcal{L}(f(x + \delta), y^{\prime}) = \max_{i \neq j}(Z(x^{\prime}_{i}) - Z(x^{\prime})_{y})^{+}}$, y ${Z(x) = z}$$ son los logits. Como el algoritmo minimiza la métrica ${\mathcal{L}(.)}$, encuentra la entrada ${x^{\prime}}$ que tiene mayor puntuación para ser clasificada como ${y^{\prime}}$. Al buscar el valor de ${c}$, buscamos la constante que produzca la menor distancia entre ${x}$ y ${x^{\prime}}$.

#### 3.1.7 Ground Truth Adversarial Example (GTAE)

Hasta ahora la mayoría de los ataques, aunque motivados por la generación de nuevas defensas, son independientes del algoritmo de defensa. En el algoritmo propuesto por [62], la defensa certificada propuesta en [63], se utiliza como base para la optimización y búsqueda de ejemplos adversarios.

El algoritmo abstrae la ${\theta}$ y el conjunto de datos ${(x, y)}$ con el uso de un solucionador SMT, y resuelve el sistema para comprobar si existen ${x^{\prime}}$ cerca de ${x}$, dentro de la distancia de norma establecida, que puedan causar una clasificación errónea. El ejemplo adversario verdadero se encuentra reduciendo el tamaño de ${\epsilon}$ hasta el punto en que el sistema ya no puede encontrar un ${x}$ 0 adecuado. El ejemplo adversario se considera el ${x^{\prime}}$ encontrado con el ${\epsilon}$ inmediatamente mayor. Es el primer método que calcula una solución exacta demostrable para una perturbación mínima que puede engañar a los modelos ML. En cambio, como afirman los autores, el hecho de que el modelo dependa de un solucionador SMT, restringe la aplicabilidad del algoritmo a modelos con no más de unos cientos de nodos. Este ataque ha sido revisado por [64] y [65].


#### 3.1.8 Universal Adversarial Perturbations 

A diferencia de los métodos anteriores, la perturbación adversarial universal (UAP), propuesta en [66], busca una única perturbación capaz de engañar a todas las muestras del conjunto de datos de entrenamiento. Las perturbaciones, independientes de la entrada, también están restringidas a no ser detectadas por humanos. Las perturbaciones se construyen basándose en:

$$
P_{x \sim D} \left( f(x) \neq f(x+\delta) \right) \geq \beta, \text{s.t.} {\lVert \delta \rVert}_{p} \leq \epsilon
$$

en el que ${\epsilon}$ define el tamaño de la perturbación basada en una ${l_{p}}$-norma y ${\beta}$ define la probabilidad de que una imagen muestreada del conjunto de datos de entrenamiento sea engañada por la perturbación generada. En este caso, el algoritmo optimiza la probabilidad de engañar al clasificador. El método para calcular las perturbaciones universales se basa en el algoritmo DeepFool, en el que la entrada se empuja gradualmente hacia el límite de decisión del modelo. Se diferencia de DeepFool en el hecho de que en lugar de empujar una sola entrada, todos los miembros de ${\mathcal{D}}$ se modifican en la dirección de la frontera de decisión. Las perturbaciones, calculadas para cada imagen, se acumulan de forma gradual. A continuación, el acumulador se proyecta de nuevo hacia la bola ${B_{\epsilon}}$ especificada, de radio ${\epsilon}$. Se ha demostrado que, con variaciones del 4%, se alcanza una precisión de engaño del 80%.

#### 3.1.9 Shadow Attack

En [67], se propuso un ataque dirigido a defensas certificadas. En su trabajo, se dirigen a defensas que certifican el modelo con respecto a un radio definido por la ${l_{p}}$-norma. Una idea intuitiva para construir una defensa certificada es comprobar dentro de un cierto radio ${B_{\epsilon}}$ de entrada, la existencia de una perturbación ${\delta}$, capaz de cambiar la decisión del clasificador. El ataque sombra se construye para aprovechar esta premisa, y construir una perturbación fuera de la zona de certificación. Se afirma que después de etiquetar una imagen, estas defensas comprueban si existe una imagen de una etiqueta diferente a menos de ${\epsilon}$ de distancia (en ${l_{p}}$ métrica) de la entrada, donde ${\epsilon}$ es un parámetro de seguridad seleccionado por el usuario. Si dentro de la bola ${B_{\epsilon}}$ todas las entradas se clasifican con la misma etiqueta, entonces el modelo se certifica de forma robusta. Su modelo tiene como objetivo no sólo el clasificador, sino también el certificado. Se hace añadiendo perturbaciones adversarias a imágenes que son grandes en la ${l_{p}}$-norma y producen imágenes de ataque que están rodeadas por una gran bola que contiene exclusivamente imágenes de la misma etiqueta. Con el fin de producir imágenes que están cerca de la original, de una manera percepción, pero puede engañar al clasificador, que utilizan la siguiente función objetivo:

$$
\underset{y^{\prime} \neq y, \delta}{\max} ~ -\mathcal{L}(\theta, x + \delta \vert y^{\prime}) - \lambda_{c}C(\delta)
$$

$$
-\lambda_{tv} TV (\delta) - \lambda_{S}Dissim(\delta)
$$

en el que ${\mathcal{L}(\theta, x+\delta \vert \bar{y})}$ se refiere a la pérdida de entrenamiento adversaria, ${\lambda_{c}C(\delta)}$ es un término de regularización del color, es un término de penalización por suavidad, y ${\lambda_{S}Dissim(\delta)}$ garantiza que todos los canales de color reciben una perturbación similar.

#### 3.1.10 Other Attacks

Los ataques presentados son sólo una parte de muchos más que se han publicado en muchos lugares diferentes. A continuación enumeramos otros métodos de ataque relevantes disponibles en la literatura.

* **EAD: Elastic-net attack** - Similar a L-BFGS el algoritmo en [68] propone encontrar la perturbación aditiva mínima, que engaña al clasificador. Diferentemente incorpora una asociación de las normas ${l_{1}}$ y ${l_{1}}$. Se ha demostrado que las defensas fuertes contra las normas ${l_{\infty}}$ y ${l_{1}}$$ siguen fallando a la hora de rechazar los ataques basados en ${l_{1}}$.
* **Objetivo Métrico y Algoritmo de Descenso Gradiente (OMGDA)** - El algoritmo propuesto por [69], es muy similar a DeepFool, con la optimización del tamaño del paso. En lugar de utilizar un tamaño de paso fijo y heurísticamente determinado en la optimización, en Jang et al., el tamaño de paso utiliza conocimientos de la capa softmax. El tamaño del paso se determina en función del tamaño de la perturbación deseada y varía con el tiempo.
* **Ataque por Transformación Espacial (STA)** - En [70], en lugar de generar cambios en la intensidad de los píxeles, los autores han propuesto un método basado en pequeñas perturbaciones traslacionales y rotacionales. Las perturbaciones siguen siendo imperceptibles para el ojo humano. De forma similar, en [71] también se explota el aspecto espacial de la entrada para la generación de ejemplos adversarios.
*  **Ejemplos Adversarios No Restringidos con Modelos Generativos (UAEGM)** - Basado en AC-GAN ([72]), [73], ha propuesto el uso de redes generativas para generar ejemplos que no están restringidos a estar en la vecindad de los datos de entrada. Los ataques generados no son necesariamente similares a los del conjunto de datos, pero son lo suficientemente parecidos como para que los humanos no se den cuenta y engañen a los clasificadores.

### 3.2 Black-Box Attacks

Bajo la restricción de caja negra, los modelos son diferentes de la caja blanca actualmente expuesta, con respecto a la información a la que tiene acceso el atacante. En la mayoría de los casos, el adversario no tiene ninguna o parte de la información sobre el modelo objetivo, como el algoritmo utilizado, el conjunto de datos o los parámetros, como se ve en la figura 2. Un importante reto de modelado para los ataques de caja negra es modelar con precisión qué información tiene el atacante sobre el modelo aprendido o el algoritmo. En esta subsección, enumeramos los métodos más relevantes para la generación de ataques de caja negra.

#### 3.2.1 Practical Black-Box Attacks (PBBA) 

Mientras que asumir el acceso a la información del modelo permite una serie de ataques, el trabajo en [74], introduce la posibilidad de atacar modelos sobre los que el atacante tiene menos conocimiento. En este trabajo, no se asume ningún conocimiento sobre la arquitectura, sólo alguna idea del dominio de interés. También se limitan las peticiones de salida enviadas al modelo, lo que obliga al atacante a elegir sabiamente las peticiones de inferencia del modelo de la víctima. Para lograr el objetivo, Papernot et. al. introducen el marco del modelo sustituto. La estrategia de ataque consiste en entrenar una red sustituta con un pequeño número de consultas iniciales y, a continuación, perturbar iterativamente las entradas basándose en la información del gradiente de la red sustituta para aumentar el conjunto de entrenamiento.

```

```

Con los límites del modelo sustitutivo ajustados para que se aproximen al modelo original, se puede utilizar cualquiera de los métodos presentados en la sección anterior para generar la imagen perturbada.

#### 3.2.2 Zeroth Order Optimization Based Attack

En Chen et al. [52], en el ataque también conocido como ZOO, los autores asumen accesibilidad tanto a los datos de entrada como a las puntuaciones de confianza del modelo en el que dirigen su ataque. Difieren de [74] en el hecho de que el modelo no se centra en la transferibilidad (creación de un modelo sustituto) para conseguir los ejemplos adversarios. En su trabajo, proponen un ataque de optimización de orden zeroth que estima el gradiente de la DNN objetivo. En lugar del descenso de gradiente tradicional, utilizan SGD de coordenadas de orden 0. Además, para mejorar su modelo y permitir la generación de ejemplos adversos, implementan reducción de dimensionalidad, técnicas jerárquicas y muestreo de importancia. A medida que se ajustan los píxeles, el algoritmo observa los cambios en las puntuaciones de confianza.

Similar al ZOO, en el ataque de un solo píxel [75], se propone el uso de la confianza de la puntuación para perturbar la entrada y cambiar la decisión del clasificador. Este trabajo se centra en la modificación de un único píxel de la entrada. Con el uso de la evolución diferencial, el único píxel se modifica en un entorno de caja negra. Los autores basan la actualización de la perturbación en la variación de las puntuaciones de probabilidad para cada clase.

#### 3.2.3 Query-Efficient Black-Box Attacks

Uno de los mayores retos en los ataques de caja negra es el hecho de que muchos modelos de inferencia tienen mecanismos para restringir el número de consultas (cuando están basados en la nube o embebidos en el sistema), o el tiempo de inferencia puede restringir el número de consultas. Por ejemplo, el trabajo de [76], basado en estrategias de evolución natural, reduce en 2 o 3 órdenes de magnitud la cantidad de peticiones de información enviadas al modelo para generar con éxito una imagen perturbada mal clasificada. El algoritmo establece consultas en la vecindad de la entrada ${x}$. A continuación, se muestrea la salida del modelo, y estas muestras se utilizan para estimar la expectativa del gradiente alrededor del punto de interés.

El algoritmo muestrea la salida del modelo basándose en las consultas alrededor de la entrada x, y estima la expectativa de gradiente de ${F}$ sobre ${x}$. Más sobre el tema, [77], propone una familia de algoritmos basados en una nueva estimación de la dirección del gradiente utilizando únicamente la clasificación binaria del modelo. En su trabajo, se incluyen ${l_{\infty}}$ y ataques basados en la norma ${l_{2}}$, así como ataques dirigidos y no dirigidos. La figura 4 muestra una intuición de cómo se actualiza el gradiente y se utilizan los límites de la decisión para generar el ataque adversarial. El algoritmo 2 muestra cómo se define la reducción de la dimensionalidad ${d^{r}}$. Por otra parte, en la búsqueda de un ataque de caja negra eficiente para la consulta, [78] introduce un método independiente del gradiente basado en la optimización bayesiana y en modelos sustitutos de procesos gaussianos para encontrar ejemplos adversariales efectivos. En el modelo se asume que el atacante no tiene conocimiento de la arquitectura de la red, los pesos, el gradiente o los datos de entrenamiento del modelo objetivo. Pero se supone que el atacante puede consultar el modelo con la entrada ${x}$, para obtener las puntuaciones de predicción en todas las clases ${C}$. Restringen la perturbación a la norma ${l_{\infty}}$. Su objetivo es maximizar las perturbaciones:


<!-- \label{eq:10} -->
$$
\delta^{*} = \underset{\delta}{\arg\min}\left[\log\left(f(x_{origin}+g(\delta))_{t}\right)-\log(\sum_{j\neq{}t}^{C}{f(x_{origin} + g(\delta))_{j}})\right]
$$

$$
\text{s.t.} ~ \delta \in \left[-\delta_{max}, +\delta_{\max}\right]^{d_{r}}
$$

La optimización bayesiana propuesta para mejorar la eficiencia de la consulta requiere el uso de un modelo sustituto para aproximar la función objetivo, en su trabajo se utiliza un Proceso Gaussiano. Además para definir el siguiente punto de consulta se define mediante una función de adquisición. Un gran diferencial en su trabajo es el hecho de que en lugar de buscar en un espacio de alta dimensionalidad una perturbación ${\delta}$, utilizan una función para reducir la dimensionalidad de la perturbación y posteriormente reconstituir al tamaño real de la imagen.

```

```

#### 3.2.4 Attack on RL algorithm

En [79], se propuso un método para generar ejemplos adversarios en algoritmos de aprendizaje por refuerzo (RL). En RL, un ejemplo adversario puede ser una imagen modificada utilizada para capturar un estado o, en el caso de esta publicación, una política adversaria. Es importante destacar que una política adversaria no es un adversario fuerte como los que tenemos en los juegos de dos jugadores, sino uno que con un determinado comportamiento provoca un fallo en la política de la víctima. En este trabajo se propone un ataque de caja negra para desencadenar malos comportamientos en la política de la víctima. La política de la víctima se entrena usando Optimización de Política Próxima y aprende a "jugar" contra un adversario justo. La política adversaria se entrena para desencadenar fallos en la política de la víctima. La figura 5 muestra la diferencia entre una política del adversario y una política manipulada por el adversario. También en el artículo se muestra la dependencia del tamaño del espacio de entrada y la eficacia de las políticas adversarias. Cuanto mayor es la dimensionalidad del espacio de observación bajo el control del adversario, más vulnerable

### 3.3 Physical World Attack 

Las investigaciones presentadas hasta ahora se centran principalmente en la aplicación de los ataques en aplicaciones virtuales y conjuntos de datos controlados, pero la gran preocupación sobre la existencia de ejemplos adversariales es hasta qué punto pueden implicar graves consecuencias para los usuarios del sistema. Con ese objetivo, dedicamos esta sesión a explorar publicaciones con aplicaciones en el mundo real y consecuencias claramente expuestas.

En [80], se atacaron físicamente señales de tráfico colocando una pegatina en una posición específica de la señal. Su ataque consistía en encontrar inicialmente, en una imagen de la señal, la localización con mayor influencia para la decisión del clasificador. Para ese objetivo, se utilizó una l1norma porque produce perturbaciones dispersas en la imagen, lo que facilita la localización de los parches de modificación. A partir de la localización definida, se utilizó una ${l_{2}norm}$ para identificar el color más apropiado para la pegatina.

Además, como el reconocimiento facial se está haciendo muy popular como medida de seguridad biométrica, es objeto de varios ataques adversarios. Destacamos 4 ataques en el reconocimiento e identificación de caras.

* **Evaluación de la robustez de modelos DNN para el Reconocimiento de Rostros frente a ataques adversarios** - En esta publicación Goswami et al. [81] evalúan cómo la profundidad de la arquitectura impacta en la robustez del modelo en la identificación de rostros. Evalúan la robustez con respecto a escenarios adversariales teniendo en cuenta distorsiones que normalmente se observan en una escena común. Estas distorsiones son manejadas con facilidad por redes poco profundas, al contrario que las redes profundas. En su enfoque, han utilizado redes Open-Face y VGG-Face, y han logrado una alta tasa de engaño. Es importante señalar que en su ataque no se restringe la visibilidad de las perturbaciones.
* **Adversarial Attacks on Face Detectors using Neural Net based Constrained Optimization** En este trabajo, también centrado en evitar la identificación de caras, [82] ha generado un ataque, basado en el ataque de Carlini y Wagner, que fue capaz de engañar a R-CNN. Sus perturbaciones no son visibles en el ejemplo adversarial.
* **Generación de Ejemplos Adversarios mediante Ataques de Maquillaje en el Reconocimiento Facial** - En esta investigación, [83] implementan una red GAN para generar perturbaciones de maquillaje. Cuando la perturbación se aplica a la cara, el clasificador cambia su decisión a la clase objetivo.
* **Eficientes ataques adversariales de caja negra basados en decisiones para el reconocimiento de caras** - Dong et al. [84] proponen un método de ataque evolutivo. El generador de ejemplos adversarios propuesto está restringido en un entorno de caja negra. El algoritmo se centra en reducir el número de dimensiones del espacio de búsqueda modelando la geometría local de los vectores de búsqueda. Se ha demostrado que este algoritmo es aplicable a la mayoría de las tareas de reconocimiento.

#### 3.3.1 Other Attacks 

En el campo de la ciberseguridad los modelos de aprendizaje automático se aplican, en general, para detectar malware, conexiones maliciosas, clasificador de dominios maliciosos y otros. En Suciu et al. [85] se realiza una evaluación de la robustez de los modelos actuales de detección de malware. Los autores vuelven a entrenar el modelo en un conjunto de datos a escala de producción para realizar la evaluación. Con los nuevos datos, el modelo que antes era vulnerable a los ataques demostró ser más sólido y se detectaron debilidades arquitectónicas. El trabajo de Suciu et al. [85], explora cómo se transfieren los ataques en el dominio de la ciberseguridad, y principalmente el compromiso inherente entre efectividad y transferibilidad. Con respecto a la conexión y el dominio maliciosos, Chernikova et al. [86] construyen un modelo que tiene en cuenta las dependencias formales generadas por las operaciones normales aplicadas en el espacio de características. El modelo para generar ejemplos adversarios considera simultáneamente tanto las dependencias matemáticas como las restricciones del mundo real de dichas aplicaciones. El algoritmo se centra en determinar las características con mayor variabilidad y las que presentan una mayor correlación con dichas características. Esta búsqueda se realiza en cada iteración. Todas las características identificadas se modifican, pero con la restricción de preservar un límite superior en la variación máxima de las características. El límite superior respeta las limitaciones de la aplicación en el mundo físico.

En el ámbito ciberfísico, varias publicaciones demuestran la fragilidad de los modelos ML y la generación de ejemplos adversarios. En [87] se generaron ejemplos adversarios a un Robot Humanoide iCub. El ataque propuesto simplemente se extiende sobre los ataques de [88]. El principal aspecto a considerar en este trabajo es el hecho de que pone de manifiesto las elevadas consecuencias de los ejemplos adversariales en el proceso de decisión de aplicaciones críticas para la seguridad. Además, en coches autoconducidos se han derivado varios ataques como DARTS ([89]) y el trabajo de [90] que muestra el ataque a señales de tráfico, este último con experimentos reales. En un tipo de sensor diferente, el trabajo de [91] demuestra el ataque a un sensor LIDAR, en el que atacan la imagen de nube de puntos.

Además, en [92] se propuso una técnica novedosa para atacar algoritmos de seguimiento de objetos. En su enfoque, el cuadro delimitador se ataca en un solo fotograma, lo que es suficiente para engañar al algoritmo y generar un desplazamiento en la colocación del cuadro delimitador. Un ataque de este tipo sería fundamental para que los coches autoconducidos reconocieran la posición de obstáculos, otros vehículos y peatones en la carretera.
