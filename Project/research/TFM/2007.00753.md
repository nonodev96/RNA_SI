# Opportunities and Challenges in Deep Learning Adversarial Robustness: A Survey

Abstract—As we seek to deploy machine learning models beyond virtual and controlled domains, it is critical to analyze not only the accuracy or the fact that it works most of the time, but if such a model is truly robust and reliable. This paper studies strategies to implement adversary robustly trained algorithms towards guaranteeing safety in machine learning algorithms. We provide a taxonomy to classify adversarial attacks and defenses, formulate the Robust Optimization problem in a min-max setting, and divide it into 3 subcategories, namely: Adversarial (re)Training, Regularization Approach, and Certified Defenses. We survey the most recent and important results in adversarial example generation, defense mechanisms with adversarial (re)Training as their main defense against perturbations. We also survey mothods that add regularization terms which change the behavior of the gradient, making it harder for attackers to achieve their objective. Alternatively, we’ve surveyed methods which formally derive certificates of robustness by exactly solving the optimization problem or by approximations using upper or lower bounds. In addition we discuss the challenges faced by most of the recent algorithms presenting future research perspectives.

## Introduction

Deep Learning (DL) ([1]) models are changing the way we solve problems that have required many attempts from the most diverse fields of science. DL is an improvement over Artificial Intelligence (AI) Neural Networks (NN), in which more layers are stacked to grant a bigger level of abstraction and better reasoning over the data when compared to other Machine Learning (ML) algorithms ([2]). Since the raise of DL, supported in many cases by cloud environments [3], [4], [5], the base architecture and its variations have been applied in many scientific breakthroughs in the most diverse fields of knowledge, e.g. in predicting AMD disease progression ([6]), predicting DNA enhancers for gene expression programmes ([7]), elections and demographic analysis based on satellite images ([8]), filtering data for gravitational-wave signals ([9]). DL approach has also become one of the most used approaches for natural language processing ([10]) and speech recognition ([11]).

One of the most popular variations of DL architecture, Convolutional Neural Networks (CNN) have significantly boosted the performance of DL algorithms in computer vision (CV) applications ([12]), bringing it to several areas of CV such as, object detection ([13], [14]), action recognition [15], [16], pose estimation [17], [18], image segmentation [19], [20], and motion tracking [21]. Starting with ImageNet [22], proposed in 2012, the field of CNN’s have seen great improvement with super-human performance in specific tasks, providing solutions even to medical problems [23].

Fueled by the fact that new frameworks, libraries, and hardware resources are being improved and made available to the public and scientific community [24], [25], [26], Deep Neural networks (DNN) are being improved constantly and achieving new performance breakthroughs [27], [28], [29]. With the current maturity of DNN algorithms, its being applied in solving safety and security-critical problems [30], such as self-driving cars [31], [32], multi-agent aerial vehicle systems with face identification [33], robotics [34], [35], social engineering detection [36], network anomaly detection [37], deep packet inspection in networks [38]. DNN applications are already part of our day-to-day life (personal assistants [39], product recommendation [40], biometric identification [41]) and tend to occupy a bigger space as time passes.

As seen in many publications, DNN has been shown to have human-level accuracy even for significantly complex tasks such as playing games with no prior rule known, except the current frames [42]. In contrast to the aforementioned accuracy of DNN models, its been shown in earlier publications [43], [44], [45], that DNN models are susceptible to small input perturbations, in most cases imperceptible to the human eye. The results from this publications have shown the facility with which small additive targeted noise to the input image, makes models to misclassify objects which before could be identified with 99.99% confidence. More alarming is the fact that such models report high confidence in the predictions. Such perturbations, which can fool a trained model, are known as adversarial attacks. With such alarming consequences, the study of adversarial attacks and robustness against them became a great deal of research in recent years.

A considerably large number of research papers is now available concerning methods to identify adversarial attacks and defend from incursions against the model [46], [47]. One way of solving this issue is adding better intuition on the models, through explainability [48], but such models do not target the direct improvement of the model. On the other hand, several approaches have been published to generate models which are robust against adversarial attacks [49], the target of the researchers is to introduce in their models’ layers of robustness such that the models are not fooled by out of distribution examples, known or unknown attacks, targeted or untargeted attacks. Guaranteeing the accuracy of such models while safety is taken into consideration, is of utmost importance for system architects, mainly making them robust to the presence of adversarial attacks, noise, model misspecification, and uncertainty. This survey aims to bring together the recent advances in robustness for DNN’s, pointing the main research directions being followed recently to generate robust DNN models. We bring to light both applied and theoretical recent developments.

Inspired by [50], we analyze the robustness of DNN’s under the perspective of how adversarial examples can be generated, and how defenses can be formulated against such algorithms. In general words, we define the robustness against adversarial attacks problem as a dual optimization problem, in which the attackers try to maximize the loss while the defenses try to minimize the chance of a model being fooled by the attacker. In such formulation, current existing models based in non linear activation functions, introduce non-linear inequality constraints in the optimization, which generates an inherent trade off between exact solutions and scalability of the model. This trade-off comes in the form of either exact slow solutions through mixed-integer linear programming, or in approximations to the objective function which either, relies on the existing attack methods to provide a local heuristic estimation to the maximization function, or approximations of the bounds of the constraints or objective function to generate certification regions, in which no adversarial example exists.

More specifically this paper presents the following contributions:

1. We characterize defenses to adversarial attacks as a min-max optimization problem, investigating solutions involving heuristic approximation, exact solution, and upper/lower bound approximations to generate models robust to adversarial attacks
2. We investigate, analyze, and categorize the most recent and/or important approaches to generate adversarial examples, as they are the basis to generate strong defenses, through Adversarial (re)Training.
3. We investigate, analyze, and categorize the most recent and important approaches to generate defenses against adversarial attacks, providing a taxonomy, description of the methods, and the main results in such approaches.

We organize this survey in the following manner. In section 2 we describe taxonomies for both adversarial example generation and defenses. We classify the adversarial models concerning the time of the attack, information available to the attacker, objective, and the algorithm computation method. Moreover, we classify the perturbation type used by the attackers. We divide the defense methods into three categories, namely gradient-masking/obfuscation, robust optimization, and adversarial example detection. We focus this research in Robust Optimization, and further subdivide in 3 groups: Adversarial Training, Certified Defenses, and Regularization Approach. In section 3, we describe several relevant adversarial attacks, and summarize them in Table 2. In section 4 we describe the most relevant results in Robust Optimization, and provide a tree that maps these publications to the 3 sub-groups of Robust Optimization. In section 5 we discuss current challenges and opportunities in robust defenses.

## 2 TAXONOMY OF ADVERSARIAL ATTACKS AND DEFENSES

We keep a consistent notation set along with the survey, and for easiness of reading we summarize in Table 1 the most used notations and symbols which we will use along with this survey. For papers requiring some specific terms, we define them in the section in which they are presented.

### 2.1 Attack Threat Model

Several attempts have been made to categorize attacks on machine learning. We here distill the most important aspects which characterize adversarial examples generating models concerning their architecture. We focus on the aspects that are most relevant to the discussion of adversarial robustness. To that end, we classify the attacks concerning 3 timing, information, goals, and attack frequency following the proposed in [51]:

- **Timing**: A first crucial feature for modeling the adversarial attacks is when it occurs. To that end we have two possibilities, evasion and poisoning attacks. Evasion attacks are the ones in the time of inference and assume the model has already been trained. Poisoning attacks in general targets the data, and the training phase of the model.
- **Information**: Another feature of the attack references the information to which the attacker has access. In the white box context the adversary has full access to information concerning the model and the model itself, as opposed to black box setting, in which very few or no information is available. White box attacks refer to those in which the adversary can unrestrictedly query the model for any information, such as weights, gradient, model hyperparameters, prediction scores. Whereas in black-box attacks the adversary has limited or no information about these parameters, although may obtain some of the information indirectly, for example, through queries. Some also define grey-box attacks, in which attackers might only know the feature representation and the type of model that was used but have no access to dataset or the model information. A fourth setting is called restricted black-box, or also known as no-box attack. Under such an assumption, no information is available to the attacker, and the research is mainly focused on attack transferability. In wich the focus is to evaluate the possibility of transferring the attack performed in one DNN to the inaccessible objective model [52]. In this work, we evaluate models in a binary setting, the adversary either has comprehensive access to the DNN or black box having limited access through queries, which can also provide class scores.
- **Goals**: The attackers may have different reasons to target a specific algorithm. But mostly the attacker has either a specific goal, and needs the algorithm to output a specific output, case in which it is a targeted attack, or just wants to reduce the reliability of the algorithm by forcing a mistake. In the latter, we have an untargeted attack.
- **Attack Frequency**: The attack on the victim’s model can be either iterative or one-time. In the one-time, the optimization of the objective function of the attacker happens in a single step, whereas the iterative method takes several steps to generate the perturbation.

### 2.2 Attack Perturbation Type

The size of the perturbation is in the core of the adversarial attack, a small perturbation is the fundamental premise of such models. When designing an adversarial example, the attacker wants the perturbed input to be as close as possible to the original one, in the case of images, close enough that a human can not distinguish one image from the other. We analyze the perturbation concerning scope, limitation, and measurement.

- **Perturbation Scope**: The attacker can generate perturbations that are input specific, in which we call individual, or it can generate a single perturbation which will be effective to all inputs in the training dataset, which we call universal perturbation.
- **Perturbation Limitation**: Two options are possible, optimized perturbation and constraint perturbation. The optimized perturbation is the goal of the optimization problem, while the constraint perturbation is the set as the constraint to the optimization problem.
- **Perturbation Measurement**: Is the metric used to measure the magnitude of the perturbation. The most commonly used metric is the $l_{p}$-norm, with many algorithms applying $l_{0}$, $l_{2}$, $l_{\infty}$ norms.

### 2.3 Defense Methods

As seen in Figure 1 based on [53], we sub-divide the defenses to adversarial attacks in 3 main categories: Gradient Masking/Obfuscation, Robust Optimization, and Adversarial Example Detection, which are described as:

- **Gradient Masking/Obfuscation**: The core aspect of defense mechanisms based on gradient masking is constructing models with gradients that are not useful for attackers. The gradient masked/obfuscated models, in general, produce loss functions that are very smooth in the neighborhood of the input data. This smoothness around training data points makes it difficult for exploiting algorithms to find meaningful directions towards the generation of an adversarial example.
- **Robust Optimization**: Is a defense strategy that is composed of methods that improve the optimization function either by adding regularization terms, certification bounds, adversarial examples in the objective function, or modifying the model to add uncertainty in the model layers.
- **Adversarial Example Detection**: Recent work has turned to detect adversarial examples rather than making the DNN robust against creating them. Detecting adversarial examples is usually done by finding statistical outliers or training separate sub-networks that can distinguish between perturbed and normal images.

About the defense mechanisms, we focus this survey on methods related to Robust Optimization. Among the several publications in this survey, each author has its representation and view of robust optimization. In general, even with different notations and representations, most of the papers we have surveyed fit the general representation of Robust Optimization.

The training objective in a DL model is the minimization of the desired loss. The objective is to ajust the model parameters with respect to the labeled data, as seen in Equation 1,

<!-- \label{eq:1} -->

$$
\underset{\theta}{min} ~\mathcal{L}(\theta, x, y)
$$

in which $\theta$ are the model parameters, ${x}$ is the input to the model, $\mathcal{L}$ is the defined loss function, and y is its true label. With such a formulation, we seek to minimize w.r.t. $\theta$, the loss function. Such formulation fit the parameters to the data points, such that ${x}$ yields predictions $\hat{y}$ which are equal to the true label ${y}$. In an adversarial setting this scene changes, in which the objective is different,

<!-- \label{eq:2.0} -->

$$
\underset{\delta \leq \Delta }{max}~\mathcal{L}(\theta, x+\delta,y)
$$

which we are searching for a perturbation $\delta$, smaller than a maximum perturbation $\Delta$ capable of changing the decision of the classifier from prediction y, to y. The restriction on 3 the perturbation is a designer parameter which is in general defined by the $l_{p}$-norm.

Equations 1 and 2 do not incorporate the data distribution or the restrictions which come from the fact that most of the training datasets do not incorporate the true distribution of the data in which the models will perform inference. Based on the definition from [54], we have that, if D is the true distribution of the data, a training set is draw i.i.d. from ${\mathbb{D}}$, and is defined as ${ D = \{ (x_{i}, y_{i}) \sim~\mathbb{D} \} }$, for ${i = 1,...,m}$. And the empirical risk of a classifier, which is based on the training set, is defined as:

<!-- \label{eq:2.1} -->
$$
R(F,D) = \frac{1}{\lvert D \rvert}\sum_{(x,y)\in D} \mathcal{L}(f(x),y)
$$

in which $\left\lvert D \right\rvert$ is the size of the training set ${D}$. With that definition the empirical adversarial risk is defined in:

<!-- \label{eq:2.2} -->
$$
R_{adv}(F,D) = \frac{1}{\lvert D \rvert}\sum_{(x,y)\in D} \mathcal{L}(f(x+\delta),y)
$$

When dealing with adversarial defenses in the lenses of Robust Optimization, the one first solution, is to solve the combined worst-case loss, with the empirical adversarial risk Radv, known as adversarial training.

<!-- \label{eq:3} -->
$$
\underset{\theta}{min} \frac{1}{\lvert D \rvert}\sum_{(x,y)\in D} \underset{\delta \in \Delta}{max} \mathcal{L}(f(x+\delta),y)
$$

The solution of Equation 3, require special handling or a completely different formulation which define how we categorize the defense mechanisms for adversarial attacks, namely: Adversarial (re)Training, Bayesian Approach, Regularization Approach, and Certified Defenses.

#### 2.3.1 Adversarial (re)Training as a Defense Mechanism

The solution of Equation 3 requires solving the inner maximization (Equation 2), which is a high dimensional nonconvex optimization problem prohibitively hard to solve exactly by standard optimization techniques. The most popular approach to solve such a problem is by approximating Equation 2 with the use of a heuristics in which we are searching for a lower bound for Equation 2. While promising and shown to improve robustness even for large models (ImageNet [55]), such models come with a drawback which when instantiated in practice with the approximation heuristics, they are unable to provide robustness guarantees or certifications. This class of defenses even though very practical to implement can not provide a guarantee that no adversarial example exists in the neighborhood of ${x}$ capable of fooling ${f(.)}$.

#### 2.3.2 Certified Defenses

The problem stated in Equation 3, defines the general objective of adversarial training. But as seen, even with the best methods to find a local approximation to the maximization problem, we are subjective to the effectiveness of the attacking method. A way around this inconvenience has been proposed in the literature, which is to exactly solve the maximization problem or approximate to a solvable set of constraints. To formally define Certified Defenses, initially, we consider a threat model where the adversary is allowed to transform an input ${x \in \mathbb{R}^{d}}$ into any point from a set ${\mathbb{S}_{0} (x) \subseteq R^{d}}$. Such set represents the neighborhood of the point ${x}$ generated by either ${l_{p}}$ perturbations, geometric transformations, semantic perturbations, or another kind of transformation in ${x}$. In case of an ${l_{p}}$ perturbation, the set is defined as ${ \mathbb{S}_0 = \left\{ x^{\prime} \in \mathbb{R}^{d},  {\left\lVert x - x^{\prime} \right\rVert}_{p} < \epsilon \right\}}$.
We further expand the model ${f(.)}$ as a function of its ${k}$ hidden layers and parameters ${\theta}$, where

<!-- \label{eq:4} -->

$$
f(x) = f_{\theta}^{k} \circ f_{\theta}^{k-1} \circ \cdots  \circ f_{\theta}^{1}
$$

in which ${f_{\theta}^{i} : \mathbb{R}^{d_{i}-1} \to \mathbb{R}^{d_{i}}}$ denotes the nonlinear transformation applied in hidden layer ${i}$. The objective is to prove a property on the output of the neural network, encoded via a linear constraint:

$$
c^{T} f_{\theta}(x^{\prime}) + d < 0 ~\forall x^{\prime} \in \mathbb{S}_{0}(x)
$$

in which ${c}$ and ${d}$ are property specific vector and scalar values.

To understand the complexity of the certification, based on Equation 4, we define the layer-wise adversarial optimiza- tion objective. For ${ z_{1} = x, x_{i+1} = f_{i}(W_{i} z_{i} + b_{i})}$

<!-- $$
\begin{split}
\underset{z_{1},\cdots, d+1}{\text{max}}   & (e_{y}-e_{y_{t~arg}})^{T} ~z_{d+1}    \\
\text{s.t.}                                & z_{1}^{\prime} \in \mathbb{S}_{0}     \\
                                           & z_{i+1} = f_{i}(W_{i} z_{i} + b_{i}), i = 1, \cdots, d - 1   \\
                                           & z_{d+1} = W_{d} z_{d} + b_{d}
\end{split}
$$ -->

in which ${e_{i}}$ unit basis, vectors with value 1 in the class ${i^{th}}$ position and zeros everywhere else. Such formulation requires special handling given that we have a nonlinear constraint defined by the activation function. Several techniques have been proposed to solve such a problem and they are within the scope of study on this survey. The method to train certified neural networks is based on the computation of an upper bound to the inner loss, as opposed to a lower bound computed for adversarial training. These methods are typically referred to as provable defenses as they provide guarantees on the robustness of the resulting network, under any kind of attack inside the threat model. Typical methods to compute the certifications are based on convex relaxations, interval propagation, SMT solvers, abstract interpretation, mixed-integer linear programs, linear relaxations, or combinations of these methods. We explore the diverse techniques in subsection 4.3.

#### 2.3.3 Regularization Approach

Regularization techniques focus on making small modifica- tions to the learning algorithm, such that it can generalize better. In a certain way, it improves the performance of the model in unseen data. It prevents model over-fitting to the noise data, by penalizing weight matrices of the nodes. In the specific case of Robust Optimization, the objective of regularization techniques is similar, but focusing on avoiding that small variation on the input, can generate changes in the decision of the algorithm. It does so by either expanding the decision boundaries or limiting changes in the gradient of the model.

Many regularization techniques have been proposed with the most used being the ${l_{p}}$ based ones. The ${l_{2}}$ regularization technique is introduced to reduce the parameters value which translates to variance reduction. It introduces a penalty term to the original objective function (Loss), adding the weighted sum of the squared parameters of the model. With that, we have a regularized loss LR defined as:

$$
\mathcal{L}_{R}(x + \delta, y) = \mathcal{L}(x + \delta, y) + \lambda {\lVert \theta \rVert}_{2}
$$

in which a small ${\lambda}$ lets the parameters to grow unchecked while a large ${\lambda}$ encourages the reduction of the model parameters. Regularization methods are not restricted to ${L_{p}}$ approaches and can involve Lipschitz Regularization, the Jacobian Matrix, and other techniques that we survey on subsection 4.2.

## 3 METHODS FOR GENERATING ADVERSARIAL ATTACKS

Studying adversarial attacks in the image classification domain improve our insights, as we can visually analyze the dissimilarities between disturbed and non-disturbed inputs. Moreover, the image data, even though high dimensional, are simpler represented than other domains such as audio, graphs, and cyber-security data. Along this section, we’ll revise the attack generating algorithms in the image classification domain which can be applied to standard deep neural networks (DNN) and convolutional neural networks (CNN). In which we classify in: white box, black box, and applied real world attacks. In Table 2 we summarize all the attacks described in this section, highlighting the distance metric used, information access level, algorithm type, and the domain it was applied in the specific publication.

In the following sub-sections, we list and describe the most popular approaches for the adversarial attack in ML models. We list them in chronological order and focus on giving the most important details on these methods.

### 3.1 White-box Attacks

As stated, in white box attacks, there is no restriction in the level of information to which the attacker can access. As a consequence the adversary knows model parameters, dataset, or any other information regarding the model. Under such assumption, given a model ${f(.)}$, an input ${(x, y)}$, the main objective is to produce ${x^{\prime}}$, which is within certain distance from the original ${x}$ and maximizes the loss ${\mathcal{L}(f(x+\delta), y)}$.

$$
\underset{\delta\in\Delta}{\max} ~\mathcal{L}(f(x+\delta), y)
$$

#### 3.1.1 Box-Constrained L-BFGS

In, [43], the existence of small perturbations capable of misleading a classifier were first demonstrated. In the paper, Szegedy et. al. proposed to compute an additive noise $\delta$, which could be added to the original input ${x}$, capable ofmisleading the classifier but with minimal or no perceptible distortion to the image. We find the optimal delta, ${\delta}$, with:

<!-- \label{eq:6} -->
$$
\underset{\delta}{\text{min}} ~ c{\lVert \delta \rVert}_{2} \\
\text{s.t.} ~ f(x + \delta) = y^{\prime} ~\text{all pixel in} (x + \delta) \in [0,1]
$$

in which ${f(.)}$ is the parameterized DNN model, ${y}$ is the true label, ${y^{\prime}}$ is the target label. As is this is a hard problem. Using a box constrained L-BFGS the authors proposed an approximate solution to the problem stated as:

<!-- \label{eq:6} -->

$$
\underset{\delta}{\text{min}}~~~c{\lVert \delta \rVert}_{2} + \mathcal{L}(f(x + \delta), y^{\prime}) \\
\text{s.t.} ~ \text{all pixel in} (x + \delta) \in [0,1]
$$

In this model, Szegedy et. al., managed to generate images that were visually indistinguishable from the original ones, but were able to fool the classifiers into classifying them as another class. This was the first result and publication which has exploited this weakness of deep learning models.

#### 3.1.2 Fast Sign Gradient Method

In, [56], introduced a one step adversarial attack framework. The attack image, ${x^{\prime}}$, is obtained by a simple additive disturb:

$$
x^{\prime} = x + \delta_{ut} \\
x^{\prime} = x - \delta_{tg}
$$

in which for the untargeted setting, $\delta{ut}$, we obtain the perturbation from:

$$
\underset{{\lVert \delta_{ut} \rVert}_{p} \leq \epsilon}{\max} ~ \mathcal{L}(f(x+\delta), y)
$$

and in the the targeted setting, $\delta_{tg}$, from:

$$
\underset{{\lVert \delta_{tg} \rVert}_{p} \leq \epsilon}{\max} ~ (\mathcal{L}(f(x+\delta), y) - \mathcal{L}(f(x+\delta), y^{\prime}))
$$

in which ${\epsilon}$ is the ball defined normally by an ${l_{p}}$-norm.

The core of fast sign gradient method maximize the norm of the vector between originally labeled class and the currently assigned label, while in the targeted setting, it focus on minimizing the distance to the target class. As it is a one-step algorithm, it is not very resilient to current defenses but is very fast implementation. Figure 3 shows the attack of an image and the false prediction.

#### 3.1.3 DeepFool

The Deepfool attack proposed by [57], is a white box attack which explores the boundaries of the classification model. In the multi-class algorithm, Deepfool initializes with an input ${x}$ which is assumed to be within the boundaries of the classifier model ${f(x)}$. With an iterative process, the image isperturbedbyasmallvectortowardsthedirectionofthe decision boundaries. The boundaries are approximated by linear functions, more specifically a hyperplane, defined in the algorithm as ${\hat{l}}$. At each step, the perturbations are accumulated to form the final perturbation to the image. With smaller perturbations than in FGSM ([56]), the authors have shown similar or better attack success rate.

#### 3.1.4 Jacobian-based Saliency Map Attack

The Jacobian-based Saliency Map attack (JSMA) differs from most of the adversarial attack literature with respect to the norm it uses on the perturbation restriction. While most of the attacks focus on the ${l_{\infty}}$ or ${l_{2}}$ norms, JSMA, proposed in [58], focus on the l0 norm. Under this norm, penalizes the change in a binary way, if the pixel has been changed or not, opposed to ${l_{2}}$ based algorithms which takes into consideration the size of the change in the pixels.

In this attack, Papernot et. al., calculates the Jacobian of a score matrix ${F}$. The model executes the attack in a greedy way. It modifies the pixel which has the highest impact on the model’s decision. The Jacobian Matrix is defined as:

$$
J_{F} (x) = \frac{\partial F(x)}{\partial(x)} = \left\{ \frac{\partial F_{j}(x)}{\partial x_{i}}  \right\} x \times j
$$

it models the influence of changes in the input ${x}$ to the predicted label ${\hat{y}}$. One at a time pixels from the unperturbed image are modified by the algorithm in order to create a salience map. The main idea of salience map is the correlation between the gradient of the output and the input. It is a guide to the most influential variables of the input, or the ones that probably can deceive the classifier with less manipulation. Based on that, the algorithm performs modifications in the most influential pixel.

### 3.1.5 Projected Gradient Descend (PGD)

Also known as the basic iterative method, was initially proposed in [59]. It is based on the FGSM, but instead a single step of the projected gradient descend, it iterates through more steps, as in:

$$
\delta \coloneqq P(\delta + \alpha \nabla_{\delta} \mathcal{L}(f(x+\delta), y))
$$

in which ${P}$ denotes the projection over the ball of interest. With such formulation, the PGD, requires more fine-tuning, in choosing the step size ${\alpha}$. In [54], Madry et. al. proposed an iterative method with a random initialization for ${\delta}$.

#### 3.1.6 Carlini and Wagner Attacks (CW)

In [60], 3 ${l_{p}}$-norm attacks ${(l_{0}, l_{2}, l_{\infty})}$ were proposed as a response to [61], which proposed the use of Distillation as a defense strategy. In their paper, Papernot et. al., successfully presented a defense mechanism capable of reducing the effectiveness of the FGSM and L-BFGS. CW proposes to solve the same problem stated in FGSM, which is given an input ${x}$ find a minimal perturbation ${\delta}$ capable of shifting the classification prediction of the model. The problem is addressed as:

<!-- \label{eq:8} -->

$$
\underset{\delta}{\min} ~ c{\lVert \delta \rVert}_{p} + \mathcal{L}(f(x + \delta), y^{\prime}) \\
$$

$$
\text*{s.t.} ~ (x + \delta) \in {\left[0, 1\right]}^{n}
$$

in which ${\mathcal{L}(f(x + \delta), y^{\prime}) = \max_{i \neq j}(Z(x^{\prime}_{i}) - Z(x^{\prime})_{y})^{+}}$, and ${Z(x) = z}$ are the logits. As the algorithm minimizes the metrics ${\mathcal{L}(.)}$, it finds the input ${x^{\prime}}$ that has larger score to be classified as ${y^{\prime}}$. As we search for the value of ${c}$, we look for the constant which will produce the smaller distance between ${x}$ and ${x^{\prime}}$.

#### 3.1.7 Ground Truth Adversarial Example (GTAE)

So far most of the attacks, even if motivated by the generation of new defenses, are independent of the defense algorithm. In the algorithm proposed by [62], the certified defense proposed in [63], is used as a base for the optimization and search for adversarial examples.

The algorithm abstract the ${\theta}$ and dataset ${(x, y)}$ with the use of an SMT solver, and solves the system to check if there exist ${x^{\prime}}$ near ${x}$, within the established norm distance, which can cause a misclassification. The ground truth adversarial example is found by reducing the size of ${\epsilon}$ up to the point that the system can no longer find a suitable ${x}$ 0 . The adversarial example is considered the ${x^{\prime}}$ found with the immediately larger ${\epsilon}$. It is the first method to calculate an exact provable solution to a minimal perturbation which can fool ML models. In contrast, as stated by the authors, the fact that the model relies on an SMT solver, restrict the applicability of the algorithm to models with no more than a few hundred nodes. This attack has been revisited by [64] and [65].

#### 3.1.8 Universal Adversarial Perturbations

Different from the previous methods, the universal adversarial perturbation (UAP), proposed in [66], search for a single perturbation capable of fooling all samples from the training dataset. The perturbations, independent of the input, are also restricted to not be detected by humans. The perturbations are constructed based on:

$$
P_{x \sim D} \left( f(x) \neq f(x+\delta) \right) \geq \beta, \text{s.t.} {\lVert \delta \rVert}_{p} \leq \epsilon
$$

in which ${\epsilon}$ defines the size of the perturbation based on an ${l_{p}}$-norm and ${\beta}$ defines the probability of an image sampled from the training dataset being fooled by the generated perturbation. In this case, the algorithm optimizes the probability of fooling the classifier. The method to calculate the universal perturbations is based on the DeepFool algorithm, in which the input is gradually pushed towards the model’s decision boundary. It differs from DeepFool in the fact that instead of pushing a single input, all members of ${\mathcal{D}}$ are modified in the direction of the decision boundary. The perturbations, calculated for each image, are accumulated in a gradual manner. The accumulator is then projected back towards the specified ${B_{\epsilon}}$ ball, of radius ${\epsilon}$. Its been shown that with variations of 4%, a fooling accuracy of 80% has been achieved.

#### 3.1.9 Shadow Attack

In [67], an attack targeting certified defenses was proposed. In their work, they target defenses that certify the model with respect to a radius defined by the ${l_{p}}$-norm. One intuitive idea to construct a certified defense is to check within a certain radius ${B_{\epsilon}}$ of input, the existence of a perturbation ${\delta}$, capable of changing the decision of the classifier. The shadow attack is constructed to leverage this premise, and construct a perturbation outside of the certification zone. It is claimed that after labeling an image, these defenses check whether there exists an image of a different label within ${\epsilon}$ distance (in ${l_{p}}$ metric) of the input, where ${\epsilon}$ is a security parameter selected by the user. If within the ${B_{\epsilon}}$ ball all inputs are classified with the same label, then the model is robustly certified. Their model targets not only the classifier but also the certificate. It is done by adding adversarial perturbations to images that are large in the ${l_{p}}$-norm and produce attack images that are surrounded by a large ball exclusively containing images of the same label. In order to produce images that are close to the original, in a perception way, but can fool the classifier, they use the following objective function: 

$$
\underset{y^{\prime} \neq y, \delta}{\max} ~ -\mathcal{L}(\theta, x + \delta \vert y^{\prime}) - \lambda_{c}C(\delta)
$$

$$
-\lambda_{tv} TV (\delta) - \lambda_{S}Dissim(\delta)
$$


in which ${\mathcal{L}(\theta, x+\delta \vert \bar{y})}$ refers to the adversarial training loss, ${\lambda_{c}C(\delta)}$ is a color regularization term, ${\lambda_{tv} TV (\delta)}$ is a smoothness penalty term, and ${\lambda_{S}Dissim(\delta)}$ guarantees that all color channels receive similar perturbation.

#### 3.1.10 Other Attacks

The presented attacks are just part of many more which have been published in many different venues. Here we list some other relevant attack methods available in the literature. 

* **EAD: Elastic-net attack** - Similar to L-BFGS the algorithm in [68] proposes to find the minimum additive perturbation, which misleads the classifier. Differently it incorporates an association of the norms ${l_{1}}$ and ${l_{1}}$. It has been shown that strong defenses against ${l_{\infty}}$ and ${l_{1}}$ norms still fail to reject ${l_{1}}$ based attacks. 
* **Objective Metrics and Gradient Descend Algorithm (OMGDA)** - The algorithm proposed by [69], is very similar to DeepFool, with the optimization of the step size. Instead of utilizing a fixed and heuristically determined step size in the optimization, in Jang et al., the step size utilizes insights from the softmax layer. The step size is determined based on the size of the desired perturbation and varies over time. 
* **Spatially Transformed Attack (STA)** - In [70], instead of generating changes in the intensity of the pixels, the authors have proposed a method based on small translational and rotational perturbations. The perturbations are still not noticeable by the human eyes. Similarly in [71] the spatial aspect of the input is also exploited for the adversarial example generation. 
* **Unrestricted Adversarial Examples with Generative Models (UAEGM)** - Based on AC-GAN ([72]), [73], has proposed the use of generative networks to generate examples which are not restricted to being in the neighborhood of the input data. The generated attacks, are not necessarily similar to the ones in the dataset but are similar enough to humans not notice and fool the classifiers.

### 3.2 Black-Box Attacks

Under Black box restriction, models are different from the currently exposed white box, with respect to the information the attacker has access to. In most cases, the adversary does not have any or some information about the targeted model, like the algorithm used, dataset, or parameters, as seen in Figure 2. An important modeling challenge for black-box attacks is to model precisely what information the attacker has about either the learned model or the algorithm. In this sub-section, we list the most relevant methods for black attack generation. 

#### 3.2.1 Practical Black-Box Attacks (PBBA) 

While assuming access to the information of the model enables a series of attacks, the work in [74], introduces the possibility of attacking models about which the attacker has less knowledge. In this work, no knowledge about the architecture is assumed, only some idea of the domain of interest. It is also limited the requests for output sent to the model, which requires the attacker to choose wisely the inference requests from the victim’s model. To achieve the goal, Papernot et. al. introduce the substitute model framework. The attack strategy is to train a substitute network on a small number of initial queries, then iteratively perturb inputs based on the substitute network’s gradient information to augment the training set

```

```

With the boundaries of substitute model adjusted to be close to the original model, any of the methods presented in the previous section can be used to generate the perturbed image.

#### 3.2.2 Zeroth Order Optimization Based Attack

In Chen et al. [52], in the attack also known as ZOO, the authors assume accessibility to both, the input data and the confidence scores of the model in which they target their attack. They differ from [74] in the fact that the model does not focus on transferability (creating a substitute model) to achieve the adversarial examples. In their work, they propose a zeroth-order optimization attack which estimates the gradient of the targeted DNN. Instead of traditional gradient descend, they use order 0 coordinate SGD. Moreover, to improve their model and enable the adversarial example generation, they implement dimensionality reduction, hierarchical techniques, and importance sampling. As the pixels are tuned, the algorithm observes the changes in the confidence scores. 

Similar to the ZOO, in the one-pixel attack [75], it is proposed the use of the score confidence to perturb the input and change the decision of the classifier. This paper focuses on modifying a single pixel of the input. With the use of differential evolution, the single pixel is modified in a blackbox setting. The authors base the update of the perturbation in the variation of the probability scores for each class.

#### 3.2.3 Query-Efficient Black-Box Attacks

One of the biggest challenges in black-box attacks is the fact that many inference models have mechanisms to restrict the number of queries (when cloud-based or system embedded), or the inference time can restrict the number of queries. One line of research in black-box models looks into making such models more query efficient, for example, the work from [76], based on natural evolution strategies reduces by 2 or 3 order of magnitude the amount of information requests sent to the model to successfully generate a misclassified perturbed image. The algorithm set queries in the neighborhood of the input ${x}$. The output of the model is then sampled, and these samples are used to estimate the expectation of the gradient around the point of interest.

The algorithm sample the model’s output based on the queries around the input x, and estimate the expectation of a gradient of ${F}$ on ${x}$. More on the topic, [77], proposes a family of algorithms based on a new gradient direction estimate using only the binary classification of the model. In their work, it is included ${l_{\infty}}$ and l2 norm-based attacks as well as targeted and untargeted attacks. Figure 4 shows an intuition on how the gradient is updated and the boundaries of the decision are used to generate the adversarial attack. Algorithm 2 shows how the dimensionality reduction ${d^{r}}$ is defined. Moreover, in the search for query efficient black box attack, [78] introduces a method that is independent of the gradient based on Bayesian optimization and Gaussian process surrogate models to find effective adversarial examples. In the model it is assumed that the attacker has no knowledge of the network architecture, weights, gradient or training data of the target model. But it is assumed that the attacker can query model with input ${x}$, to obtain the prediction scores on all classes ${C}$. They restrict the perturbation to the ${l_{\infty}}$ norm. Their objective is to maximize over the perturbations: 

<!-- \label{eq:10} -->
$$
\delta^{*} = \underset{\delta}{\arg\min}\left[\log\left(f(x_{origin}+g(\delta))_{t}\right)-\log(\sum_{j\neq{}t}^{C}{f(x_{origin} + g(\delta))_{j}})\right]
$$

$$
\text{s.t.} ~ \delta \in [-\delta_{max}, +\delta_{\max}]^{d_{r}}
$$

The Bayesian optimization proposed to improve the query efficiency requires the use of a surrogate model to approximate the objective function, in their work a Gaussian Process is used. Moreover to define the next query point is defined by an acquisition function. A big differential in their work is the fact that instead of searching in a high-dimensional space for a perturbation ${\delta}$, they utilize a function to reduce the dimensionality of the perturbation and later reconstitute to the true image size. 

```

```

#### 3.2.4 Attack on RL algorithm

In [79], a method for generating adversarial examples in reinforcement learning (RL) algorithms was proposed. In RL, an adversarial example can either be a modified image used to capture a state or in the case of this publication, an adversarial policy. It is important to highlight that an adversarial policy is not a strong adversary as we have in two-player games, but one that with a certain behavior triggers a failure in the victim’s policy. In this paper, a blackbox attack is proposed to trigger bad behaviors in the victim’s policy. The victim’s policy is trained using Proximal Policy Optimization and learns to "play" against a fair opponent. The adversarial policy is trained to trigger failures in the victim’s policy. Figure 5 shows the difference between an opponent’s policy and an adversarial manipulated policy. Also in the paper, it was shown the dependence of the size of the input space and the effectiveness of adversarial policies. The greater the dimensionality of the observation space under the control of the adversary, the more vulnerable

### 3.3 Physical World Attack 

The research presented so far is mostly focused on applying the attacks in virtual applications and controlled datasets, but the great concern about the existence of adversarial examples is the extent to which they can imply severe consequences to the users of the system. With that objective, we dedicate this session on exploring publications with realworld applications and consequences clearly stated. 

In [80], road signs were physically attacked by placing a sticker in a specific position of the sign. Their attack consisted of initially finding, in a sign image, the location with the most influence for the decision of the classifier. For that objective, an l1norm was used because it renders sparse perturbations in the image, making it easier to locate the modification patches. Based on the defined location, an ${l_{2}norm}$ was used to identify the most appropriate color for the sticker. 

Moreover, as face recognition is becoming very popular as a biometric security measure it is the focus of several adversarial attacks. We highlight 4 attacks in face recognition and identification.

* **Evaluation of the robustness of DNN models to Face Recognition against adversarial attacks** - In this publication Goswami et al. [81] evaluates how the depth of the architecture impacts the robustness of the model in identifying faces. They evaluate the robustness with respect to adversarial settings taking into consideration distortions which are normally observed in a common scene. These distortions are handled with ease by shallow networks on the contrary of deep networks. In their approach, they’ve used Open-Face and VGG-Face networks, and have achieved a high fooling rate. It is important to notice that in their attack no restrictions are made in the visibility of the perturbations. 
* **Adversarial Attacks on Face Detectors using Neural Net based Constrained Optimization** In this work, also focusing on prevent face identification, [82] has generated an attack, based on Carlini and Wagner attack, which was able to fool R-CNN. Their perturbations are not visible in the adversarial example. 
* **Generating Adversarial Examples by Makeup Attacks on Face Recognition** - In this research, [83] implement a GAN network to generate make-up perturbation. When the perturbation is applied to the face, the classifier shifts its decision to the target class. 
* **Efficient decision-based black-box adversarial attacks on face recognition** - Dong et al. [84] propose an evolutionary attack method. The proposed adversarial example generator is constrained in a black-box setting. The algorithm focus on reducing the number of dimensions of the search space by modeling the local geometry of the search vectors. Such an algorithm has been shown to be applicable to most recognition tasks.

#### 3.3.1 Other Attacks 

In the field of Cyber-security machine learning models are, in general, applied to detect malware, malicious connections, malicious domain classifier, and others. In Suciu et al. [85] an evaluation of the robustness of current malware detection models is performed. The authors retrain the model in a production-scale dataset to perform the evaluation. With the new data, the model which was previously vulnerable to attacks was shown to be stronger and architectural weaknesses were reported. The work of Suciu et al. [85], explores how attacks transfer in the cyber-security domain, and mainly the inherent trade-off between effectiveness and transferability. With respect to malicious connection and domain, Chernikova et al. [86] builds a model that takes into consideration the formal dependencies generated by the normal operations applied in the feature space. The model to generate adversarial examples simultaneously consider both the mathematical dependencies and the real-world constraints of such applications. The algorithm focus on determining the features with higher variability and the ones with a higher correlation with these features. This search is performed for each iteration. All the identified features are modified but constrained to preserve an upper bound on the maximum variation of the features. The upper bound respects physical-world application limitations. 

In the Cyber-Physical domain, several publications demonstrate the brittleness of ML models and the generation of adversarial examples. [87] generated adversarial examples to an iCub Humanoid Robot. The attack proposed simply extends over the attacks in [88]. The main aspect to be considered in this paper is the fact that it highlights the high consequences of the adversarial examples in the decision process of safety-critical applications. Moreover, in selfdriving cars several attacks have been derived like DARTS ( [89]) and the work of [90] which shows the attack of traffic signs, the latter with real experiments. In a different sensor type, the work of [91] demonstrates the attack to a LIDAR sensor, in which they attack the point cloud image. 

Moreover in [92], a novel technique was proposed to attack object tracking algorithms. In their approach, the bounding box is attacked in a single frame, which is enough to fool the algorithm and generate an offset in the placement of the bounding box. Such an attack would be critical to self-driving cars to recognize the position of obstacles, other vehicles, and pedestrians on the road.