# Opportunities and Challenges in Deep Learning Adversarial Robustness: A Survey

Abstract—As we seek to deploy machine learning models beyond virtual and controlled domains, it is critical to analyze not only the accuracy or the fact that it works most of the time, but if such a model is truly robust and reliable. This paper studies strategies to implement adversary robustly trained algorithms towards guaranteeing safety in machine learning algorithms. We provide a taxonomy to classify adversarial attacks and defenses, formulate the Robust Optimization problem in a min-max setting, and divide it into 3 subcategories, namely: Adversarial (re)Training, Regularization Approach, and Certified Defenses. We survey the most recent and important results in adversarial example generation, defense mechanisms with adversarial (re)Training as their main defense against perturbations. We also survey mothods that add regularization terms which change the behavior of the gradient, making it harder for attackers to achieve their objective. Alternatively, we’ve surveyed methods which formally derive certificates of robustness by exactly solving the optimization problem or by approximations using upper or lower bounds. In addition we discuss the challenges faced by most of the recent algorithms presenting future research perspectives.

## Introduction

Deep Learning (DL) ([1]) models are changing the way we solve problems that have required many attempts from the most diverse fields of science. DL is an improvement over Artificial Intelligence (AI) Neural Networks (NN), in which more layers are stacked to grant a bigger level of abstraction and better reasoning over the data when compared to other Machine Learning (ML) algorithms ([2]). Since the raise of DL, supported in many cases by cloud environments [3], [4], [5], the base architecture and its variations have been applied in many scientific breakthroughs in the most diverse fields of knowledge, e.g. in predicting AMD disease progression ([6]), predicting DNA enhancers for gene expression programmes ([7]), elections and demographic analysis based on satellite images ([8]), filtering data for gravitational-wave signals ([9]). DL approach has also become one of the most used approaches for natural language processing ([10]) and speech recognition ([11]).

One of the most popular variations of DL architecture, Convolutional Neural Networks (CNN) have significantly boosted the performance of DL algorithms in computer vision (CV) applications ([12]), bringing it to several areas of CV such as, object detection ([13], [14]), action recognition [15], [16], pose estimation [17], [18], image segmentation [19], [20], and motion tracking [21]. Starting with ImageNet [22], proposed in 2012, the field of CNN’s have seen great improvement with super-human performance in specific tasks, providing solutions even to medical problems [23].

Fueled by the fact that new frameworks, libraries, and hardware resources are being improved and made available to the public and scientific community [24], [25], [26], Deep Neural networks (DNN) are being improved constantly and achieving new performance breakthroughs [27], [28], [29]. With the current maturity of DNN algorithms, its being applied in solving safety and security-critical problems [30], such as self-driving cars [31], [32], multi-agent aerial vehicle systems with face identification [33], robotics [34], [35], social engineering detection [36], network anomaly detection [37], deep packet inspection in networks [38]. DNN applications are already part of our day-to-day life (personal assistants [39], product recommendation [40], biometric identification [41]) and tend to occupy a bigger space as time passes.

As seen in many publications, DNN has been shown to have human-level accuracy even for significantly complex tasks such as playing games with no prior rule known, except the current frames [42]. In contrast to the aforementioned accuracy of DNN models, its been shown in earlier publications [43], [44], [45], that DNN models are susceptible to small input perturbations, in most cases imperceptible to the human eye. The results from this publications have shown the facility with which small additive targeted noise to the input image, makes models to misclassify objects which before could be identified with 99.99% confidence. More alarming is the fact that such models report high confidence in the predictions. Such perturbations, which can fool a trained model, are known as adversarial attacks. With such alarming consequences, the study of adversarial attacks and robustness against them became a great deal of research in recent years.

A considerably large number of research papers is now available concerning methods to identify adversarial attacks and defend from incursions against the model [46], [47]. One way of solving this issue is adding better intuition on the models, through explainability [48], but such models do not target the direct improvement of the model. On the other hand, several approaches have been published to generate models which are robust against adversarial attacks [49], the target of the researchers is to introduce in their models’ layers of robustness such that the models are not fooled by out of distribution examples, known or unknown attacks, targeted or untargeted attacks. Guaranteeing the accuracy of such models while safety is taken into consideration, is of utmost importance for system architects, mainly making them robust to the presence of adversarial attacks, noise, model misspecification, and uncertainty. This survey aims to bring together the recent advances in robustness for DNN’s, pointing the main research directions being followed recently to generate robust DNN models. We bring to light both applied and theoretical recent developments.

Inspired by [50], we analyze the robustness of DNN’s under the perspective of how adversarial examples can be generated, and how defenses can be formulated against such algorithms. In general words, we define the robustness against adversarial attacks problem as a dual optimization problem, in which the attackers try to maximize the loss while the defenses try to minimize the chance of a model being fooled by the attacker. In such formulation, current existing models based in non linear activation functions, introduce non-linear inequality constraints in the optimization, which generates an inherent trade off between exact solutions and scalability of the model. This trade-off comes in the form of either exact slow solutions through mixed-integer linear programming, or in approximations to the objective function which either, relies on the existing attack methods to provide a local heuristic estimation to the maximization function, or approximations of the bounds of the constraints or objective function to generate certification regions, in which no adversarial example exists.

More specifically this paper presents the following contributions:

1. We characterize defenses to adversarial attacks as a min-max optimization problem, investigating solutions involving heuristic approximation, exact solution, and upper/lower bound approximations to generate models robust to adversarial attacks
2. We investigate, analyze, and categorize the most recent and/or important approaches to generate adversarial examples, as they are the basis to generate strong defenses, through Adversarial (re)Training.
3. We investigate, analyze, and categorize the most recent and important approaches to generate defenses against adversarial attacks, providing a taxonomy, description of the methods, and the main results in such approaches.

We organize this survey in the following manner. In section 2 we describe taxonomies for both adversarial example generation and defenses. We classify the adversarial models concerning the time of the attack, information available to the attacker, objective, and the algorithm computation method. Moreover, we classify the perturbation type used by the attackers. We divide the defense methods into three categories, namely gradient-masking/obfuscation, robust optimization, and adversarial example detection. We focus this research in Robust Optimization, and further subdivide in 3 groups: Adversarial Training, Certified Defenses, and Regularization Approach. In section 3, we describe several relevant adversarial attacks, and summarize them in Table 2. In section 4 we describe the most relevant results in Robust Optimization, and provide a tree that maps these publications to the 3 sub-groups of Robust Optimization. In section 5 we discuss current challenges and opportunities in robust defenses.

## 2 TAXONOMY OF ADVERSARIAL ATTACKS AND DEFENSES

We keep a consistent notation set along with the survey, and for easiness of reading we summarize in Table 1 the most used notations and symbols which we will use along with this survey. For papers requiring some specific terms, we define them in the section in which they are presented.

### 2.1 Attack Threat Model

Several attempts have been made to categorize attacks on machine learning. We here distill the most important aspects which characterize adversarial examples generating models concerning their architecture. We focus on the aspects that are most relevant to the discussion of adversarial robustness. To that end, we classify the attacks concerning 3 timing, information, goals, and attack frequency following the proposed in [51]:

- **Timing**: A first crucial feature for modeling the adversarial attacks is when it occurs. To that end we have two possibilities, evasion and poisoning attacks. Evasion attacks are the ones in the time of inference and assume the model has already been trained. Poisoning attacks in general targets the data, and the training phase of the model.
- **Information**: Another feature of the attack references the information to which the attacker has access. In the white box context the adversary has full access to information concerning the model and the model itself, as opposed to black box setting, in which very few or no information is available. White box attacks refer to those in which the adversary can unrestrictedly query the model for any information, such as weights, gradient, model hyperparameters, prediction scores. Whereas in black-box attacks the adversary has limited or no information about these parameters, although may obtain some of the information indirectly, for example, through queries. Some also define grey-box attacks, in which attackers might only know the feature representation and the type of model that was used but have no access to dataset or the model information. A fourth setting is called restricted black-box, or also known as no-box attack. Under such an assumption, no information is available to the attacker, and the research is mainly focused on attack transferability. In wich the focus is to evaluate the possibility of transferring the attack performed in one DNN to the inaccessible objective model [52]. In this work, we evaluate models in a binary setting, the adversary either has comprehensive access to the DNN or black box having limited access through queries, which can also provide class scores.
- **Goals**: The attackers may have different reasons to target a specific algorithm. But mostly the attacker has either a specific goal, and needs the algorithm to output a specific output, case in which it is a targeted attack, or just wants to reduce the reliability of the algorithm by forcing a mistake. In the latter, we have an untargeted attack.
- **Attack Frequency**: The attack on the victim’s model can be either iterative or one-time. In the one-time, the optimization of the objective function of the attacker happens in a single step, whereas the iterative method takes several steps to generate the perturbation.

### 2.2 Attack Perturbation Type

The size of the perturbation is in the core of the adversarial attack, a small perturbation is the fundamental premise of such models. When designing an adversarial example, the attacker wants the perturbed input to be as close as possible to the original one, in the case of images, close enough that a human can not distinguish one image from the other. We analyze the perturbation concerning scope, limitation, and measurement.

- **Perturbation Scope**: The attacker can generate perturbations that are input specific, in which we call individual, or it can generate a single perturbation which will be effective to all inputs in the training dataset, which we call universal perturbation.
- **Perturbation Limitation**: Two options are possible, optimized perturbation and constraint perturbation. The optimized perturbation is the goal of the optimization problem, while the constraint perturbation is the set as the constraint to the optimization problem.
- **Perturbation Measurement**: Is the metric used to measure the magnitude of the perturbation. The most commonly used metric is the $l_{p}$-norm, with many algorithms applying $l_{0}$, $l_{2}$, $l_{\infin}$ norms.

### 2.3 Defense Methods

As seen in Figure 1 based on [53], we sub-divide the defenses to adversarial attacks in 3 main categories: Gradient Masking/Obfuscation, Robust Optimization, and Adversarial Example Detection, which are described as:

- **Gradient Masking/Obfuscation**: The core aspect of defense mechanisms based on gradient masking is constructing models with gradients that are not useful for attackers. The gradient masked/obfuscated models, in general, produce loss functions that are very smooth in the neighborhood of the input data. This smoothness around training data points makes it difficult for exploiting algorithms to find meaningful directions towards the generation of an adversarial example.
- **Robust Optimization**: Is a defense strategy that is composed of methods that improve the optimization function either by adding regularization terms, certification bounds, adversarial examples in the objective function, or modifying the model to add uncertainty in the model layers.
- **Adversarial Example Detection**: Recent work has turned to detect adversarial examples rather than making the DNN robust against creating them. Detecting adversarial examples is usually done by finding statistical outliers or training separate sub-networks that can distinguish between perturbed and normal images.

About the defense mechanisms, we focus this survey on methods related to Robust Optimization. Among the several publications in this survey, each author has its representation and view of robust optimization. In general, even with different notations and representations, most of the papers we have surveyed fit the general representation of Robust Optimization.

The training objective in a DL model is the minimization of the desired loss. The objective is to ajust the model parameters with respect to the labeled data, as seen in Equation 1,

<!-- \label{eq:1} -->
$$ \underset{\theta}{min} ~\mathcal{L}(\theta, x, y) $$

in which $\theta$ are the model parameters, ${x}$ is the input to the model, $\mathcal{L}$ is the defined loss function, and y is its true label. With such a formulation, we seek to minimize w.r.t. $\theta$, the loss function. Such formulation fit the parameters to the data points, such that ${x}$ yields predictions $\hat{y}$ which are equal to the true label ${y}$. In an adversarial setting this scene changes, in which the objective is different, 

<!-- \label{eq:2.0} -->
$$ \underset{\delta \leq \Delta }{max}~\mathcal{L}(\theta, x+\delta,y) $$

which we are searching for a perturbation $\delta$, smaller than a maximum perturbation $\Delta$ capable of changing the decision of the classifier from prediction y, to y. The restriction on 3 the perturbation is a designer parameter which is in general defined by the $l_{p}$-norm. 

Equations 1 and 2 do not incorporate the data distribution or the restrictions which come from the fact that most of the training datasets do not incorporate the true distribution of the data in which the models will perform inference. Based on the definition from [54], we have that, if D is the true distribution of the data, a training set is draw i.i.d. from $\mathbb{D}$, and is defined as ${D=\left\{(x_{i},y_{i})\sim~\mathbb{D}\right\}}$, for ${i = 1,...,m}$. And the empirical risk of a classifier, which is based on the training set, is defined as:

<!-- \label{eq:2.1} -->
$$ R(F,D) = \frac{1}{\lvert D \rvert}\sum_{(x,y)\in D} \mathcal{L}(f(x),y) $$

in which $\left\lvert D \right\rvert$ is the size of the training set ${D}$. With that definition the empirical adversarial risk is defined in:

<!-- \label{eq:2.2} -->
$$ R_{adv}(F,D) = \frac{1}{\lvert D \rvert}\sum_{(x,y)\in D} \mathcal{L}(f(x+\delta),y) $$

When dealing with adversarial defenses in the lenses of Robust Optimization, the one first solution, is to solve the combined worst-case loss, with the empirical adversarial risk Radv, known as adversarial training.

<!-- \label{eq:3} -->
$$ \underset{\theta}{min} \frac{1}{\lvert D \rvert}\sum_{(x,y)\in D}  \underset{\delta \in \Delta}{max} \mathcal{L}(f(x+\delta),y) $$

The solution of Equation 3, require special handling or a completely different formulation which define how we categorize the defense mechanisms for adversarial attacks, namely: Adversarial (re)Training, Bayesian Approach, Regularization Approach, and Certified Defenses.

### 2.3.1 Adversarial (re)Training as a Defense Mechanism

The solution of Equation 3 requires solving the inner maximization (Equation 2), which is a high dimensional nonconvex optimization problem prohibitively hard to solve exactly by standard optimization techniques. The most popular approach to solve such a problem is by approximating Equation 2 with the use of a heuristics in which we are searching for a lower bound for Equation 2. While promising and shown to improve robustness even for large models (ImageNet [55]), such models come with a drawback which when instantiated in practice with the approximation heuristics, they are unable to provide robustness guarantees or certifications. This class of defenses even though very practical to implement can not provide a guarantee that no adversarial example exists in the neighborhood of x capable of fooling $f(.)$.

### 2.3.2 Certified Defenses

The problem stated in Equation 3, defines the general objective of adversarial training. But as seen, even with the best methods to find a local approximation to the maximization problem, we are subjective to the effectiveness of the attacking method. A way around this inconvenience has been proposed in the literature, which is to exactly solve the maximization problem or approximate to a solvable set of constraints. To formally define Certified Defenses, initially, we consider a threat model where the adversary is allowed to transform an input ${x \in \mathbb{R}^{d}}$ into any point from a set ${\mathbb{S}_{0} (x) \subseteq R^{d}}$. Such set represents the neighborhood of the point ${x}$ generated by either lp perturbations, geometric transformations, semantic perturbations, or another kind of transformation in ${x}$. In case of an ${l_{p}}$ perturbation, the set is defined as ${ \mathbb{S}_0 = \left\{ x^{\prime} \in \mathbb{R}^{d},  {\left\lVert x - x^{\prime} \right\rVert}_{p} < \epsilon \right\}}$.
We further expand the model ${f(.)}$ as a function of its k hidden layers and parameters θ, where 

<!-- \label{eq:4} -->
$$ f(x) = f_{\theta}^{k} \circ f_{\theta}^{k-1} \circ \cdots  \circ f_{\theta}^{1} $$

in which ${f_{\theta}^{i} : \mathbb{R}^{d_{i}-1} \to \mathbb{R}^{d_{i}}}$ denotes the nonlinear transfor- mation applied in hidden layer i. The objective is to prove a property on the output of the neural network, encoded via a linear constraint: 

$$ c^{T} f_{\theta}(x^{\prime}) + d < 0 ~\forall x^{\prime} \in \mathbb{S}_{0}(x) $$

in which ${c}$ and ${d}$ are property specific vector and scalar values. 

To understand the complexity of the certification, based on Equation 4, we define the layer-wise adversarial optimiza- tion objective. For ${ z_{1} = x, x_{i+1} = f_{i}(W_{i} z_{i} + b_{i})}$

$$
\begin{split}
\underset{z_{1},\cdots, d+1}{max}   & (e_{y}-e_{y_{t~arg}})^{T} ~z_{d+1}    \\
s.t.                                & z_{1}^{\prime} \in \mathbb{S}_{0}     \\
                                    & z_{i+1} = f_{i}(W_{i} z_{i} + b_{i}), i = 1, \cdots, d - 1   \\
                                    & z_{d+1} = W_{d} z_{d} + b_{d}  
\end{split}
$$