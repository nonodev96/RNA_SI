# Opportunities and Challenges in Deep Learning Adversarial Robustness: A Survey

Abstract—As we seek to deploy machine learning models beyond virtual and controlled domains, it is critical to analyze not only the accuracy or the fact that it works most of the time, but if such a model is truly robust and reliable. This paper studies strategies to implement adversary robustly trained algorithms towards guaranteeing safety in machine learning algorithms. We provide a taxonomy to classify adversarial attacks and defenses, formulate the Robust Optimization problem in a min-max setting, and divide it into 3 subcategories, namely: Adversarial (re)Training, Regularization Approach, and Certified Defenses. We survey the most recent and important results in adversarial example generation, defense mechanisms with adversarial (re)Training as their main defense against perturbations. We also survey mothods that add regularization terms which change the behavior of the gradient, making it harder for attackers to achieve their objective. Alternatively, we’ve surveyed methods which formally derive certificates of robustness by exactly solving the optimization problem or by approximations using upper or lower bounds. In addition we discuss the challenges faced by most of the recent algorithms presenting future research perspectives.

## Introduction

Deep Learning (DL) ([1]) models are changing the way we solve problems that have required many attempts from the most diverse fields of science. DL is an improvement over Artificial Intelligence (AI) Neural Networks (NN), in which more layers are stacked to grant a bigger level of abstraction and better reasoning over the data when compared to other Machine Learning (ML) algorithms ([2]). Since the raise of DL, supported in many cases by cloud environments [3], [4], [5], the base architecture and its variations have been applied in many scientific breakthroughs in the most diverse fields of knowledge, e.g. in predicting AMD disease progression ([6]), predicting DNA enhancers for gene expression programmes ([7]), elections and demographic analysis based on satellite images ([8]), filtering data for gravitational-wave signals ([9]). DL approach has also become one of the most used approaches for natural language processing ([10]) and speech recognition ([11]).

One of the most popular variations of DL architecture, Convolutional Neural Networks (CNN) have significantly boosted the performance of DL algorithms in computer vision (CV) applications ([12]), bringing it to several areas of CV such as, object detection ([13], [14]), action recognition [15], [16], pose estimation [17], [18], image segmentation [19], [20], and motion tracking [21]. Starting with ImageNet [22], proposed in 2012, the field of CNN’s have seen great improvement with super-human performance in specific tasks, providing solutions even to medical problems [23].

Fueled by the fact that new frameworks, libraries, and hardware resources are being improved and made available to the public and scientific community [24], [25], [26], Deep Neural networks (DNN) are being improved constantly and achieving new performance breakthroughs [27], [28], [29]. With the current maturity of DNN algorithms, its being applied in solving safety and security-critical problems [30], such as self-driving cars [31], [32], multi-agent aerial vehicle systems with face identification [33], robotics [34], [35], social engineering detection [36], network anomaly detection [37], deep packet inspection in networks [38]. DNN applications are already part of our day-to-day life (personal assistants [39], product recommendation [40], biometric identification [41]) and tend to occupy a bigger space as time passes.

As seen in many publications, DNN has been shown to have human-level accuracy even for significantly complex tasks such as playing games with no prior rule known, except the current frames [42]. In contrast to the aforementioned accuracy of DNN models, its been shown in earlier publications [43], [44], [45], that DNN models are susceptible to small input perturbations, in most cases imperceptible to the human eye. The results from this publications have shown the facility with which small additive targeted noise to the input image, makes models to misclassify objects which before could be identified with 99.99% confidence. More alarming is the fact that such models report high confidence in the predictions. Such perturbations, which can fool a trained model, are known as adversarial attacks. With such alarming consequences, the study of adversarial attacks and robustness against them became a great deal of research in recent years.

A considerably large number of research papers is now available concerning methods to identify adversarial attacks and defend from incursions against the model [46], [47]. One way of solving this issue is adding better intuition on the models, through explainability [48], but such models do not target the direct improvement of the model. On the other hand, several approaches have been published to generate models which are robust against adversarial attacks [49], the target of the researchers is to introduce in their models’ layers of robustness such that the models are not fooled by out of distribution examples, known or unknown attacks, targeted or untargeted attacks. Guaranteeing the accuracy of such models while safety is taken into consideration, is of utmost importance for system architects, mainly making them robust to the presence of adversarial attacks, noise, model misspecification, and uncertainty. This survey aims to bring together the recent advances in robustness for DNN’s, pointing the main research directions being followed recently to generate robust DNN models. We bring to light both applied and theoretical recent developments.

Inspired by [50], we analyze the robustness of DNN’s under the perspective of how adversarial examples can be generated, and how defenses can be formulated against such algorithms. In general words, we define the robustness against adversarial attacks problem as a dual optimization problem, in which the attackers try to maximize the loss while the defenses try to minimize the chance of a model being fooled by the attacker. In such formulation, current existing models based in non linear activation functions, introduce non-linear inequality constraints in the optimization, which generates an inherent trade off between exact solutions and scalability of the model. This trade-off comes in the form of either exact slow solutions through mixed-integer linear programming, or in approximations to the objective function which either, relies on the existing attack methods to provide a local heuristic estimation to the maximization function, or approximations of the bounds of the constraints or objective function to generate certification regions, in which no adversarial example exists.

More specifically this paper presents the following contributions:

1. We characterize defenses to adversarial attacks as a min-max optimization problem, investigating solutions involving heuristic approximation, exact solution, and upper/lower bound approximations to generate models robust to adversarial attacks
2. We investigate, analyze, and categorize the most recent and/or important approaches to generate adversarial examples, as they are the basis to generate strong defenses, through Adversarial (re)Training.
3. We investigate, analyze, and categorize the most recent and important approaches to generate defenses against adversarial attacks, providing a taxonomy, description of the methods, and the main results in such approaches.

We organize this survey in the following manner. In section 2 we describe taxonomies for both adversarial example generation and defenses. We classify the adversarial models concerning the time of the attack, information available to the attacker, objective, and the algorithm computation method. Moreover, we classify the perturbation type used by the attackers. We divide the defense methods into three categories, namely gradient-masking/obfuscation, robust optimization, and adversarial example detection. We focus this research in Robust Optimization, and further subdivide in 3 groups: Adversarial Training, Certified Defenses, and Regularization Approach. In section 3, we describe several relevant adversarial attacks, and summarize them in Table 2. In section 4 we describe the most relevant results in Robust Optimization, and provide a tree that maps these publications to the 3 sub-groups of Robust Optimization. In section 5 we discuss current challenges and opportunities in robust defenses.

## 2 TAXONOMY OF ADVERSARIAL ATTACKS AND DEFENSES

We keep a consistent notation set along with the survey, and for easiness of reading we summarize in Table 1 the most used notations and symbols which we will use along with this survey. For papers requiring some specific terms, we define them in the section in which they are presented.

### 2.1 Attack Threat Model

Several attempts have been made to categorize attacks on machine learning. We here distill the most important aspects which characterize adversarial examples generating models concerning their architecture. We focus on the aspects that are most relevant to the discussion of adversarial robustness. To that end, we classify the attacks concerning 3 timing, information, goals, and attack frequency following the proposed in [51]:

- **Timing**: A first crucial feature for modeling the adversarial attacks is when it occurs. To that end we have two possibilities, evasion and poisoning attacks. Evasion attacks are the ones in the time of inference and assume the model has already been trained. Poisoning attacks in general targets the data, and the training phase of the model.
- **Information**: Another feature of the attack references the information to which the attacker has access. In the white box context the adversary has full access to information concerning the model and the model itself, as opposed to black box setting, in which very few or no information is available. White box attacks refer to those in which the adversary can unrestrictedly query the model for any information, such as weights, gradient, model hyperparameters, prediction scores. Whereas in black-box attacks the adversary has limited or no information about these parameters, although may obtain some of the information indirectly, for example, through queries. Some also define grey-box attacks, in which attackers might only know the feature representation and the type of model that was used but have no access to dataset or the model information. A fourth setting is called restricted black-box, or also known as no-box attack. Under such an assumption, no information is available to the attacker, and the research is mainly focused on attack transferability. In wich the focus is to evaluate the possibility of transferring the attack performed in one DNN to the inaccessible objective model [52]. In this work, we evaluate models in a binary setting, the adversary either has comprehensive access to the DNN or black box having limited access through queries, which can also provide class scores.
- **Goals**: The attackers may have different reasons to target a specific algorithm. But mostly the attacker has either a specific goal, and needs the algorithm to output a specific output, case in which it is a targeted attack, or just wants to reduce the reliability of the algorithm by forcing a mistake. In the latter, we have an untargeted attack.
- **Attack Frequency**: The attack on the victim’s model can be either iterative or one-time. In the one-time, the optimization of the objective function of the attacker happens in a single step, whereas the iterative method takes several steps to generate the perturbation.

### 2.2 Attack Perturbation Type

The size of the perturbation is in the core of the adversarial attack, a small perturbation is the fundamental premise of such models. When designing an adversarial example, the attacker wants the perturbed input to be as close as possible to the original one, in the case of images, close enough that a human can not distinguish one image from the other. We analyze the perturbation concerning scope, limitation, and measurement.

- **Perturbation Scope**: The attacker can generate perturbations that are input specific, in which we call individual, or it can generate a single perturbation which will be effective to all inputs in the training dataset, which we call universal perturbation.
- **Perturbation Limitation**: Two options are possible, optimized perturbation and constraint perturbation. The optimized perturbation is the goal of the optimization problem, while the constraint perturbation is the set as the constraint to the optimization problem.
- **Perturbation Measurement**: Is the metric used to measure the magnitude of the perturbation. The most commonly used metric is the $l_{p}$-norm, with many algorithms applying $l_{0}$, $l_{2}$, $l_{\infty}$ norms.

### 2.3 Defense Methods

As seen in Figure 1 based on [53], we sub-divide the defenses to adversarial attacks in 3 main categories: Gradient Masking/Obfuscation, Robust Optimization, and Adversarial Example Detection, which are described as:

- **Gradient Masking/Obfuscation**: The core aspect of defense mechanisms based on gradient masking is constructing models with gradients that are not useful for attackers. The gradient masked/obfuscated models, in general, produce loss functions that are very smooth in the neighborhood of the input data. This smoothness around training data points makes it difficult for exploiting algorithms to find meaningful directions towards the generation of an adversarial example.
- **Robust Optimization**: Is a defense strategy that is composed of methods that improve the optimization function either by adding regularization terms, certification bounds, adversarial examples in the objective function, or modifying the model to add uncertainty in the model layers.
- **Adversarial Example Detection**: Recent work has turned to detect adversarial examples rather than making the DNN robust against creating them. Detecting adversarial examples is usually done by finding statistical outliers or training separate sub-networks that can distinguish between perturbed and normal images.

About the defense mechanisms, we focus this survey on methods related to Robust Optimization. Among the several publications in this survey, each author has its representation and view of robust optimization. In general, even with different notations and representations, most of the papers we have surveyed fit the general representation of Robust Optimization.

The training objective in a DL model is the minimization of the desired loss. The objective is to ajust the model parameters with respect to the labeled data, as seen in Equation 1,

<!-- \label{eq:1} -->

$$
\underset{\theta}{min} ~\mathcal{L}(\theta, x, y)
$$

in which $\theta$ are the model parameters, ${x}$ is the input to the model, $\mathcal{L}$ is the defined loss function, and y is its true label. With such a formulation, we seek to minimize w.r.t. $\theta$, the loss function. Such formulation fit the parameters to the data points, such that ${x}$ yields predictions $\hat{y}$ which are equal to the true label ${y}$. In an adversarial setting this scene changes, in which the objective is different,

<!-- \label{eq:2.0} -->

$$
\underset{\delta \leq \Delta }{max}~\mathcal{L}(\theta, x+\delta,y)
$$

which we are searching for a perturbation $\delta$, smaller than a maximum perturbation $\Delta$ capable of changing the decision of the classifier from prediction y, to y. The restriction on 3 the perturbation is a designer parameter which is in general defined by the $l_{p}$-norm.

Equations 1 and 2 do not incorporate the data distribution or the restrictions which come from the fact that most of the training datasets do not incorporate the true distribution of the data in which the models will perform inference. Based on the definition from [54], we have that, if D is the true distribution of the data, a training set is draw i.i.d. from ${\mathbb{D}}$, and is defined as ${ D = \{ (x_{i}, y_{i}) \sim~\mathbb{D} \} }$, for ${i = 1,...,m}$. And the empirical risk of a classifier, which is based on the training set, is defined as:

<!-- \label{eq:2.1} -->
$$
R(F,D) = \frac{1}{\lvert D \rvert}\sum_{(x,y)\in D} \mathcal{L}(f(x),y)
$$

in which $\left\lvert D \right\rvert$ is the size of the training set ${D}$. With that definition the empirical adversarial risk is defined in:

<!-- \label{eq:2.2} -->
$$
R_{adv}(F,D) = \frac{1}{\lvert D \rvert}\sum_{(x,y)\in D} \mathcal{L}(f(x+\delta),y)
$$

When dealing with adversarial defenses in the lenses of Robust Optimization, the one first solution, is to solve the combined worst-case loss, with the empirical adversarial risk Radv, known as adversarial training.

<!-- \label{eq:3} -->
$$
\underset{\theta}{min} \frac{1}{\lvert D \rvert}\sum_{(x,y)\in D} \underset{\delta \in \Delta}{max} \mathcal{L}(f(x+\delta),y)
$$

The solution of Equation 3, require special handling or a completely different formulation which define how we categorize the defense mechanisms for adversarial attacks, namely: Adversarial (re)Training, Bayesian Approach, Regularization Approach, and Certified Defenses.

#### 2.3.1 Adversarial (re)Training as a Defense Mechanism

The solution of Equation 3 requires solving the inner maximization (Equation 2), which is a high dimensional nonconvex optimization problem prohibitively hard to solve exactly by standard optimization techniques. The most popular approach to solve such a problem is by approximating Equation 2 with the use of a heuristics in which we are searching for a lower bound for Equation 2. While promising and shown to improve robustness even for large models (ImageNet [55]), such models come with a drawback which when instantiated in practice with the approximation heuristics, they are unable to provide robustness guarantees or certifications. This class of defenses even though very practical to implement can not provide a guarantee that no adversarial example exists in the neighborhood of ${x}$ capable of fooling ${f(.)}$.

#### 2.3.2 Certified Defenses

The problem stated in Equation 3, defines the general objective of adversarial training. But as seen, even with the best methods to find a local approximation to the maximization problem, we are subjective to the effectiveness of the attacking method. A way around this inconvenience has been proposed in the literature, which is to exactly solve the maximization problem or approximate to a solvable set of constraints. To formally define Certified Defenses, initially, we consider a threat model where the adversary is allowed to transform an input ${x \in \mathbb{R}^{d}}$ into any point from a set ${\mathbb{S}_{0} (x) \subseteq R^{d}}$. Such set represents the neighborhood of the point ${x}$ generated by either ${l_{p}}$ perturbations, geometric transformations, semantic perturbations, or another kind of transformation in ${x}$. In case of an ${l_{p}}$ perturbation, the set is defined as ${ \mathbb{S}_0 = \left\{ x^{\prime} \in \mathbb{R}^{d},  {\left\lVert x - x^{\prime} \right\rVert}_{p} < \epsilon \right\}}$.
We further expand the model ${f(.)}$ as a function of its ${k}$ hidden layers and parameters ${\theta}$, where

<!-- \label{eq:4} -->

$$
f(x) = f_{\theta}^{k} \circ f_{\theta}^{k-1} \circ \cdots  \circ f_{\theta}^{1}
$$

in which ${f_{\theta}^{i} : \mathbb{R}^{d_{i}-1} \to \mathbb{R}^{d_{i}}}$ denotes the nonlinear transformation applied in hidden layer ${i}$. The objective is to prove a property on the output of the neural network, encoded via a linear constraint:

$$
c^{T} f_{\theta}(x^{\prime}) + d < 0 ~\forall x^{\prime} \in \mathbb{S}_{0}(x)
$$

in which ${c}$ and ${d}$ are property specific vector and scalar values.

To understand the complexity of the certification, based on Equation 4, we define the layer-wise adversarial optimiza- tion objective. For ${ z_{1} = x, x_{i+1} = f_{i}(W_{i} z_{i} + b_{i})}$

<!-- $$
\begin{split}
\underset{z_{1},\cdots, d+1}{\text{max}}   & (e_{y}-e_{y_{t~arg}})^{T} ~z_{d+1}    \\
\text{s.t.}                                & z_{1}^{\prime} \in \mathbb{S}_{0}     \\
                                           & z_{i+1} = f_{i}(W_{i} z_{i} + b_{i}), i = 1, \cdots, d - 1   \\
                                           & z_{d+1} = W_{d} z_{d} + b_{d}
\end{split}
$$ -->

in which ${e_{i}}$ unit basis, vectors with value 1 in the class ${i^{th}}$ position and zeros everywhere else. Such formulation requires special handling given that we have a nonlinear constraint defined by the activation function. Several techniques have been proposed to solve such a problem and they are within the scope of study on this survey. The method to train certified neural networks is based on the computation of an upper bound to the inner loss, as opposed to a lower bound computed for adversarial training. These methods are typically referred to as provable defenses as they provide guarantees on the robustness of the resulting network, under any kind of attack inside the threat model. Typical methods to compute the certifications are based on convex relaxations, interval propagation, SMT solvers, abstract interpretation, mixed-integer linear programs, linear relaxations, or combinations of these methods. We explore the diverse techniques in subsection 4.3.

#### 2.3.3 Regularization Approach

Regularization techniques focus on making small modifica- tions to the learning algorithm, such that it can generalize better. In a certain way, it improves the performance of the model in unseen data. It prevents model over-fitting to the noise data, by penalizing weight matrices of the nodes. In the specific case of Robust Optimization, the objective of regularization techniques is similar, but focusing on avoiding that small variation on the input, can generate changes in the decision of the algorithm. It does so by either expanding the decision boundaries or limiting changes in the gradient of the model.

Many regularization techniques have been proposed with the most used being the ${l_{p}}$ based ones. The ${l_{2}}$ regularization technique is introduced to reduce the parameters value which translates to variance reduction. It introduces a penalty term to the original objective function (Loss), adding the weighted sum of the squared parameters of the model. With that, we have a regularized loss LR defined as:

$$
\mathcal{L}_{R}(x + \delta, y) = \mathcal{L}(x + \delta, y) + \lambda {\lVert \theta \rVert}_{2}
$$

in which a small ${\lambda}$ lets the parameters to grow unchecked while a large ${\lambda}$ encourages the reduction of the model parameters. Regularization methods are not restricted to ${L_{p}}$ approaches and can involve Lipschitz Regularization, the Jacobian Matrix, and other techniques that we survey on subsection 4.2.

## 3 METHODS FOR GENERATING ADVERSARIAL ATTACKS

Studying adversarial attacks in the image classification domain improve our insights, as we can visually analyze the dissimilarities between disturbed and non-disturbed inputs. Moreover, the image data, even though high dimensional, are simpler represented than other domains such as audio, graphs, and cyber-security data. Along this section, we’ll revise the attack generating algorithms in the image classification domain which can be applied to standard deep neural networks (DNN) and convolutional neural networks (CNN). In which we classify in: white box, black box, and applied real world attacks. In Table 2 we summarize all the attacks described in this section, highlighting the distance metric used, information access level, algorithm type, and the domain it was applied in the specific publication.

In the following sub-sections, we list and describe the most popular approaches for the adversarial attack in ML models. We list them in chronological order and focus on giving the most important details on these methods.

### 3.1 White-box Attacks

As stated, in white box attacks, there is no restriction in the level of information to which the attacker can access. As a consequence the adversary knows model parameters, dataset, or any other information regarding the model. Under such assumption, given a model ${f(.)}$, an input ${(x, y)}$, the main objective is to produce ${x^{\prime}}$, which is within certain distance from the original ${x}$ and maximizes the loss ${\mathcal{L}(f(x+\delta), y)}$.

$$
\underset{\delta\in\Delta}{\max} ~\mathcal{L}(f(x+\delta), y)
$$

#### 3.1.1 Box-Constrained L-BFGS

In, [43], the existence of small perturbations capable of misleading a classifier were first demonstrated. In the paper, Szegedy et. al. proposed to compute an additive noise $\delta$, which could be added to the original input ${x}$, capable ofmisleading the classifier but with minimal or no perceptible distortion to the image. We find the optimal delta, ${\delta}$, with:

<!-- \label{eq:6} -->
$$
\underset{\delta}{\text{min}} ~ c{\lVert \delta \rVert}_{2} \\
\text{s.t.} ~ f(x + \delta) = y^{\prime} ~\text{all pixel in} (x + \delta) \in [0,1]
$$

in which ${f(.)}$ is the parameterized DNN model, ${y}$ is the true label, ${y^{\prime}}$ is the target label. As is this is a hard problem. Using a box constrained L-BFGS the authors proposed an approximate solution to the problem stated as:

<!-- \label{eq:6} -->

$$
\underset{\delta}{\text{min}}~~~c{\lVert \delta \rVert}_{2} + \mathcal{L}(f(x + \delta), y^{\prime}) \\
\text{s.t.} ~ \text{all pixel in} (x + \delta) \in [0,1]
$$

In this model, Szegedy et. al., managed to generate images that were visually indistinguishable from the original ones, but were able to fool the classifiers into classifying them as another class. This was the first result and publication which has exploited this weakness of deep learning models.

#### 3.1.2 Fast Sign Gradient Method

In, [56], introduced a one step adversarial attack framework. The attack image, ${x^{\prime}}$, is obtained by a simple additive disturb:

$$
x^{\prime} = x + \delta_{ut} \\
x^{\prime} = x - \delta_{tg}
$$

in which for the untargeted setting, $\delta{ut}$, we obtain the perturbation from:

$$
\underset{{\lVert \delta_{ut} \rVert}_{p} \leq \epsilon}{\max} ~ \mathcal{L}(f(x+\delta), y)
$$

and in the the targeted setting, $\delta_{tg}$, from:

$$
\underset{{\lVert \delta_{tg} \rVert}_{p} \leq \epsilon}{\max} ~ (\mathcal{L}(f(x+\delta), y) - \mathcal{L}(f(x+\delta), y^{\prime}))
$$

in which ${\epsilon}$ is the ball defined normally by an ${l_{p}}$-norm.

The core of fast sign gradient method maximize the norm of the vector between originally labeled class and the currently assigned label, while in the targeted setting, it focus on minimizing the distance to the target class. As it is a one-step algorithm, it is not very resilient to current defenses but is very fast implementation. Figure 3 shows the attack of an image and the false prediction.

#### 3.1.3 DeepFool

The Deepfool attack proposed by [57], is a white box attack which explores the boundaries of the classification model. In the multi-class algorithm, Deepfool initializes with an input ${x}$ which is assumed to be within the boundaries of the classifier model ${f(x)}$. With an iterative process, the image isperturbedbyasmallvectortowardsthedirectionofthe decision boundaries. The boundaries are approximated by linear functions, more specifically a hyperplane, defined in the algorithm as ${\hat{l}}$. At each step, the perturbations are accumulated to form the final perturbation to the image. With smaller perturbations than in FGSM ([56]), the authors have shown similar or better attack success rate.

#### 3.1.4 Jacobian-based Saliency Map Attack

The Jacobian-based Saliency Map attack (JSMA) differs from most of the adversarial attack literature with respect to the norm it uses on the perturbation restriction. While most of the attacks focus on the ${l_{\infty}}$ or ${l_{2}}$ norms, JSMA, proposed in [58], focus on the l0 norm. Under this norm, penalizes the change in a binary way, if the pixel has been changed or not, opposed to ${l_{2}}$ based algorithms which takes into consideration the size of the change in the pixels.

In this attack, Papernot et. al., calculates the Jacobian of a score matrix ${F}$. The model executes the attack in a greedy way. It modifies the pixel which has the highest impact on the model’s decision. The Jacobian Matrix is defined as:

$$
J_{F} (x) = \frac{\partial F(x)}{\partial(x)} = \left\{ \frac{\partial F_{j}(x)}{\partial x_{i}}  \right\} x \times j
$$

it models the influence of changes in the input ${x}$ to the predicted label ${\hat{y}}$. One at a time pixels from the unperturbed image are modified by the algorithm in order to create a salience map. The main idea of salience map is the correlation between the gradient of the output and the input. It is a guide to the most influential variables of the input, or the ones that probably can deceive the classifier with less manipulation. Based on that, the algorithm performs modifications in the most influential pixel.

### 3.1.5 Projected Gradient Descend (PGD)

Also known as the basic iterative method, was initially proposed in [59]. It is based on the FGSM, but instead a single step of the projected gradient descend, it iterates through more steps, as in:

$$
\delta \coloneqq P(\delta + \alpha \nabla_{\delta} \mathcal{L}(f(x+\delta), y))
$$

in which ${P}$ denotes the projection over the ball of interest. With such formulation, the PGD, requires more fine-tuning, in choosing the step size ${\alpha}$. In [54], Madry et. al. proposed an iterative method with a random initialization for ${\delta}$.

#### 3.1.6 Carlini and Wagner Attacks (CW)

In [60], 3 ${l_{p}}$-norm attacks ${(l_{0}, l_{2}, l_{\infty})}$ were proposed as a response to [61], which proposed the use of Distillation as a defense strategy. In their paper, Papernot et. al., successfully presented a defense mechanism capable of reducing the effectiveness of the FGSM and L-BFGS. CW proposes to solve the same problem stated in FGSM, which is given an input ${x}$ find a minimal perturbation ${\delta}$ capable of shifting the classification prediction of the model. The problem is addressed as:

<!-- \label{eq:8} -->

$$
\underset{\delta}{\min} ~ c{\lVert \delta \rVert}_{p} + \mathcal{L}(f(x + \delta), y^{\prime}) \\
$$

$$
\text*{s.t.} ~ (x + \delta) \in {\left[0, 1\right]}^{n}
$$

in which ${\mathcal{L}(f(x + \delta), y^{\prime}) = \max_{i \neq j}(Z(x^{\prime}_{i}) - Z(x^{\prime})_{y})^{+}}$, and ${Z(x) = z}$ are the logits. As the algorithm minimizes the metrics ${\mathcal{L}(.)}$, it finds the input ${x^{\prime}}$ that has larger score to be classified as ${y^{\prime}}$. As we search for the value of ${c}$, we look for the constant which will produce the smaller distance between ${x}$ and ${x^{\prime}}$.

#### 3.1.7 Ground Truth Adversarial Example (GTAE)

So far most of the attacks, even if motivated by the generation of new defenses, are independent of the defense algorithm. In the algorithm proposed by [62], the certified defense proposed in [63], is used as a base for the optimization and search for adversarial examples.

The algorithm abstract the ${\theta}$ and dataset ${(x, y)}$ with the use of an SMT solver, and solves the system to check if there exist ${x^{\prime}}$ near ${x}$, within the established norm distance, which can cause a misclassification. The ground truth adversarial example is found by reducing the size of ${\epsilon}$ up to the point that the system can no longer find a suitable ${x}$ 0 . The adversarial example is considered the ${x^{\prime}}$ found with the immediately larger ${\epsilon}$. It is the first method to calculate an exact provable solution to a minimal perturbation which can fool ML models. In contrast, as stated by the authors, the fact that the model relies on an SMT solver, restrict the applicability of the algorithm to models with no more than a few hundred nodes. This attack has been revisited by [64] and [65].

#### 3.1.8 Universal Adversarial Perturbations

Different from the previous methods, the universal adversarial perturbation (UAP), proposed in [66], search for a single perturbation capable of fooling all samples from the training dataset. The perturbations, independent of the input, are also restricted to not be detected by humans. The perturbations are constructed based on:

$$
P_{x \sim D} \left( f(x) \neq f(x+\delta) \right) \geq \beta, \text{s.t.} {\lVert \delta \rVert}_{p} \leq \epsilon
$$

in which ${\epsilon}$ defines the size of the perturbation based on an ${l_{p}}$-norm and ${\beta}$ defines the probability of an image sampled from the training dataset being fooled by the generated perturbation. In this case, the algorithm optimizes the probability of fooling the classifier. The method to calculate the universal perturbations is based on the DeepFool algorithm, in which the input is gradually pushed towards the model’s decision boundary. It differs from DeepFool in the fact that instead of pushing a single input, all members of ${\mathcal{D}}$ are modified in the direction of the decision boundary. The perturbations, calculated for each image, are accumulated in a gradual manner. The accumulator is then projected back towards the specified ${B_{\epsilon}}$ ball, of radius ${\epsilon}$. Its been shown that with variations of 4%, a fooling accuracy of 80% has been achieved.

#### 3.1.9 Shadow Attack

In [67], an attack targeting certified defenses was proposed. In their work, they target defenses that certify the model with respect to a radius defined by the ${l_{p}}$-norm. One intuitive idea to construct a certified defense is to check within a certain radius ${B_{\epsilon}}$ of input, the existence of a perturbation ${\delta}$, capable of changing the decision of the classifier. The shadow attack is constructed to leverage this premise, and construct a perturbation outside of the certification zone. It is claimed that after labeling an image, these defenses check whether there exists an image of a different label within ${\epsilon}$ distance (in ${l_{p}}$ metric) of the input, where ${\epsilon}$ is a security parameter selected by the user. If within the ${B_{\epsilon}}$ ball all inputs are classified with the same label, then the model is robustly certified. Their model targets not only the classifier but also the certificate. It is done by adding adversarial perturbations to images that are large in the ${l_{p}}$-norm and produce attack images that are surrounded by a large ball exclusively containing images of the same label. In order to produce images that are close to the original, in a perception way, but can fool the classifier, they use the following objective function: 

$$
\underset{y^{\prime} \neq y, \delta}{\max} ~ -\mathcal{L}(\theta, x + \delta \vert y^{\prime}) - \lambda_{c}C(\delta)
$$

$$
-\lambda_{tv} TV (\delta) - \lambda_{S}Dissim(\delta)
$$


in which ${\mathcal{L}(\theta, x+\delta \vert \bar{y})}$ refers to the adversarial training loss, ${\lambda_{c}C(\delta)}$ is a color regularization term, ${\lambda_{tv} TV (\delta)}$ is a smoothness penalty term, and ${\lambda_{S}Dissim(\delta)}$ guarantees that all color channels receive similar perturbation.

#### 3.1.10 Other Attacks

The presented attacks are just part of many more which have been published in many different venues. Here we list some other relevant attack methods available in the literature. 

* **EAD: Elastic-net attack** - Similar to L-BFGS the algorithm in [68] proposes to find the minimum additive perturbation, which misleads the classifier. Differently it incorporates an association of the norms ${l_{1}}$ and ${l_{1}}$. It has been shown that strong defenses against ${l_{\infty}}$ and ${l_{1}}$ norms still fail to reject ${l_{1}}$ based attacks. 
* **Objective Metrics and Gradient Descend Algorithm (OMGDA)** - The algorithm proposed by [69], is very similar to DeepFool, with the optimization of the step size. Instead of utilizing a fixed and heuristically determined step size in the optimization, in Jang et al., the step size utilizes insights from the softmax layer. The step size is determined based on the size of the desired perturbation and varies over time. 
* **Spatially Transformed Attack (STA)** - In [70], instead of generating changes in the intensity of the pixels, the authors have proposed a method based on small translational and rotational perturbations. The perturbations are still not noticeable by the human eyes. Similarly in [71] the spatial aspect of the input is also exploited for the adversarial example generation. 
* **Unrestricted Adversarial Examples with Generative Models (UAEGM)** - Based on AC-GAN ([72]), [73], has proposed the use of generative networks to generate examples which are not restricted to being in the neighborhood of the input data. The generated attacks, are not necessarily similar to the ones in the dataset but are similar enough to humans not notice and fool the classifiers.

### 3.2 Black-Box Attacks

Under Black box restriction, models are different from the currently exposed white box, with respect to the information the attacker has access to. In most cases, the adversary does not have any or some information about the targeted model, like the algorithm used, dataset, or parameters, as seen in Figure 2. An important modeling challenge for black-box attacks is to model precisely what information the attacker has about either the learned model or the algorithm. In this sub-section, we list the most relevant methods for black attack generation. 

#### 3.2.1 Practical Black-Box Attacks (PBBA) 

While assuming access to the information of the model enables a series of attacks, the work in [74], introduces the possibility of attacking models about which the attacker has less knowledge. In this work, no knowledge about the architecture is assumed, only some idea of the domain of interest. It is also limited the requests for output sent to the model, which requires the attacker to choose wisely the inference requests from the victim’s model. To achieve the goal, Papernot et. al. introduce the substitute model framework. The attack strategy is to train a substitute network on a small number of initial queries, then iteratively perturb inputs based on the substitute network’s gradient information to augment the training set

```

```

With the boundaries of substitute model adjusted to be close to the original model, any of the methods presented in the previous section can be used to generate the perturbed image.

#### 3.2.2 Zeroth Order Optimization Based Attack

In Chen et al. [52], in the attack also known as ZOO, the authors assume accessibility to both, the input data and the confidence scores of the model in which they target their attack. They differ from [74] in the fact that the model does not focus on transferability (creating a substitute model) to achieve the adversarial examples. In their work, they propose a zeroth-order optimization attack which estimates the gradient of the targeted DNN. Instead of traditional gradient descend, they use order 0 coordinate SGD. Moreover, to improve their model and enable the adversarial example generation, they implement dimensionality reduction, hierarchical techniques, and importance sampling. As the pixels are tuned, the algorithm observes the changes in the confidence scores. 

Similar to the ZOO, in the one-pixel attack [75], it is proposed the use of the score confidence to perturb the input and change the decision of the classifier. This paper focuses on modifying a single pixel of the input. With the use of differential evolution, the single pixel is modified in a blackbox setting. The authors base the update of the perturbation in the variation of the probability scores for each class.

#### 3.2.3 Query-Efficient Black-Box Attacks

One of the biggest challenges in black-box attacks is the fact that many inference models have mechanisms to restrict the number of queries (when cloud-based or system embedded), or the inference time can restrict the number of queries. One line of research in black-box models looks into making such models more query efficient, for example, the work from [76], based on natural evolution strategies reduces by 2 or 3 order of magnitude the amount of information requests sent to the model to successfully generate a misclassified perturbed image. The algorithm set queries in the neighborhood of the input ${x}$. The output of the model is then sampled, and these samples are used to estimate the expectation of the gradient around the point of interest.

The algorithm sample the model’s output based on the queries around the input x, and estimate the expectation of a gradient of ${F}$ on ${x}$. More on the topic, [77], proposes a family of algorithms based on a new gradient direction estimate using only the binary classification of the model. In their work, it is included ${l_{\infty}}$ and l2 norm-based attacks as well as targeted and untargeted attacks. Figure 4 shows an intuition on how the gradient is updated and the boundaries of the decision are used to generate the adversarial attack. Algorithm 2 shows how the dimensionality reduction ${d^{r}}$ is defined. Moreover, in the search for query efficient black box attack, [78] introduces a method that is independent of the gradient based on Bayesian optimization and Gaussian process surrogate models to find effective adversarial examples. In the model it is assumed that the attacker has no knowledge of the network architecture, weights, gradient or training data of the target model. But it is assumed that the attacker can query model with input ${x}$, to obtain the prediction scores on all classes ${C}$. They restrict the perturbation to the ${l_{\infty}}$ norm. Their objective is to maximize over the perturbations: 

<!-- \label{eq:10} -->
$$
\delta^{*} = \underset{\delta}{\arg\min}\left[\log\left(f(x_{origin}+g(\delta))_{t}\right)-\log(\sum_{j\neq{}t}^{C}{f(x_{origin} + g(\delta))_{j}})\right]
$$

$$
\text{s.t.} ~ \delta \in [-\delta_{max}, +\delta_{\max}]^{d_{r}}
$$

The Bayesian optimization proposed to improve the query efficiency requires the use of a surrogate model to approximate the objective function, in their work a Gaussian Process is used. Moreover to define the next query point is defined by an acquisition function. A big differential in their work is the fact that instead of searching in a high-dimensional space for a perturbation ${\delta}$, they utilize a function to reduce the dimensionality of the perturbation and later reconstitute to the true image size. 

```

```

#### 3.2.4 Attack on RL algorithm

In [79], a method for generating adversarial examples in reinforcement learning (RL) algorithms was proposed. In RL, an adversarial example can either be a modified image used to capture a state or in the case of this publication, an adversarial policy. It is important to highlight that an adversarial policy is not a strong adversary as we have in two-player games, but one that with a certain behavior triggers a failure in the victim’s policy. In this paper, a blackbox attack is proposed to trigger bad behaviors in the victim’s policy. The victim’s policy is trained using Proximal Policy Optimization and learns to "play" against a fair opponent. The adversarial policy is trained to trigger failures in the victim’s policy. Figure 5 shows the difference between an opponent’s policy and an adversarial manipulated policy. Also in the paper, it was shown the dependence of the size of the input space and the effectiveness of adversarial policies. The greater the dimensionality of the observation space under the control of the adversary, the more vulnerable

### 3.3 Physical World Attack 

The research presented so far is mostly focused on applying the attacks in virtual applications and controlled datasets, but the great concern about the existence of adversarial examples is the extent to which they can imply severe consequences to the users of the system. With that objective, we dedicate this session on exploring publications with realworld applications and consequences clearly stated. 

In [80], road signs were physically attacked by placing a sticker in a specific position of the sign. Their attack consisted of initially finding, in a sign image, the location with the most influence for the decision of the classifier. For that objective, an l1norm was used because it renders sparse perturbations in the image, making it easier to locate the modification patches. Based on the defined location, an ${l_{2}norm}$ was used to identify the most appropriate color for the sticker. 

Moreover, as face recognition is becoming very popular as a biometric security measure it is the focus of several adversarial attacks. We highlight 4 attacks in face recognition and identification.

* **Evaluation of the robustness of DNN models to Face Recognition against adversarial attacks** - In this publication Goswami et al. [81] evaluates how the depth of the architecture impacts the robustness of the model in identifying faces. They evaluate the robustness with respect to adversarial settings taking into consideration distortions which are normally observed in a common scene. These distortions are handled with ease by shallow networks on the contrary of deep networks. In their approach, they’ve used Open-Face and VGG-Face networks, and have achieved a high fooling rate. It is important to notice that in their attack no restrictions are made in the visibility of the perturbations. 
* **Adversarial Attacks on Face Detectors using Neural Net based Constrained Optimization** In this work, also focusing on prevent face identification, [82] has generated an attack, based on Carlini and Wagner attack, which was able to fool R-CNN. Their perturbations are not visible in the adversarial example. 
* **Generating Adversarial Examples by Makeup Attacks on Face Recognition** - In this research, [83] implement a GAN network to generate make-up perturbation. When the perturbation is applied to the face, the classifier shifts its decision to the target class. 
* **Efficient decision-based black-box adversarial attacks on face recognition** - Dong et al. [84] propose an evolutionary attack method. The proposed adversarial example generator is constrained in a black-box setting. The algorithm focus on reducing the number of dimensions of the search space by modeling the local geometry of the search vectors. Such an algorithm has been shown to be applicable to most recognition tasks.

#### 3.3.1 Other Attacks 

In the field of Cyber-security machine learning models are, in general, applied to detect malware, malicious connections, malicious domain classifier, and others. In Suciu et al. [85] an evaluation of the robustness of current malware detection models is performed. The authors retrain the model in a production-scale dataset to perform the evaluation. With the new data, the model which was previously vulnerable to attacks was shown to be stronger and architectural weaknesses were reported. The work of Suciu et al. [85], explores how attacks transfer in the cyber-security domain, and mainly the inherent trade-off between effectiveness and transferability. With respect to malicious connection and domain, Chernikova et al. [86] builds a model that takes into consideration the formal dependencies generated by the normal operations applied in the feature space. The model to generate adversarial examples simultaneously consider both the mathematical dependencies and the real-world constraints of such applications. The algorithm focus on determining the features with higher variability and the ones with a higher correlation with these features. This search is performed for each iteration. All the identified features are modified but constrained to preserve an upper bound on the maximum variation of the features. The upper bound respects physical-world application limitations. 

In the Cyber-Physical domain, several publications demonstrate the brittleness of ML models and the generation of adversarial examples. [87] generated adversarial examples to an iCub Humanoid Robot. The attack proposed simply extends over the attacks in [88]. The main aspect to be considered in this paper is the fact that it highlights the high consequences of the adversarial examples in the decision process of safety-critical applications. Moreover, in selfdriving cars several attacks have been derived like DARTS ( [89]) and the work of [90] which shows the attack of traffic signs, the latter with real experiments. In a different sensor type, the work of [91] demonstrates the attack to a LIDAR sensor, in which they attack the point cloud image. 

Moreover in [92], a novel technique was proposed to attack object tracking algorithms. In their approach, the bounding box is attacked in a single frame, which is enough to fool the algorithm and generate an offset in the placement of the bounding box. Such an attack would be critical to self-driving cars to recognize the position of obstacles, other vehicles, and pedestrians on the road.

## 4 DEFENSE MECHANISMS BASED ON ROBUST OPTIMIZATION AGAINST ADVERSARIAL ATTACKS

From the examples presented so far, we see that both DNN’s and CNN’s are very unstable locally and susceptible to misclassify examples with perturbations that are barely perceivable by the human eye. Several works have reported structured algorithms and formulations to improve the robustness of ML models employing robust optimization. The goal of this section is to visit the most commonly identified techniques to achieve robustness (through the eyes of the optimization), namely Adversarial Training, Bayesian Approach, Certified Defenses, and Regularization Approaches, which are summarized in Figure 6. We summarize in Table 3 the results presented in the surveyed papers. We’ve compiled the information available in each of those papers concerning the error rate under the tested conditions. Each of the papers has different evaluation criteria and conditions. The dataset is highly influential in the accuracy of the model. To that end, we’ve been careful to better express the conditions in which the models were evaluated. Papers in which the results were not clear or have used a very specific metric are not listed in these tables to keep consistency among the results.

### 4.1 Defending through Adversarial (re)Training

After the first introduction of adversarial examples ( [43] and [88]), defense mechanisms to train robust neural networks were built based on the inclusion of adversarial examples to the training set. Models trained using adversarial training with projected gradient descent (PGD) were shown to be robust against the strongest known attacks. This is in contrast to other defense mechanisms that have been broken by new attack techniques. In this section, we explore robustness mechanisms that explicitly or implicitly address the robustness in deep learning through either adding adversarial examples in the dataset or incorporate them in the objective function for the optimization.

#### 4.1.1 Harnessing Adversarial examples

In his work, [56] Goodfellow et. al. suggests the use of adversarial examples in the training process to improve the robustness of machine learning models. It is a very simple idea, which worked for the proposed configuration. The algorithm, using FGSM in an untargeted setting, would generate a set of adversarial examples ${x^{\prime}}$ , which were fed to the learning algorithm with the true label, ${(x^{\prime}, y)}$. Important to notice the limitation of such a framework, it is robust against FGSM attacks, but susceptible to other attacks, such as iterative methods. Such weakness was pointed in a later work by [107], which also shown single-step attacks could fool such a defense. In [98], it is studied an adaptation of FGSM in adversarial training. The initially proposed FGSM training was shown to create a massive over-fitting in the model having it not robust to iterative attack methods such as PGD. In this new publication, Wong et. al., proposes small modifications in the initialization of the FGSM algorithm to accommodate randomness as a way to prevent over-fitting in the training process. Instead of having a fixed initialization, the perturbations are generated as follows:

1. ${\delta = Uniform(-\epsilon, \epsilon)}$
2. ${\delta = \delta + \alpha~ sign(\nabla_{\delta} \mathcal{L}(f_{\theta}(x_{i}+\delta), y_{i}))}$
3. ${\delta = \max(\min(\delta, \epsilon), -\epsilon)}$

The addition of the sampling from a uniform distribution to the initial perturbation allowed the algorithm to better estimate the inner maximization, improving the robustness of adversarially trained models with FGSM, maintaining the speed of the adversarial example generation. 

4.1.2 Towards Deep Learning Models Resistant to Adversarial Examples In their publication, Madry et. al. [54], among an extensive discussion of robustness in the machine learning models, propose to incorporate iterative methods to approximate the inner maximization problem shown in:

$$
\underset{\theta}{\min} \left( \frac{1}{D} \sum_{(x,y)~\in~D}{\underset{\delta~\in~\Delta(x)}{\max} \mathcal{L}(f(x+\delta), y)} \right)
$$

where ${\delta}$ is the perturbation, ${f(.)}$ is the model. In this publication, the authors have approximated the inner maximization problem with the PGD attack method, but it is important to highlight in this formulation, that the model will be as robust as the attack it was trained on. If new and more efficient adversarial examples are presented to the model at inference time, nothing can be stated regarding the robustness of the model.

#### 4.1.3 Ensemble Adversarial Training 

In their research [107], the authors show that when the model is trained directly on single/iterative methods, the model trains on examples crafted to maximize a linear approximation of the loss. Moreover, such a model training method converges to a degenerate global minimum. These artifacts near the inputs ${x}$ obfuscate the approximation of the loss. According to their findings, the model learns weak perturbations rather than generating robustness against strong ones. As a countermeasure, the paper implements a framework in which the model is trained with adversarial examples from similar classifiers. In their method, the generation of adversarial examples is dissociated from the network being optimized. Under such circumstances, their proposed framework approximates to the work in which black-box attacks are generated with the use of auxiliary models, and consequently makes their approach more resilient to it. In the algorithm, to train classifier ${F_{0}}$, they train other sets of classifiers, ${F_{1}, F_{2}, ..., F_{n}}$, generate adversarial examples for these classifiers, and use the adversarially generated examples to train ${F_{0}}$. Their defense model even for the ImageNet dataset, has shown robustness against black-box attacks. In a different work, using a similar concept, [106] proposes the augmentation of the model robustness with the use of random noise layers to prevent the strong gradient attacks. It is a gradient masking attack, but the authors position the algorithm as an improvement to ensemble adversarial defense, by stating that the defense is equivalent to ensembling an infinite number of noisy models. Such ensembling is claimed by the authors to be equivalent to training the original model with a Lipschitz regularizing term. They’ve achieved significant results against Carlini and Wagner’s strong attacks. Algorithm 3 shows the steps to implement the proposed algorithm.


```
```

More recently in [105], a mixed-precision ensemble was proposed. Ensemble of Mixed Precision Deep Networks for Increased Robustness (EMPIR) is based on the observation that quantized neural networks often show higher robustness to adversarial attacks than full precision networks. Such models sacrifice accuracy. EMPIR combines the accuracy of full models with the robustness of quantized models by composing them in an ensemble.


#### 4.1.4 Principled Adversarial Training 

In [104], Sinha et. al. have presented a method to solve the outer maximization with a Lagrange relaxation approach. By doing so, they presented both, an adversarial training method based on the min-max formulation and a method to prove the robustness of the model. In addition to the adversarial training, a penalty is added on the loss term to help regularize the learning. It is used as a Lagrangian formulation to generate this penalty. It perturbs the underlying data distribution within a Wasserstein ball. The model’s efficiency is restricted to smooth losses, but under such constraint, it can achieve a moderate level of robustness. The computational or statistical cost, when compared to empirical risk minimization is small.

#### 4.1.5 Network Robustness with Adversary Critic

The Adversarial Training formulation is an optimization problem that naturally involves minimization and maximization. In [103] and [116] the GAN framework is proposed to generate the noisy perturbations, which will lead to the adversarial example. On the other-hand the discriminator of the network act as a critic which will discern if the presented input ${x}$ is adversarial or not. In the work, it is highlighted that the generated adversarial networks are also robust to black-box attacks, showing similar or better performance than state-of-the-art defense mechanisms.

4.1.6 Adversarial Logit Pairing In [54] Madry et. al. suggested the use of the Equation 11 to adversarial train the model, and achieve robustness in their model. Kannan et al. [102] implements a mixed version of this defense. Instead of training the robust model only on adversary perturbed images, they incorporate a mix of clean ${(x, y)}$ and perturbed ${(x^{\prime} , y)}$ batches, which they call mixed mini-batch PGD (M-PGD). 

In their work, they go beyond to analyze the fact that most of the adversarial training framework, train the models with information that ${x^{\prime}}$ should belong to the class ${t}$, but the model is not given any information indicating that ${x^{\prime}}$ is more similar to the actual sample ${x}$ than any other belonging to the same class. To that extent, they propose another algorithm called adversarial logit pairing. 

For a model ${f(.)}$ trained on a mini-batch ${\Gamma}$ of clean examples ${\{x_1, x_2, ..., x_m\}}$ and corresponding adversarial examples ${\{x_{1}^{\prime}, x_{2}^{\prime}, ..., x_{m}^{\prime}\}}$, with ${f(x)}$ mapping the input to a prediction. With ${\mathcal{L}(\Gamma, \theta)}$ being the cost function used to perform the adversarial training, the adversarial logit pairing consists of minimizing the loss: 

$$
\mathcal{L}(\Gamma, \Theta) + \lambda \frac{1}{m} \sum_{i=1}^{m} {\mathcal{L}\left(f(x_{i}), f(x_{i}^{\prime})\right)}
$$

#### 4.1.7 ME-Net 

ME-Net [101] introduces the concept of utilizing matrix estimation as a way to augment the adversarial sample size and eliminate the perturbations from adversarial examples. The training procedure to ME-Net is described as follows, the algorithm creates a mask in which each pixel is preserved with probability ${p}$, and set to zero with probability ${1 - p}$. For each input image ${x}$, n masks are applied, with different pixel drop probability. The generated masked images, ${X}$, are then processed through a Matrix Estimation algorithm, which would obtain the reconstructed images ${\hat{X}}$. The DNN model is trained on the reconstructed images, which can be further processed with the use of more adversarial training techniques. For inference, to each input ${x}$, a mask is randomly sampled from the pool of masks obtained in the training time, applied to  ${x}$, and then reconstruct to generate ${\hat{x}}$. The process of masking and reconstructing the images is claimed by the authors to reduce the effects of adversarial perturbations in the image.

#### 4.1.8 Robust Dynamic Inference Networks 

In [99], an input-adaptive dynamic inference model to the adversarial defense is proposed. In this method each input, regardless of clean or adversarial samples, adaptively chooses which output layer to take for its prediction. Therefore, a large portion of input inferences can be terminated early when the samples can already be inferred with high confidence. The benefit of the use of such models comes from the fact that the multiple sources of losses provide much larger flexibility to compose attacks (and defenses), compared to the typical framework. In this work, a methodology to attack and defend in such models is proposed.

#### 4.1.9 Defending against Occlusion Attacks

In [97], the authors investigate defenses against physically realizable attacks, more specifically they investigate defenses to attacks in which part of the object is occluded by a physical patch. It’s been demonstrated that adversarial training with either PGD or Randomized Smoothing did not improve the robustness of the models significantly. In their work, they propose, implement, and use a Rectangular Occlusion Attack(ROA). The ROA enabled the emulation of physical attacks in the virtual world and the consequent training of adversarial resistant models. 

#### 4.1.10 Robust Local Features for Improving the generalization of Adversarial Training 

In their research,Song et al. [96] investigates the fact that when models are trained with no adversarial techniques they gather better information about local features, which improves the model’s generalization ability. Opposed, DNN models which were adversarially trained tend to have a bias in favor of a global understanding of the features. In their work, they propose a method to train adversarial robust models, which are biased towards local features. In their work they define the Random Block Shuffle, to randomize the features of input inside the image. Such an approach prevents the adversarial learning method from learning only global features. The model learns from a combination of shuffled and unshuffled images.

<!-- \label{eq:12} -->
$$
TODO
$$

In the loss defined in Equation 12, the factor η balances the contribution between the local feature-oriented loss and the global oriented.

#### 4.1.11 Misclassification Aware Adversarial Training

In [95], the authors propose the analysis of the misclassification examples, intending to improve the accuracy of the model against perturbed inputs. To perform such a study they’ve trained a classifier with 10-step PGD, obtaining 87% training accuracy, they extracted the 13% misclassified examples and sampled 13% correctly classified examples from the training dataset. The examples originally misclassified by the model, are the ones that impact the most the final robustness. Compared to standard adversarial training the final robustness drops drastically if misclassified examples are not perturbed during adversarial training. In contrast, the same operation on sampled correctly classified examples only slightly affects the final robustness. 

Based on the observations a regularization term is proposed to incorporate an explicit differentiation of misclassified examples. Initially they propose a Boosted Cross Entropy loss, defined as:

$$
TODO
$$

in which ${p_{t}(\hat{x}_{i}^{\prime}, \theta)}$ is the softmax on logits of ${x_{i}}$ belonging to class ${t}$. With that the objective function is defined as: 

$$
TODO
$$

#### 4.1.12 Input Transformation Methods

Input transformation methods propose to train the network in transformed images, such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting or an ensemble of these methods to improve the robustness of the model.In [128], the use of JPEG compression was proposed as a countermeasure of the pixel displacement generated by the adversarial attacks. In [129], a combination of total variation minimization and image quilting is used to defend against strong attacks. Even-though these transformations are nonlinear, a neural network was used to approximate the transformations, making them differentiable, and consequently easier to obtain the gradient. Different in [130], an ensemble of weak transformation defenses was proposed to improve the robustness of the models, among the transformations included in the defense are: color precision reduction, JPEG noise, Swirl, Noise Injection, FFT perturbation, Zoom Group, Color Space Group, Contrast Group, Grey Scale Group, and Denoising Group.

### 4.2 Regularization Techniques 

As shown in subsection 3.1, several algorithms depend on the model’s gradient to estimate the local optima perturbations which will fool the classifier. A stream of research towards adversarial robust optimization, look into applying regularization approaches to reduce the influence of small perturbations in the input on the output decisions. In this section, we review some relevant publications in the field in which the author’s objective is to improve the robustness of the model.


#### 4.2.1 Towards DNN Architectures Robust to Adversarial Examples

In their work, Gu et. al. [127] propose the use of Contractive Autoencoder. To regularize the gradient, they add a penalty to the loss in the back-propagation concerning the partial derivatives at each layer. By incorporating a layer-wise contractive penalty (in partial derivatives), they show that adversarial generated from such networks have significantly higher distortion. In their approach, the network could still be fooled by adversarial examples, but the level of noise necessary to fool such a network is considerably higher than standard networks in which there is no contractive penalty.

#### 4.2.2 Robust Large Margin Deep Neural Networks 

The work presented in [126] analyzes the generalization error of DNN’s through their classification margin. In their work, they initially derive bounds to the Generalization Error (GE) (adversarial attacks as consequence) and express these bounds as a dependence of the model’s Jacobian Matrix (JM). In their work, it was shown that the depth of the architecture does not affect the existence of a GE bound, conditioned to fact that the spectral norm of the JM is also bounded, around the training inputs. With this definition a weigh and batch normalization regularizer is derived. The regularizer is based on the bound derived based on the JM.

#### 4.2.3 Input Gradient Regularization 

Parseval networks [125] is a layer-wise regularization method to reduce the networks variability to small disturbances in the input ${x}$. The work starts with the principle that DNN’s are a composition of functions presented by layers. To keep the variability of the output controlled, they propose to maintain the Lipschitz constant small at every hidden layer (for fully connected, convolutional, or residual layer). For that, they analyze the spectral norm of the weight matrix. 

In [124], Ros et. al. based on the same principle of gradient regularization, propose the use of such techniques to improve both the robustness and interpretability of Deep Neural Networks. The authors claim that raw input gradients are what many attacks use to generate adversarial examples. Explanation techniques that smooth out gradients in background pixels may be inappropriately hiding the fact that the model is quite sensitive to them. They hypothesize that by training a model to have smooth input gradients with fewer extreme values, it would not only make the model more interpretable but also more resistant to adversarial examples. Their gradient regularization is given by:

$$
TODO
$$

in which ${\lambda}$ specifies the penalty strength. The goal of this update is to ensure that if any input changes slightly the KL divergence between predictions will not change significantly.

#### 4.2.4 DeepDefense 

Yan et al. [123] propose the algorithm called DeepDefense focusing on the improvement of the robustness of the DNN models, which is an regularization method based on the generated adversarial perturbation. Similar to adversarial (re)Training the algorithm incorporates the adversarial generation the loss function. It does not occur as shown in Equation 3, differently it is presented as: 

$$
TODO
$$

in which we see that at the same time that the loss is minimized a regularization term based on the perturbation ${\delta_{x_{k}}}$ is added to penalize the norm of the adversarial perturbations. The penalty function ${R(.)}$, treats the samples different depoending if they were correctly classified or not, it increases monotonically when the sample is correctly classified. With such a behavior the function gives preference to those parameter settings which are able to resist even to small ${\frac{{\lVert \delta_{x_{k}} \rVert}_{p}}{{\lVert x_{k} \rVert}_{p}}}$.

#### 4.2.5 TRADES

In their work, Zhang et. al. [121] states the intrinsic trade-off between robustness and accuracy. In their work, they derive a differentiable upper bound for the natural and boundary errors of the DNN model. To derive such bound, the error generated by the adversarial examples (called robust error) is decomposed in two parts: 1 - the natural misclassification, and 2 - the boundary errors. The bounds are shown to be the tightest overall probability distributions. Based on these bounds a defense mechanism called TRADES is proposed. In its core, the algorithm still minimizes the natural loss, consequently increasing the accuracy of the model, but at the same time, it introduces a regularization term, which induces a shift of the decision boundaries away from the training data points. The expansion of the decision boundaries can be seen in Figure 7.

#### 4.2.6 Metric Learning for adversarial Robustness

[120] focuses on learning a distance metric for the latent representation of the input. Through an empirical analysis, the authors have observed that inputs under PGD adversarial attacks shift its latent representation to a false class. The shift in the latent representations spread in the false class and become indistinguishable from the original images in the class. In their paper, they’ve added a new constraint to the model with metric learning. Their model implements a variation of the naive triplet loss, called TLA (triplet loss adversarial training), which overcome the variance of the adversarial data over the false class. The TLA works by approximating the samples from the same class, independently if they are adversarial or unperturbed samples, and enlarge the boundary distance concerning other classes. 

#### 4.2.7 Adversarially Robust Transfer Learning 

In [131], a study on transfer learning for robust models is performed. In their paper, they robustly train a WideResNet 32-10 [132] using the algorithm proposed in [54]. In their first experiment, they sub-divide the model in blocks of layers and analyze the impact of changing the number of unfrozen blocks in the transfer learning process. It was seen that when only the fully connected layer and the batch normalization blocks were re-trained, the network had similar or improved robustness in the new domain. Opposed, when more blocks were re-trained the accuracy and robustness dropped drastically. The authors claimed that the robust model’s feature extractors act as filters that ignore the irrelevant parts of the images. 

With the intent of improving the overall performance of classifiers transferred from a robust source model by improving their generalization on natural images, the authors proposed an end-to-end transfer learning model with a noforgetting property. To do so, they only fine-tune the feature extraction parameters ${\theta}$. It consists basically of the addition of a regularization term to the loss of the model.

$$
TODO
$$


###  4.3 Certified Defenses 

Certified defenses try to theoretically find certificates in distances or probability to certify the robustness of DNN models. These methods are explored in this section. 


#### 4.3.1 Exact Methods

The work of [63], was a big first step in the direction of formalizing methods to certify the robustness of neural networks. In their work, also, to provide a formulation for SMT solver for the ReLU activation function, they’ve proven safety in a small neural network for aircraft collision prediction. They were able to use their solver to prove/disprove local adversarial robustness for their example DNN for a few arbitrary combinations of input ${x}$ and disturbances ${\delta}$. In their experiments, the verification took from few seconds to a few hours depending on the size of the perturbation, with bigger perturbations taking longer to be verified. Expanding over the Reluplex idea, the work in [119], provide a more complete framework to prove formulas over a DNN. The key concept of ${AI^{2}}$ is the use of abstract interpretation to approximate mathematical functions with an infinity set of behaviors into logical functions witch are finite and consequently computable. To evaluate a DNN they propose to over-approximate the model in the abstract domain with the use of logical formulas capable of capturing certain shapes. Figure 8 shows the exemplification of how the abstraction of the layers is used to evaluate properties in the DNN. 

Building on the work of [119], Singh et al. [118] published a work in which not only the ReLU activation function is available, but also Sigmoid and TanH. In their work, they implement a parallel version of the layer transformation which improved significantly the verification speed.

#### 4.3.2 Estimating the lower bound 

In their work,Hein et al. [117] the search of model robustness through the optics of formal guarantees. They’ve proposed in their work, a proven lower bound which establishes the minimal necessary perturbation to change a model’s decision. "*We provide a guarantee that the classifier decision does not change in a certain ball around the considered instance*" [117]. Moreover, based on the proposed Cross-Lipschitz Regularazation method, they show the increase in the adversarial robustness of the models trained with such regularization. The generated bound is defined as:

$$
TODO
$$  


It is known that an exact solution for the optimization problem which leads to the certification of DNN’s is intractable for large networks. In [116], they propose CLEVER (Cross Lipschitz Extreme Value for nEtwork Robustness). In their work, the lower bound is defined as the minimum ${\epsilon}$ necessary to be added in the input to change the decision of the model in an adversary setting.

Based on extreme value theory, the CLEVER metric is attack agnostic and is capable of estimating the lower bound for an attack to be effective in any model. But it restricts itself on providing a certification, it only provides an estimate of the lower bound. Improving from CLEVER, [115] provides a certified lower bound for multi-layer perceptrons restricted to ReLU activation. In [114], CROWN is proposed extending the exact certification to general activations. In [113] the same research group proposed the CNN-Cert a framework to certify more general DNN.

In Zhang et al. [133], a mix between inner bound propagation and linear relaxation is proposed. Linear relaxation of Neural Networks is one of the most popular methods to provide certified defenses and uses linear programming to provide linear relaxation, also known as the (convex adversarial polytope). Even though such methods generate an implementation that is tractable and solvable, they are still very demanding of computational resources. IBP on the contrary is less complex and brings more efficiency when optimizing verifiable networks. Also known as interval bound propagation (IBP), are in general loose during the initial training phase which generates instabilities in the training and makes the model very sensitive to the hyperparameters.

$$
TODO
$$

in which ${\Psi}$ is the IBP bound, ${\phi}$ is the CROWN-IBP bound, and ${\underline{m}(x, \epsilon)}$ is the combination of both bounds.

#### 4.3.3 Upper Bounding the adversarial Loss 

In the works of [112] and [111] the certification of robustness is searched through means of defining an upper bound for the adversarial loss. For an adversarial loss defined as: 

$$
TODO
$$

both try to find a larger certificate ${C(x, F)}$ when compared to the loss of the perturbed example. If the certificate is smaller than 0, it is guaranteed that the true label will have the bigger score, and it can be stated that within this distance the model is safe. The works differ on the means to find the certificate. [112] transforms the problem into a linear programming problem. [111] derives the certificate using semidefinite programming. An upper-bound estimation based on statistical methods is also proposed in Webb et al. [134]. 

#### 4.3.4 Randomized Smoothing 

Randomized Smoothing is a set of algorithms based on a mathematical formalist inspired in cryptography, differential privacy (DP). This set of algorithms explore the connection between DP and robustness against norm-bounded adversarial examples in ML.

A classifier ${f : R^{d} \rightarrow [0, 1]^{k}}$ which maps the input ${x}$ with probability ${[0, 1]}$ to any of the ${d}$ classes, is said to be ${\epsilon}$-robust at ${x}$ if:

$$
TODO
$$

moreover, if ${f}$ is ${L − Lipschitz}$ then ${f}$ is ${\epsilon}$-robust at ${x}$ with

$$
TODO
$$

MMR-Unirversal. The regularizer is then incorporated in the loss function to improve the robustness of the model: 

$$
TODO
$$

During the optimization, the regularizer aims at pushing both the polytope boundaries and the decision hyperplanes farther than ${l_{\infty}}$ and ${l_{1}}$ distances from the training point ${x}$, to achieve robustness close or better than ${l_{\infty}}$ and ${l_{1}}$ respectively.

## 5 CHALLENGES AND FUTURE OPPORTUNITIES 

We’ve discussed and presented results in three methods for generating robust machine learning algorithms: adversarial (re)training, regularization, and certified defenses. The search for an optimal method to strengthen DL algorithms against adversaries has a solid structure but still requires a significant effort to achieve the major objective. While a great number of new algorithms have been published every year, both in the realm of attacks and defenses, no algorithm based on adversarial (re)Training or attack generation can claim to be the final and optimal method. At the speed new defenses arise, attackers exploit the gradient or other nuances from these defensive algorithms to generate their effective low norm perturbation. The reason for this arms race, is the fact that the defenses are not absolute, or that while trying to solve maximization in Equation 3, only an empirical approximation is used, and no global optimality is achieved. Moreover as the algorithms fail to account for all possible scenarios, there will always be an example in the neighborhood of ${x}$, such that ${{\lVert x − x^{\prime} \rVert}_{p} \leq \delta}$. 

Other than optimizing Equation 3, with an empirical solution, certified defense mechanisms present a formal alternative to achieve robust deep learning models, but this certified solution comes with the cost of efficiency, and high computational cost. It derives from Equation 5. The nonlinear equality constraint for each layer of the neural network is unsolvable with standard techniques, and several methods have been proposed to achieve the optimal solution namely: linear relaxations, convex relaxations, interval propagation, abstract interpretation, mixed integer linear programming, or combination of these methods. We see great research opportunities and challenges in further improving such algorithms. They are in the early stages of development, in which the abstractions and formal approximations for the nonlinearity constraints shown in Equation 5 are not optimized for parallel computation as their numerical counterparts are. Such restriction makes it almost impractical to have such mechanisms to be applied in larger datasets. More in the topic, approximations of the optimization constraints either by a lower bound or upper bound has shown to speed-up the training process of such algorithms, but at the cost of having over-estimated bounds for the maximum allowed perturbation. These algorithms have not yet demonstrated high accuracy in large datasets corrupted by adversaries, or smaller datasets with high level of corruption. Part of the issue comes from the imposed convex relaxations. As they are not tight enough, it requires the verification algorithms to explore a bigger region, than actually necessary to verify the existence of adversarial examples in the neighborhood of the input. 

Moreover as seen in [67], even certified defenses can be broken when a big enough disturbance is applied to the model. It is arguable that even with the rigorous mathematical formulation of the defenses and certifications the constraint imposed by lp norm is weak. Most of the models can not achieve certifications beyond ${\epsilon = 0.3}$ disturbance in ${l_{2}}$ norm, while disturbances ${\epsilon = 4}$ added to the target input are barely noticeable by human eyes, and ${\epsilon = 100}$, when applied to the original image are still easily classified by humans as belonging to the same class. As discussed by many authors, the perception of multi-dimensional space by human eyes goes beyond what the ${l_{p}}$ norm is capable of capturing and synthesizing. It is yet to be proposed more comprehensive metrics and algorithms capable of capturing the correlation between pixels of an image or input data which can better translate to optimization algorithms how humans distinguish features of an input image. Such a metric would allow the optimization algorithms to have better intuition on the subtle variations introduced by adversaries in the input data. As we seek to apply machine learning algorithms in safecritical applications, a model that works most of the time is not enough to guarantee safety of such implementations. It is imperative to know the operational restrictions of the algorithm, and the level of corruption they can safely handle. For such, formally certifying the algorithms is crucial, but increasing the neighborhood around the input that the certification can be guaranteed is fundamental for the practical application of current available techiniques.

## 6 CONCLUSION 

Training safe and robust DNN algorithms is essential for their usage in safe-critical applications. This paper studies strategies to implement adversary robustly trained algorithms towards guaranteeing safety in machine learning models. We initially provide a taxonomy to classify adversarial attacks and defenses. Moreover, we provide a mathematical formulation for the Robust Optimization problem and divide it into a set of sub-categories which are: Adversarial (re)Training, Regularization Approach, and Certified Defenses. With the objective of elucidating methods to approximate the maximization problem, we present the most recent and important results in adversarial example generation. Furthermore, we describe several defense mechanisms that incorporate adversarial (re)Training as their main defense against perturbations or add regularization terms that change the behavior of the gradient, making it harder for attackers to achieve their objective. In addition, we surveyed methods which formally derive certificates of robustness by exactly solving the optimization problem or by approximations using upper or lower bounds