\section{Normativa y estándares}

\subsection{El Reglamento Europeo de Inteligencia Artificial}

A partir de 2022 se creó el primer reglamento a nivel Europeo del uso de la Inteligencia artificial. El reglamento establece una jerarquía de riesgos en función del uso de la \acrshort{IA} y sobre las categorías detectadas se establecen unas obligaciones \citet{El-Reglamento-Europeo-de-IA-en-resumen}.

Mediante un análisis de riesgos se han clasificado un conjunto de familias de sistemas de \acrshort{IA} que pueden considerarse de alto riesgo respecto a un riesgo a la salud, seguridad o derechos fundamentales.

El reglamento detalla sistemas de identificación biométrica, infraestructuras críticas, selección y promoción de personal, usos en fronteras o usados por las Fuerzas y Cuerpos de Seguridad del Estado o la Administración de Justicia. También se indica que la lista no es fija y puede ser modificada por un acto delegado.

Los productos que ya cuenten con una normativa armonizada por la \acrshort{UE} deberán ser sometidos a una evaluación de conformidad. También se definen una autoridad de supervisión de mercado como autoridades nacionales competentes.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{figures/chapter02/Pirámide Reglamento Europeo.pdf}
    \caption{Reglamento Europeo\newline{}Fuente: Elaboración propia, inspirado en \cite{El-Reglamento-Europeo-de-IA-en-resumen,reglamento-ia-telefonica}}
    \label{fig:piramide-reglamento-europeo}
\end{figure}

El reglamento define el siguiente esquema para la certificación de productos, una autoridad notificante habilita a organismos de evaluación de conformidad para hacer las evaluaciones de conformidad en materia de \acrshort{IA} a productos que quieran comercializar o poner en funcionamiento. Los productos con regulación deben ser sujetos a la supervisión designada.

Para el caso de sistemas de identificación biométrica utilizados por Fuerzas y Cuerpos de Seguridad del Estado, migración y administración de justicia, las autoridades de supervisión serán las encargadas de supervisar las actividades de seguridad, migración y asilo, en su defecto será la agencia de protección de datos. 

El Comité orientará sobre el reglamento, elaborará guías y establecerá las reglas básicas para elaborar \textit{sandboxes}. Los \textit{sandboxes} permitirán desarrollar y probar sistemas de inteligencia artificial en un entorno regulado. Esto facilitará la elaboración de directrices y reglas más precisas y aplicables, asegurando que las innovaciones en inteligencia artificial se desplieguen de manera segura, ética y conforme a la legislación Europea.

\subsubsection{Sistemas de Inteligencia artificial prohibidos}

Se prohíben los siguientes sistemas de la \acrfull{IA}.

\begin{itemize}
    \item Técnicas subliminales que distorsionen el comportamiento de una persona.
    \item Explotación de vulnerabilidades de un grupo específico de personas por su edad, discapacidad o situación social o económica.
    \item Elaboración de perfiles de personas según su comportamiento, creando un ``baremo social'' que pueda resultar en un trato  des proporcionadamente desfavorable.
    \item Identificación biométrica en tiempo real en lugares accesibles al público por parte de las fuerzas y cuerpos de seguridad, exceptuando muchos casos.
\end{itemize}

El reglamento especifica que la lista puede ser modificada por la Comisión.

\subsubsection{Sistemas de Inteligencia artificial de Alto Riesgo (HRAIS)}

Se definen como sistema de alto riesgo los que cumplan alguna de estas tareas.

\begin{itemize}
    \item Los sistemas que ya cuenten con una legislación Europea Armonizada y estén sujetos a evaluación de conformidad.
    \item Sistemas de identificación biométrica.
    \item Gestión de infraestructuras críticas.
    \item Educación y formación profesional.
    \item Selección de personal y gestión de las relaciones laborales.
    \item Gestión del acceso de las personas a servicios esenciales, públicos y privados.
    \item Actividades de fuerzas y cuerpos de seguridad.
    \item Migración, asilo y control de fronteras.
    \item Administración de justicia y procesos democráticos.
\end{itemize}

Los sistemas de alto riesgo deben cumplir con un nivel adecuado de las siguientes exigencias.

\begin{itemize}
    \item Gestión de riesgos.
    \item Gobernanza y gestión de los datos.
    \item Documentación técnica actualizada.
    \item Registros de actividad del sistema.
    \item Información a los usuarios.
    \item Supervisados por una o varias personas.
    \item Precisión, robustez y seguridad informática.
\end{itemize}

\subsubsection{Obligaciones de transparencia de ciertos sistemas de Inteligencia artificial}

Los proveedores de sistemas deben informar a las personas que están interactuando con un sistema de \acrfull{IA}, excepto cuando se usen para la persecución del crimen.

También se deberá informar a las personas de los sistemas \textit{deep fakes} excepto en persecución del crimen, contenidos evidentemente creativos, satíricos o ficticios.

\subsubsection{Medidas de apoyo a la innovación}

Las autoridades nacionales pueden crear \textit{sandboxes} regulatorios para desarrollar, entrenar, probar y validar sistemas de \acrfull{IA} bajo su guía, supervisión y soporte.

\subsubsection{Gobernanza}

El reglamento designa dos entidades para la gobernanza, el Comité Europeo de Inteligencia Artificial y las Autoridades nacionales competentes.

El Comité Europeo de Inteligencia Artificial, con un representante de cada \acrshort{EEMM} y un grupo permanente de interesados. Sus tareas incluyen, recolección de información técnica y de las mejores prácticas, armonización de pruebas de conformidad, cooperación con organizaciones de expertos en servicios digitales, publicar recomendaciones sobre especificaciones, estándares y guías. Aconsejar a la Comisión en líneas maestras para la implantación del reglamento, etc.

La Autoridad nacional competente deberá designar una autoridad notificadora y una autoridad de supervisión, estas deben ser objetivas e imparciales y deben ser dotadas de los recursos adecuados.

\subsubsection{Monitorización post-comercialización, compartición de información y supervisión de mercado}

Los proveedores de sistemas de alto riesgo deben contar con planes de monitorización apropiados a los riesgos detectados. En caso de cualquier incidente, los proveedores deberán informar a las autoridades de supervisión y a otras autoridades encargadas.

Sé informarán a la Comisión sobre las actividades relacionadas con el Reglamento. Los \acrshort{HRAIS} serán supervisados según la normativa correspondiente: los financieros por el supervisor financiero, los sistemas biométricos por fuerzas de seguridad o la agencia de protección de datos, y las instituciones de la Unión por el Supervisor Europeo de Protección de Datos. Los Estados miembros deben coordinar a las autoridades nacionales de supervisión.

Las autoridades de mercado pueden acceder a datos, código y documentación de los \acrshort{HRAIS} para verificar el cumplimiento, otorgar o denegar permisos para pruebas, y suspender pruebas ante incidentes serios.

Las autoridades de mercado evaluarán sistemas que tengan riesgos y exigir correcciones. Se puede prohibir o restringir sistemas no corregidos y exigir medidas para los que presentan algún riesgo elevado. Se crearán centros de pruebas y un banco de expertos en \acrfull{IA} para apoyar y asesorar.

\subsubsection{Códigos de conducta}

Los códigos de conducta deben ser redactados por los Estados miembros y por la comisión, estos fomentarán la aplicación voluntaria de estos códigos a los sistemas que no sean de alto riesgo.

\subsubsection{Confidencialidad y sanciones}

El reglamento define una tabla de sanciones en función de la falta y la entidad agresora, estas sanciones pueden ser modificadas, además de que se pueden aplicar superiores e inferiores, valorando las circunstancias y el proveedor.


% https://eur-lex.europa.eu/resource.html?uri=cellar:e0649735-a372-11eb-9585-01aa75ed71a1.0008.02/DOC_1&format=PDF
% https://eur-lex.europa.eu/resource.html?uri=cellar:e0649735-a372-11eb-9585-01aa75ed71a1.0008.02/DOC_2&format=PDF

% Según la unión europea toda la seguridad es este apartado :ok: 
% (51) La ciberseguridad es fundamental para garantizar que los sistemas de IA resistan a las actuaciones de terceros maliciosos que, aprovechando las vulnerabilidades del sistema, traten de alterar su uso, conducta o funcionamiento o de poner en peligro sus propiedades de seguridad. Los ciberataques contra sistemas de IA pueden dirigirse contra elementos específicos de la IA, como los conjuntos de datos de entrenamiento (p. ej., contaminación de datos) o los modelos entrenados (p. ej., ataques adversarios), o aprovechar las vulnerabilidades de los elementos digitales del sistema de IA o la infraestructura de TIC subyacente. Por lo tanto, para asegurar un nivel de ciberseguridad adecuado a los riesgos, los proveedores de sistemas de IA de alto riesgo deben adoptar medidas adecuadas teniendo también en cuenta, cuando proceda, la infraestructura de TIC subyacente.

% https://www.youtube.com/@felipebravom

% Feb 1990: Generative Adversarial Networks / Curiosity Generative Adversarial Networks (GANs) have become very popular.[MOST] They were first published in 1990 in Munich under the moniker Artificial Curiosity. [AC90-20][GAN1] Two dueling NNs (a probabilistic generator and a predictor) are trying to maximize each other's loss in a minimax game.[AC](Sec. 1) The generator (called the controller) generates probabilistic outputs (using stochastic units[AC90] like in the much later StyleGANs[GAN2]). The predictor (called the world model) sees the outputs of the controller and predicts environmental reactions to them. Using gradient descent, the predictor NN minimizes its error, while the generator NN tries to make outputs that maximize this error: one net's loss is the other net's gain.[AC90] (The world model can also be used for continual online action planning.[AC90][PLAN2-3][PLAN])
% 4 years before a 2014 paper on GANs,[GAN1] my well-known 2010 survey[AC10] summarised the generative adversarial NNs of 1990 as follows: a "neural network as a predictive world modelis used to maximize the controller's intrinsic reward, which is proportional to the model's prediction errors" (which are minimized).
% The 2014 GANs are an instance of this where the trials are very short (like in bandit problems) and the environment simply returns 1 or 0 depending on whether the controller's (or generator's) output is in a given set.[AC20][AC][T22](Sec. XVII)
% Other early adversarial machine learning settings[S59][H90] were very different—they neither involved unsupervised NNs nor were about modeling data nor used gradient descent.
% The 1990 principle has been widely used for exploration in Reinforcement Learning[SIN5][OUD13] [PAT17][BUR18] and for synthesis of realistic images,[GAN1,2] although the latter domain was recently taken over by Rombach et al.'s Latent Diffusion, another method published in Munich,[DIF1] building on Jarzynski's earlier work in physics from the previous millennium[DIF2]  and more recent papers.[DIF3-5]
% In 1991, I published yet another ML method based on two adversarial NNs called Predictability Minimization for creating disentangled representations of partially redundant data, applied to images in 1996.

% endregion subsection Normativa y estándares
