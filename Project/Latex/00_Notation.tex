\chapter*{Notación}
\addcontentsline{toc}{chapter}{\protect\numberline{}Notación}

% TODO: https://nthu-datalab.github.io/ml/slides/Notation.pdf
% TODO: https://www.mlfactor.com/notdata.html


\begin{table}[H]
    \begin{center}
        \begin{tabularx}{\textwidth}{|r|X|}
            \hline
            \textbf{Notación}   & \textbf{Definición}                                                                                               \\
            \hline
            $ \mathbb{Z} $      & Números enteros. i.e. \scriptsize{${ \mathbb{Z}  = \left\{ \ldots, -2, -1, 0, +1, +2,~\ldots \right\} }$}         \\
            $ \mathbb{Z}^{+} $  & Números enteros positivos. i.e. \scriptsize{${ \mathbb{Z}^{+}  = \left\{ 0, +1, +2,~\ldots \right\} }$}           \\
            $ \mathbb{Z}^{++} $ & Números enteros positivos sin el cero. i.e. \scriptsize{${ \mathbb{Z}^{++}  = \left\{ +1, +2,~\ldots \right\} }$} \\
            $ \mathbb{N} $      & Números naturales                                                                                                 \\
            $ \mathbb{R} $      & Números reales                                                                                                    \\
            $ \mathbb{C} $      & Números complejos                                                                                                 \\
            $ R^{n} $           & Espacio vectorial $n$-dimensional de números reales                                                               \\
            $ \epsilon $        & Para cantidades, arbitrariamente pequeñas.                                                                        \\
            \hline
        \end{tabularx}
        \caption{Notación números y arrays}
        \label{tab:notation-part-2}
    \end{center}
\end{table}


\begin{table}[H]
    \begin{center}
        \begin{tabularx}{\textwidth}{|r|X|}
            \hline
            \textbf{Notación}                                                 & \textbf{Definición}                                                 \\
            \hline
            % Notación de vectores y matrices
            $ \mathVector{a} = [a_{i}]_{i=1, ~\ldots, n} $                    & Vectores definidos en minúscula, negrita y cursiva                  \\
            $ \mathMatrix{A} = [a_{i,j}]_{i=1, ~\ldots, n, j=1, ~\ldots, m} $ & Matrices definidas en mayúscula, negrita y cursiva                  \\
            $ \mathTensor{A} $                                                & Tensores definidos en mayúscula, negrita y en estilo sans serif     \\
            \hline
            % Indices de vectores y matrices
            ${ \mathVector{a}_{i} }$                                          & Elemento $i$ del vector $\mathVector{a}$, empezando el índice por 1 \\
            ${ \mathMatrix{A}_{i,j} }$                                        & Elemento ($i,j$) de la matriz $\mathMatrix{A}$                      \\
            ${ \mathMatrix{A}_{i,:} }$                                        & Fila $i$ de la matriz $\mathMatrix{A}$                              \\
            ${ \mathMatrix{A}_{:,j} }$                                        & Columna $j$ de la matriz $\mathMatrix{A}$                           \\
            ${ \mathTensor{A}_{i,j,k} }$                                      & Elemento ($i,j,k$) 3D del tensor $\mathTensor{A}$                   \\
            ${ \mathTensor{A}_{:,:,k} }$                                      & Desplazamiento 2D del Tensor 3D                                     \\
            \hline
        \end{tabularx}
        \caption{Notación de índices para vectores, matrices y tensores}
        \label{tab:notation-part-v-1}
    \end{center}
\end{table}

\begin{table}[H]
    \begin{center}
        \begin{tabularx}{\textwidth}{|r|X|}
            \hline
            \textbf{Notación}     & \textbf{Definición}                                                                                                  \\
            \hline

            % Vectores, matrices y tensores especiales
            $ I_{n} $             & Matriz identidad de tamaño $ n \times n $                                                                            \\
            $ 0_{n,m} $           & Matriz de ceros de tamaño $ n \times m $                                                                             \\
            $ \underline{0}_{n} $ & Vector de ceros de tamaño $ n $                                                                                      \\
            $ 1_{n,m} $           & Matriz de unos de tamaño $ n \times m $                                                                              \\
            $ \underline{1}_{n} $ & Vector de unos de tamaño $n $                                                                                        \\
            $ e_{i} $             & Vector estándar o vector canónico.\newline i.e. \scriptsize{${ v_{x} = (1,0,0), v_{y} = (0,1,0), v_{z} = (0,0,1)} $} \\
            \hline
        \end{tabularx}
        \caption{Notación de vectores, matrices y tensores especiales}
        \label{tab:notation-part-v-2}
    \end{center}
\end{table}


% Lp norma --> https://www.youtube.com/watch?v=NKuLYRui-NU

\begin{table}[H]
    \begin{center}
        \begin{tabularx}{\textwidth}{|r|X|}
            \hline
            \textbf{Símbolo}                                                                                                     & \textbf{Definición}                                                                         \\
            \hline
            % Operaciones tipicicas
            $ \mathMatrix{A}^{\dagger} $                                                                                         & Traspuesta de la matriz $\mathbf{A}$                                                        \\
            $ \text{rk}\left(\mathMatrix{A}\right) $                                                                             & Rango de la matriz $\mathbf{A}$                                                             \\
            $ \text{tr}\left(\mathMatrix{A}\right) $                                                                             & Traza de la matriz $\mathbf{A}$                                                             \\
            $ \text{det}\left(\mathMatrix{A}\right) $                                                                            & Determinante de la matriz $\mathbf{A}$                                                      \\
            $ \text{Im}\left(\Phi\right) $                                                                                       & Imagen del mapeo lineal $\Phi$                                                              \\
            $ \text{ker}\left(\Phi\right) $                                                                                      & Núcleo (espacio nulo) de un mapeo lineal $\Phi$                                             \\
            % $ \text{span}\left[\mathbf{b}_{1}\right] $                        & Sistema generador de $\mathbf{b}_{1}$                                                                                \\
            \hline
            % Notación matemática de operaciones con vectores y matrices
            $ {\lvert \cdot \rvert} $                                                                                            & Determinante o valor absoluto                                                               \\
            $ {\lVert x \rVert}_{p} = \sqrt[p]{\sum_{i=1}^{n} {\lvert x_{i} \rvert}^{p} }$                                       & Norma $L^{p}$ de $x$                                                                        \\
            $ {\lVert x \rVert} $                                                                                                & Norma $L^{2}$ de $x$                                                                        \\
            $ {\Vert \underline{x} - \underline{y} \Vert}_{q} $                                                                  & Distancia en la misma dimensión entre $\underline{x}$ e $\underline{y}$                     \\
            $ \underline{x} \odot \underline{m} $                                                                                & Operación por elementos de los vectores o matrices                                          \\
            $ \left\langle \underline{z}, \underline{y} \right\rangle = \underline{z}' \underline{y} = \sum_{j=1}^{N}{z_j y_j} $ & Producto escalar de vectores por columnas $\underline{z}, \underline{y} \in \mathbb{R}^{N}$ \\
            \hline
        \end{tabularx}
        \caption{Notación de operaciones con vectores y matrices}
        \label{tab:notation-part-operations-v-m}
    \end{center}
\end{table}


\begin{table}[H]
    \begin{center}
        \begin{tabularx}{\textwidth}{|r|X|}
            \hline
            \textbf{Notación}                                                                                                                                                                                                                 & \textbf{Definición}                                                                                   \\
            \hline
            % Notación matemática de gradientes, funciones a minimizar, operador
            % $ g \circ f $                          & Función composiciones (g después de f)                                                                                        \\
            $ f: \mathbb{A} \rightarrow \mathbb{B} $                                                                                                                                                                                          & Una función $f$ con dominio $\mathbb{A}$ y rango $\mathbb{B}$                                         \\
            $ \nabla f = \left( \frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}, \frac{\partial f}{\partial z} \right) $                                                                                                         & Gradiente, es un vector que indica la dirección de mayor pendiente de una superficie en un punto dado \\
            $ \nabla f(\mathbf{a}) \in \mathbb{R}^{n} $                                                                                                                                                                                       & Gradiente de la función $f: \mathbb{R}^{n} \rightarrow \mathbb{R}$ con entrada $\mathbf{a}$           \\
            $ \nabla \cdot f =  \frac{\partial f}{\partial x} + \frac{\partial f}{\partial y} + \frac{\partial f}{\partial z} $                                                                                                               & Divergencia                                                                                           \\
            $ \nabla \times f =  \left( \frac{\partial f}{\partial y} - \frac{\partial f}{\partial z}, \frac{\partial f}{\partial z} - \frac{\partial f}{\partial x}, \frac{\partial f}{\partial x} - \frac{\partial f}{\partial y}  \right)$ & Rotación                                                                                              \\
            $ f_{*} = \text{min}_x~f(x) $                                                                                                                                                                                                     & El valor de función más pequeño de $f$                                                                \\
            $ {x}_x \in \text{arg min}_{x}~f(x) $                                                                                                                                                                                             & El valor $x_{*}$ (conjunto de valores) que minimiza $f$                                               \\
            \hline
        \end{tabularx}
        \caption{Notación de funciones}
        \label{tab:notation-part-functions}
    \end{center}
\end{table}


\begin{table}[H]
    \begin{center}
        \begin{tabularx}{\textwidth}{|r|X|}
            \hline
            \textbf{Notación}                                                                     & \textbf{Definición}                                                                                                                                                     \\
            \hline % Notación de ANN (redes neuronales, conjunto de datos, entrada, salida, clases, etc)
            $ \mathcal{X} $                                                                       & Conjunto de datos, donde $\mathDataset{X} \in \mathbb{R}^{N}$\newline{} La dimensión de la matriz de características es ${\mathcal{X} = I \times K}$                    \\
            $ \mathcal{Y}, y, tag(x) $                                                            & Clases del conjunto de datos $\mathDataset{X}$, esto es, $\textit{K} = \lvert \mathcal{Y} \rvert$. \newline i.e. \scriptsize{${ \mathcal{Y} = \{1,2,...,\textit{K}\}}$} \\
            $ (x^{(i)},y^{(i)}) $                                                                 & La tupla elementos $i$ en el conjunto $\mathDataset{X}$ (Aprendizaje supervisado)                                                                                       \\
            $ x^{(i)} $                                                                           & Los elementos $i$ en el conjunto $\mathDataset{X}$ (Aprendizaje no supervisado)                                                                                         \\
            $ N $                                                                                 & Dimensión del espacio de entrada $\mathcal{X}$                                                                                                                          \\
            $ \left\{x_{1}, x_{2}, \ldots ,x_{n}\right\} $                                        & Conjunto con $n$ elementos                                                                                                                                              \\
            ${T~\text{o}~\mathDataset{X}          = \left\{x^{(i)}, y^{(i)}\right\}}$             & Conjunto de datos de entrenamiento ${D \subset \mathDataset{X}}$                                                                                                        \\
            ${V~\text{o}~\mathDataset{X}^{\prime} = \left\{x^{\prime(i)}, y^{\prime(i)}\right\}}$ & Conjunto de datos de validación ${H \subset \mathDataset{X}}$                                                                                                           \\
            $ I $                                                                                 & Número de instancias, registros u observaciones                                                                                                                         \\
            $ K $                                                                                 & Número de atributos, características, entradas o predictores                                                                                                            \\
            % $ \mathcal{D} $                                                                       & Subconjunto para el entrenamiento, donde $\mathcal{D} \subset \mathcal{X}$                                                                                                      \\
            % $ H $                                                                                 & Subconjunto de validación, este conjunto contiene muestras de todas las clases                                                                                                     \\
            $ \theta $                                                                            & Parámetros del modelo                                                                                                                                                   \\
            $ \eta $                                                                              & Tasa de aprendizaje                                                                                                                                                     \\
            $ \hat{y} $                                                                           & Predicción del modelo                                                                                                                                                   \\
            $ \mathcal{L} $                                                                       & Función de perdida                                                                                                                                                      \\
            $ \phi ,\varphi $                                                                     & Función de activación                                                                                                                                                   \\
            $ f\left(\cdot\right) $                                                               & Modelo                                                                                                                                                                  \\
            $ {c}\left(\cdot\right) $                                                             & Modelo clasificador                                                                                                                                                     \\
            $ {p}\left(\cdot\right) $                                                             & Modelo predictor                                                                                                                                                        \\
            \hline
        \end{tabularx}
        \caption{Notación de \textit{Machine Learning}}
        \label{tab:notation-part-ml}
    \end{center}
\end{table}

\begin{table}[H]
    \begin{center}
        \begin{tabularx}{\textwidth}{|r|X|}
            \hline
            \textbf{Notación} & \textbf{Definición}                                  \\
            \hline % Notación de GANS (redes neuronales adversariales)
            $ z $             & Ruido                                                \\
            $ \mathrm{z} $    & Código latente                                       \\
            $ x^{\prime} $    & Dato adversarial                                     \\
            $ \hat{x} $       & Instancia generada                                   \\
            $ y^{\prime} $    & Clase objetiva del ejemplo adversario                \\
            $ \delta $        & Perturbación                                         \\
            $ \theta $        & Parámetros del modelo en la red generadora $(G)$     \\
            $ \omega $        & Parámetros del modelo en la red discriminadora $(D)$ \\
            $I^{LR}$          & Imagen de baja resolución                            \\
            $I^{HR}$          & Imagen de alta resolución                            \\
            ${\ell}$          & Funciones Lipschitz                                  \\
            ${\ell_{2}}$      & Funciones Lipschitz                                  \\
            ${\ell_{\infty}}$ & Funciones Lipschitz                                  \\
            % $ \hat{p}\left(\cdot\right) $ & Modelo predictor                                    \\
            % $ \hat{c}\left(\cdot\right) $ & Modelo inferido clasificador                                 \\
            % $ \Delta, \epsilon  $ & Restricción en la perturbación                                                   \\
            \hline
        \end{tabularx}
        \caption{Notación de \textit{GAN}}
        \label{tab:notation-part-gans}
    \end{center}
\end{table}



\begin{table}[H]
    \begin{center}
        \begin{tblr}{rows={valign=m},colspec={|r|X|}} 
            \hline
            \textbf{Notación}         & \textbf{Definición}                                                                     \\
            \hline % Notación probabilidad y estadística
            ${Q(x)}$                  & Función de densidad de probabilidad de la variable aleatoria ${x}$                      \\
            ${\mathbb{E}_{x \sim Q}}$ & Esperanza de $f(x)$ con respecto a la distribución de probabilidad de ${x}$ según ${Q}$ \\
            ${I(X;Y)}$                & Información mutua, ${I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)}$                           \\
            ${W_{p}, W_{p}(F,G)}$     & Distancia de Wasserstein                                                                \\
            \hline
        \end{tblr}
        \caption{Notación probabilidad y estadística}
        \label{tab:notation-p-e}
    \end{center}
\end{table}




% \newpage
% \KOMAoptions{paper=landscape,pagesize}
% \recalctypearea
% \section{This is my Landscape Page}
% Text in my landscape section\footnote{Footnote in Landscape}

% \newpage
% \KOMAoptions{paper=portrait,pagesize}
% \recalctypearea


% \begin{enumerate}
%     \item $\mathcal{Y}$ es el conjunto de clases del conjunto de datos $\mathcal{X}$.
%     \item $\hat{c}(\mathcal{X})$ es el término que nos referimos al clasificador del conjunto $\mathcal{X}$
%     \item $P(C|A)$ nos referimos a la probabilidad condicionada ($P$) de la clase ($C$) con los atributos ($A$).
%           % \item $ a \gg b$ `a' mucho mayor que `b'.
%           % \item $ a \ll b$ `a' mucho menor que `b'.
% \end{enumerate}

% Le quita el nº en la tabla de contenidos
\chapter*{Fundamentos matemáticos}
\addcontentsline{toc}{chapter}{\protect\numberline{}Fundamentos matemáticos}

\section*{Teoremas}

% Maldición de la dimensionalidad -> clasificación basaa en naive bayes 
% Clasificador de Naïve Bayes
% Cada atributo como una variables independiente de la case
% Naïve significa ingenuo

\begin{theorem}[Bayes]
    Sean ${A}$, ${B}$ dos eventos, y ${P(A \vert B)}$ la probabilidad de ${A}$ dependiente dé ${B}$. Entonces.
    $$ P(A \vert B) = \frac{P\left( B \vert A \right) P\left(A\right)}{P\left(B\right)} $$
    
    \label{theorem:bayes}
\end{theorem}


% http://www.lcc.uma.es/~jmortiz/archivos/Tema4.pdf
\begin{theorem}[Convergencia de perceptrón]
    Si el conjunto de patrones de entrenamiento $ \{x^{1}, z^{1}\}, \{x^{2}, z^{2}\}, \cdots, \{x^{n}, z^{n}\} $ es linealmente separable entonces el Perceptrón simple encuentra una solución en un número finito de iteraciones, es decir, consigue que la salida de la red coincida con la salida deseada para cada uno de los patrones de entrenamiento.

    \label{theorem:ConvergenciaRosenblatt}
\end{theorem}


\begin{theorem}[Wasserstein]
    % Let $\mathbb{P}_r$ be any distribution. Let $\mathbb{P}_\theta$ be the distribution of $g_\theta(Z)$ with $Z$ a random variable with density $p$ and $g_\theta$ a function satisfying assumption.
    % Then, there is a solution $f: \mathcal{X} \rightarrow \mathbb{R}$ to the problem
    % $$ \max_{\|f\|_L \leq 1} \mathbb{E}_{x \sim \mathbb{P}_r}[f(x)] - \mathbb{E}_{x \sim \mathbb{P}_\theta} [f(x)] $$ and we have
    % $$ \nabla_\theta W(\mathbb{P}_r, \mathbb{P}_\theta) = -\mathbb{E}_{z \sim p(z)}[\nabla_\theta f(g_\theta(z))] $$when both terms are well-defined.

    Sea ${\mathbb{P}_{r}}$ una distribución cualquiera y ${\mathbb{P}_\theta}$ la distribución de ${g_\theta(Z)}$ siendo ${Z}$ una variable aleatoria con densidad ${p}$ y ${g_\theta}$ una función que satisface el supuesto.
    
    Entonces, existe una solución ${f \mathcal{X} \rightarrow \mathbb{R}}$ al problema 
    $$ \max_{|f\|_L \leq 1} \mathbb{E}_{x \sim \mathbb{P}_r}[f(x)] - \mathbb{E}_{x \sim \mathbb{P}_\theta} [f(x)] $$ y tenemos $ \nabla_\theta W(\mathbb{P}_r, \mathbb{P}_\theta) = -\mathbb{E}_{z \sim p(z)}[\nabla_\theta f(g_\theta(z))] $ cuando ambos términos están bien definidos.
    \label{theorem::gradW}
\end{theorem}

% https://www.ugr.es/~rpaya/documentos/CalculoII/2013-14/Uniforme.pdf
% https://www.youtube.com/watch?v=PwQEPYe_g1E
\begin{theorem}[Lipschitz]
    % Sean $(X, d)$ un espacio métrico\footnote{Un espacio métrico es un conjunto que lleva asociado una función distancia.}, ${A \subseteq X}$ no vació y ${x \in X}$. Demuestra que la función ${f: X \subset \mathbb{R} \rightarrow \mathbb{R} }$ ${f(x) = \text{dist}(x,A) = \displaystyle\inf_{a \in A}d(x,a)}$ es Lipschitz continua.

    Se dice que una función es continua y uniforme, también llamada función Lipschitz ${f: A \rightarrow \mathbb{R}}$ cuando existe una constante ${M \geq 0}$ que verifica:
    $${\lvert f(y) - f(x)\rvert \leq M \lvert y-x \rvert ~~\forall x,y \in A}$$
    Existe una constante mínima ${M_{0}} \geq 0$ que verifica la desigualdad anterior, ${M_{0}}$ es llamada constante de Lipschitz.
    \label{theorem::lipschitz}
\end{theorem}


\section*{Conceptos previos}

% TODO

% Tesis de cesar perez curiel resumen
\subsection*{Divergencia de Kullback-Leibler}
\label{subsection:divergencia-kl}

La divergencia de Kullback-Leibler es una medida de la diferencia entre dos valores, en concreto de dos valores de una dos distribuciones de probabilidad. Se le denomina ${\mathrm{KL}}$ o {$D_{\mathbb{KL}}$}, se puede calcular mediante la ecuación.

\begin{equation}
    D_{\mathbb{KL}}(P || Q) = \mathbb{E}_{x\sim{}P} \left[ \log{\frac{P(x)}{Q(x)}} \right]
    \label{eq:divergencia-kl}
\end{equation}

Esta medida permite conocer el límite inferior de una función que se usa para múltiples funciones de coste como en el auto-encoder variacional.